{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff77c1e-1dff-4fea-bfa0-e52720051d36",
   "metadata": {},
   "source": [
    "# <center>  LlAMA.CPP开源推理框架使用指南"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a6370-6ddc-4f4d-a0fa-282287512459",
   "metadata": {},
   "source": [
    "目录：\n",
    "\n",
    "- Llama.cpp基本信息介绍\n",
    "- 安装部署办法\n",
    "- GGUF量化格式模型调用方法\n",
    "- API调用方法\n",
    "- 以Qwen2VL为例进行模型量化并推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0d257-d767-4951-9d58-143f85de5144",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcefd80-18ed-4f2f-83fd-81f2defb2bdd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241227162540519.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c5741-adca-4429-8478-4459bfde9452",
   "metadata": {},
   "source": [
    "## Llama.cpp基本信息介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e89947-1af5-4985-95bf-5d61e5eefa96",
   "metadata": {},
   "source": [
    "- llama.cpp 使用的是 C 语言写的机器学习张量库 ggml\r",
    "- \n",
    "llama.cpp 提供了模型量化的工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbfe25-9c96-436f-8766-877e641158ed",
   "metadata": {},
   "source": [
    "该项目的最大亮点在于 无需 GPU 也能运行 LLaMA 模型。与传统框架不同，llama.cpp 构建了一个独立的生态系统，秉持轻量化设计理念，追求最小的外部依赖、多平台兼容性和广泛的硬件适配。其核心特点包括：\n",
    "\n",
    "1. 轻量化实现\n",
    "纯 C/C++ 实现：无任何外部依赖，运行高效且易于移植。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1adc753-4e32-4654-abad-ceb6ed069cb7",
   "metadata": {},
   "source": [
    "2. 硬件适配性强\n",
    "\n",
    "支持各种主流硬件平台，实现真正的跨平台兼容：\n",
    "- x86_64 CPU：支持 AVX、AVX2 和 AVX512 指令集。\n",
    "- Apple Silicon：通过 Metal 和 Accelerate 框架，支持 Apple 的 CPU 和 GPU。\n",
    "\n",
    "- GPU 支持：\n",
    "NVIDIA GPU（通过 CUDA）\n",
    "AMD GPU（通过 hipBLAS）\n",
    "Intel GPU（通过 SYCL）\n",
    "昇腾 NPU（通过 CANN）\n",
    "摩尔线程 GPU（通过 MUSA）\n",
    "Vulkan 后端：提供跨平台 GPU 支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e427f2-c154-4a2c-be03-a7ecf5e1359f",
   "metadata": {},
   "source": [
    "3. 优化推理性能\n",
    "   \n",
    "- 多种量化方案：降低内存占用，显著提升推理速度。\n",
    "- CPU+GPU 混合推理：灵活利用 CPU 和 GPU 资源，即便模型大小超出显存容量，也能高效运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aee3d4-0146-4e3c-8bad-2550300b8e8a",
   "metadata": {},
   "source": [
    "llama.cpp 的设计理念打破了对高端硬件的依赖，为轻量化、灵活性和跨平台兼容性设立了标杆，为在各种环境下运行大模型提供了全新可能。llama.cpp 可以将模型参数从 32 位浮点数转换为 16 位浮点数，甚至是 8、4 位整数。除此之外，llama.cpp 还提供了服务化组件，可以直接对外提供模型的 API 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a6294-94c2-4716-9360-dfbc849c3009",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ebe4a9-8f7b-4241-942a-b3f011393498",
   "metadata": {},
   "source": [
    "## Llama.cpp安装部署办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63fb97-1027-402f-8aa2-e7ab789824a5",
   "metadata": {},
   "source": [
    "- **Step 1. 创建conda虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5fc60-1ae3-4233-8375-5fcd274105d3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Conda创建虚拟环境的意义在于提供了一个隔离的、独立的环境，用于Python项目和其依赖包的管理。每个虚拟环境都有自己的Python运行时和一组库。这意味着我们可以在不同的环境中安装不同版本的库而互不影响。根据官方文档信息建议Python版本3.10以上。创建虚拟环境的办法可以通过使用以下命令创建：\n",
    "\n",
    "```bash\n",
    "# cpp 是你想要给环境的名称，python=3.11 指定了要安装的Python版本。你可以根据需要选择不同的名称和/或Python版本。\n",
    "\n",
    "conda create -n cpp python=3.11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15cc58f-7360-42fc-aa23-9ac0c8b845cb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220145753474.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381ab50-ec9d-4996-824a-55aace9d823d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;创建虚拟环境后，需要激活它。使用以下命令来激活刚刚创建的环境。如果成功激活，可以看到在命令行的最前方的括号中，就标识了当前的虚拟环境。虚拟环境创建完成后接下来安装torch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67073aea-3628-4fed-902a-4b4ca5507aa8",
   "metadata": {},
   "source": [
    "```\n",
    "conda activate cpp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e14368-2715-4bdf-a1bf-bd2d25d23c95",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224180047760.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03a3f6-7c1d-4f1e-a9be-9f538429b868",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果忘记或者想要管理自己建立的虚拟环境，可以通过conda env list命令来查看所有已创建的环境名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a09cf-4b7c-4735-a063-77c608c17538",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143512354.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab163e-e6ef-4fda-93a2-0c1f9878658e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果需要卸载指定的虚拟环境则通过以下指令实现：\n",
    "```\n",
    "conda env remove --name envname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb887e2-59ff-4168-8ad1-81c68a3a7706",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143643575.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57db644-1bce-4ba5-bd4c-eb2a4398b187",
   "metadata": {},
   "source": [
    "- 需要注意的是无法卸载当前激活的环境，建议卸载时先切换到base环境中再执行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86f74c-c16d-40c5-a504-e46db617b75a",
   "metadata": {},
   "source": [
    "- **Step 2. 查看当前驱动最高支持的CUDA版本**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0134f-beb7-4879-820a-9917c8969f8f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们需要根据CUDA版本选择Pytorch框架，先看下当前的CUDA版本：\n",
    "```\n",
    "nvidia -smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93fa10-f2a2-469a-90fd-f63fcb6da460",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924161818464.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824e2d7-55b6-44bf-92fa-6e85ff96cfe0",
   "metadata": {},
   "source": [
    "- **Step 3. 在虚拟环境中安装Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb333c28-2250-461c-82e3-c84bf8c4d1a7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进入Pytorch官网：https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f94e07-bb84-451f-84f8-42cf98143a34",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240103163206436.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448215e-7842-4a0d-a907-159a1938bfd4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当前的电脑CUDA的最高版本要求是12.2，所以需要找到不大于12.2版本的Pytorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb49a7-c570-4ab9-bed6-05c79ad208e7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041455184.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32d0b3-6fae-4649-8a9f-617f7a0ad7e4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直接复制对应的命令，进入终端执行即可。这实际上安装的是为 CUDA 12.1 优化的 PyTorch 版本。这个 PyTorch 版本预编译并打包了与 CUDA 12.1 版本相对应的二进制文件和库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c557089-adc3-4fa7-8ffa-244d1eba76b6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220150055394.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822f74b-9d67-4982-b782-e8aa125fd769",
   "metadata": {},
   "source": [
    "通过 pip show 的方式可以检验是否正确安装。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f295441-1a31-4413-829f-4375e61a6ce4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220150043025.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f94f5b-7506-461b-8b20-51a93b697556",
   "metadata": {},
   "source": [
    "- **Step 4. 安装必要的依赖**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ee9e8-76ef-4bb2-97e2-645706436e74",
   "metadata": {},
   "source": [
    "Transfomers是大模型推理时所需要使用的框架，通过以下指令可以下载最新版本的Transfomers："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20dfb6-3244-4acb-a5d1-54932486fdf6",
   "metadata": {},
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2527c-ab28-42a1-aac6-32bb6c198e1d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220150241431.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96dbea-a2e5-4b04-8ca1-71dc417f0659",
   "metadata": {},
   "source": [
    "pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04674496-5d0e-4579-821d-cbed19b1496d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022162830644.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d0750-787e-479e-9bb5-4e7db3a1b2df",
   "metadata": {},
   "source": [
    "pip install CMake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3845ea-ef29-48ba-91be-c0787a9fc14e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220144646165.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9622196-9919-4f7e-aa9e-4cdc2072290d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220153655104.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53480f99-f82e-4c42-a09a-0e52887ff486",
   "metadata": {},
   "source": [
    "- **Step 5. 下载LlAMA.CPP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4d5c8-7666-4454-8718-24aae783e91f",
   "metadata": {},
   "source": [
    "创建并进入新的文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1f33f-5ad4-4fa3-838b-d81878132ba8",
   "metadata": {},
   "source": [
    "```\n",
    "mkdir llamacpp\n",
    "cd llamacpp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4c81e-c239-42d5-b10b-67a8e45e95ea",
   "metadata": {},
   "source": [
    "在该文件夹中执行命令实现llama.cpp的下载\n",
    "```\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63c7f0-b792-4d5e-a389-683a67ce0ec7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220112521521.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b9be2-7db4-4027-bf63-25bde555e1e0",
   "metadata": {},
   "source": [
    "如果服务器网络不畅也可以使用手动方式在其他电脑下载，然后再传输到服务器中解压缩。GitHub上托管的网址如下： https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745df675-168a-4f6a-a0d5-d6278ea30c45",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224184307533.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaba09c-fe36-4eb3-9d2a-6534d1c2a37c",
   "metadata": {},
   "source": [
    "完成上述的下载操作后，需要解压缩，再进入到llama.cpp文件夹中。初始文件如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e975e-424a-487e-92d1-65269ace46b1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220113503406.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3af74-0ab2-4697-b67f-41b0c867a711",
   "metadata": {},
   "source": [
    "- 开启GPU加速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b84cd0-af42-4a72-a772-42551d050249",
   "metadata": {},
   "source": [
    "```\n",
    "cmake -B build -DGGML_CUDA=ON  # 启用 GPU 加速\r\n",
    "cmake --build build -- -j      # 编译\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e95f3-d47f-453f-9503-a9f82bfba96a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220162958385.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d5936-7d25-4e4c-94a0-d2055f021343",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220163425688.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e4d4a-db5e-4b56-bf85-083e226b7a80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599efc2-1321-4f00-b1b3-bf380d662bf7",
   "metadata": {},
   "source": [
    "```\n",
    "【赋范大模型技术社区】成立啦～海量硬核独家技术干货内容+无门槛技术交流，下图扫码加【助教老师】\n",
    "即可入群，社区包含：\n",
    "✅入门必备Python基础、账号注册与硬件配置方法；\n",
    "✅20+主流开源&在线大模型部署与调用方法；\n",
    "✅独家团队自研高品质技术教程&追更最新技术内容；\n",
    "✅社区交流：活跃技术氛围，技术交流&答疑；\n",
    "✅新知速递：大模型重大技术突破&最新技术信息通报；\n",
    "✅干货分享：每月2-3场硬核干货&前沿技术公开课。\n",
    "感兴趣的小伙伴抓紧时间扫码入群吧～\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d89d54-d193-49d5-ad78-18e85fa36a49",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241225191001711.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364562e-d089-49b1-b8e9-d913f58ef0a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825e1e1-05a6-46e5-9fa9-80c1fe21cb95",
   "metadata": {},
   "source": [
    "## 以Qwen2.5：14B为例实现量化推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851d81f-d5b7-4603-866d-7809e04721aa",
   "metadata": {},
   "source": [
    "#### 量化模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9d57e-4da0-430b-b366-1a17f8c7bd1a",
   "metadata": {},
   "source": [
    "首先我们通过在魔搭社区进行Qwen2.5：14B的int4量化模型下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fa140-24d3-410c-9680-fbf15abfbb16",
   "metadata": {},
   "source": [
    "Llama.cpp支持推理GGUF格式量化的模型，如下图所示选择一个通义千问2.5-14B-Instruct-GGUF的量化模型进行下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a5b99-5b6a-461a-aa49-a15922c910d3",
   "metadata": {},
   "source": [
    "https://www.modelscope.cn/models/Qwen/Qwen2.5-14B-Instruct-GGUF/files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fda21-e7b5-492f-9a19-768751fafb85",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241226141013310.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587585c-a207-407c-bdae-8de432a8be1f",
   "metadata": {},
   "source": [
    "如下图所示，该模型卡下挂载了多种不同量化参数的模型。其中，fp16表示半精度浮点模型，q2表示采用二阶整型量化的模型，而q3则是三阶整型量化的模型等等。只需下载对应某一种量化精度下的全部权重文件即可，本实例中选择下载q3精度的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4377d-5d12-4d05-b18f-b971f4e313be",
   "metadata": {},
   "source": [
    "通过指令 `modelscope download --model Qwen/Qwen2.5-14B-Instruct-GGUF qwen2.5-14b-instruct-q3_k_m-00001-of-00002.gguf --local_dir ./dir` 进行下载，其中qwen2.5-14b-instruct-q3_k_m-00001-of-00002.gguf为具体的模型文件名称，./dir为下载到的文件夹路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d0213-776c-4334-8d67-82208828ccc4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241226141617638.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de59dfc-299f-4467-913d-bf0f9c52bc80",
   "metadata": {},
   "source": [
    "使用Llama.cpp开始调用之前，需要确保模型调用路径正确。可以选择将模型部署到 llama.cpp 的 models 文件夹中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12994ff1-3eb0-47ef-a864-b80dce012c28",
   "metadata": {},
   "source": [
    "完成编译后通过以下指令实现量化模型的推理，以Qwen2.5 14B模型为例，与之进行单次推理交互："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09290433-8b62-4acd-89ff-1f6a5af502e4",
   "metadata": {},
   "source": [
    "```\n",
    "cd /build/bin\n",
    "./llama-cli -m /home/LLM/qwen2-5-14-gguf/qwen2.5-14b-instruct-q3_k_m.gguf -p \"I believe the meaning of life is\" -n 512\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbb27b-ab4f-44ab-b6cf-3b2ae14fd60c",
   "metadata": {},
   "source": [
    "其中，`./llama-cli`为启动推理的项目文件； `-m`后接需要调用的模型地址，建议选择绝对路径；`-p`后接输入的提示词prompt，使用中英文输入均可；`-n`限制返回信息长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78abc8d-7164-41f4-9fa6-7e7f5ccba2c0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220172557789.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea3b10-68f5-4dae-969b-2b655392360a",
   "metadata": {},
   "source": [
    "推理返回的结果如上图所示，进行模型加载之后开始流式返回推理结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac85495-0f87-458a-bfaf-4ac4f5572927",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220172609162.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79754a75-107b-4b0d-b7d9-d013d0d65b7e",
   "metadata": {},
   "source": [
    "正常推理14B参数大小的模型需要30G左右的显存，但是通过llama.cpp推理int 3 量化模型仅需要不到2G的显存，一般消费级显卡也可以使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56ae05-d66f-4d38-bc45-c7a223464e2a",
   "metadata": {},
   "source": [
    "完整对话内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e018d7-e8af-4965-90a4-381d213cb2ca",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220172905788.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3704d24-3605-40ff-b048-905fa73f5fa3",
   "metadata": {},
   "source": [
    "中文能力测试："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12a7cb-6b59-42f1-96e7-85f560ffbea0",
   "metadata": {},
   "source": [
    "```\n",
    "./llama-cli -m /home/LLM/qwen2-5-14-gguf/qwen2.5-14b-instruct-q3_k_m.gguf -p \"如何制作一个三明治\" -n 512\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e3021-b9de-45c5-b5e8-283ac15d9c9a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220173537752.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43daeb-5e06-4c16-81c9-1915d8003079",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220173614225.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea723322-8e86-4291-a20a-ab51af6ca170",
   "metadata": {},
   "source": [
    "数学推理测试:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7e0b2-2d46-4ceb-b362-8d9e6a65cf8b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220180005144.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01903a60-41df-4603-a1a5-c2e66ad37305",
   "metadata": {},
   "source": [
    "推理能力测试--数单词中的字母数量："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be830f-b137-4642-8e16-857e4a4bd9b1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241220180735849.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a952fa1-10e5-4dae-aa61-a2e460311395",
   "metadata": {},
   "source": [
    "通过以下代码的格式实现持续交互对话："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c75a16-3158-45eb-a8ad-876aa1884937",
   "metadata": {},
   "source": [
    "```\n",
    "./llama-cli -m /home/LLM/qwen2-5-14-gguf/qwen2.5-14b-instruct-q3_k_m.gguf -cnv --chat-template gemma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de46a9-a6d1-4a7d-ac5c-991ee189a7c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab62e0-5110-4ff7-b6ab-738a56ebbc4a",
   "metadata": {},
   "source": [
    "常用选项介绍："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652cc6f-b6c8-4ef3-9632-685211b24550",
   "metadata": {},
   "source": [
    "-m FNAME， --model FNAME：指定 LLaMA 模型文件的路径（例如， models/gemma-1.1-7b-it.Q4_K_M.gguf 如果设置，则从 --model-url 推断）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775491d8-010e-4ef1-99d9-10ec08bec6c4",
   "metadata": {},
   "source": [
    "-mu MODEL_URL --model-url MODEL_URL ：指定远程 http url 以下载文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebefb82b-0441-4097-ad45-b68f8603e2b6",
   "metadata": {},
   "source": [
    "-i， --interactive：以交互模式运行程序，允许您直接提供输入并接收实时响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bede74-561f-4ec8-a157-5434d68b3d80",
   "metadata": {},
   "source": [
    "-n N， --n-predict N：设置生成文本时要预测的标记数。调整此值会影响生成文本的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81285ffb-1fe4-4461-b624-758d27392619",
   "metadata": {},
   "source": [
    "-c N， --ctx-size N：设置提示上下文的大小。默认值为 4096，但如果 LLaMA 模型是使用较长的上下文构建的，则增加此值将为较长的输入/推理提供更好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b801059-d0a8-4ad5-8479-a8d422cf0024",
   "metadata": {},
   "source": [
    "-mli， --multiline-input：允许编写或粘贴多行代码输入，而不以单行对话输入的 '' 结尾。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332103c0-0e2e-4e5e-88b8-2dc6921572b8",
   "metadata": {},
   "source": [
    "-t N， --threads N：设置生成过程中要使用的线程数。为了获得最佳性能，建议将此值设置为系统具有的物理 CPU 内核数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25b7e8-487a-48dc-a998-ec091b3daf3d",
   "metadata": {},
   "source": [
    "-ngl N， --n-gpu-layers N：当使用 GPU 支持进行编译时，此选项允许将一些层卸载到 GPU 进行计算。通常会导致性能提高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222c132-1c4c-402b-9ace-869deb2d96cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124684a-d93c-4b2b-afe0-778d92340b93",
   "metadata": {},
   "source": [
    "#### **Llama-cli**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f6ce0-6314-4850-9fe2-648a1ee29346",
   "metadata": {},
   "source": [
    "llama-cli 程序提供了几种使用输入提示与 LLaMA 模型交互的方法："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4cf51-8e75-4d58-af1a-8397a01a49ca",
   "metadata": {},
   "source": [
    "--prompt PROMPT：直接将提示作为命令行选项提供。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3da42-7487-4300-bd81-8cf966790cb0",
   "metadata": {},
   "source": [
    "--file FNAME：提供包含一个或多个提示的文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a38cee-c9ae-4940-85ce-98ed9bc0f377",
   "metadata": {},
   "source": [
    "--interactive-first：以交互模式运行程序并立即等待输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b101d49-b9de-4c25-bd47-a24f7f32be12",
   "metadata": {},
   "source": [
    "llama-cli 程序提供了一种与 LLaMA 模型交互的无缝方式，允许用户进行实时对话或为特定任务提供说明。可以使用各种选项触发交互模式，包括 --interactive 和 --interactive-first。\n",
    "\n",
    "在交互模式下，用户可以在此过程中通过注入其输入来参与文本生成。用户可以随时按 Ctrl+C 插入并键入他们的输入，然后按 Return 将其提交到 LLaMA 模型。要在不完成输入的情况下提交其他行，用户可以用反斜杠 （\\） 结束当前行并继续键入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530406b-ce48-4ed6-acd4-2078eaa04f17",
   "metadata": {},
   "source": [
    "-i， --interactive：以交互模式运行程序，允许用户参与实时对话或向模型提供具体指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cea912-c8e4-4043-9db2-ccfc6ad6f9ef",
   "metadata": {},
   "source": [
    "--interactive-first：以交互模式运行程序，并在开始文本生成之前立即等待用户输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931988d-d900-4561-9721-e28b06d375e0",
   "metadata": {},
   "source": [
    "-cnv， --conversation： 以对话模式运行程序（不打印特殊令牌和后缀/前缀，使用默认聊天模板）（默认：false）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa1225-9b99-4d57-be55-42b964bf4a2e",
   "metadata": {},
   "source": [
    "--color：启用彩色输出，以在视觉上区分提示、用户输入和生成的文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e618ce4-2678-4cd5-8268-1b8059fabdd8",
   "metadata": {},
   "source": [
    "## Llama.cpp使用API调用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7c1e8-dedc-465d-8af2-757ee28cec00",
   "metadata": {},
   "source": [
    "Llama.cpp支持使用API实习本地模型在域内网中进行调用，在相对路径下`/llama.cpp/build/bin`启动脚本`./llama-server`，通过以下代码即可启动API调用服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c8a4f-9dcf-4f3a-bb7e-f451a92240c8",
   "metadata": {},
   "source": [
    "./llama-server -m /home/LLM/qwen2-5-14-gguf/qwen2.5-14b-instruct-q3_k_m.gguf -ngl 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862074c4-b802-430c-8818-b20419c1d3d8",
   "metadata": {},
   "source": [
    "其中`-m`是调用的模型文件路径，这里选择qwen2.5-14b-instruct-q3_k_m.gguf量化模型进行推理，`-ngl`含义是指定使用 GPU 的数量。在表示模型推理过程中将利用 8 个 GPU 进行加速。这一参数适用于多 GPU 环境下的并行计算，可显著提升推理效率。\n",
    "如果在单 GPU 或 CPU 环境中运行，则该参数可以设置为 1 或省略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236e794-c51f-4bd2-8d98-03cf257ccfcd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224142149115.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61627172-a8be-487b-8a54-25b9f9d3a6a5",
   "metadata": {},
   "source": [
    "API服务启动之后，可以在局域网中任意主机中输入以下命令实现模型对话推理:\n",
    "\n",
    "其含义是使用 curl 工具向本地运行的推理服务发送一个 POST 请求，目的是生成自然语言模型的输出结果。`curl `是一个命令行工具，用于向 HTTP 服务器发送请求。\n",
    "`--url `指定目标服务器的 URL。\n",
    "`--header `用于设置 HTTP 请求头部信息。请求体的数据格式是 JSON（JavaScript Object Notation），用于传递结构化数据。\n",
    "\n",
    "`--data `指定 POST 请求的请求体数据。`prompt`：模型的输入提示语，这里是 \"What color is the sun?\"。模型将基于此生成后续内容。\n",
    "`n_predict`：模型生成的最大 token 数，设置为 512，表示最多生成 512 个 token（包括单词、标点符号等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c47cd9-64e8-412d-9499-30b5b4fefd62",
   "metadata": {},
   "source": [
    "```\n",
    "curl --request POST \\\n",
    "    --url http://localhost:8080/completion \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    --data '{\"prompt\": \"What color is the sun?\",\"n_predict\": 512}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1b7d6-cead-402c-b2ff-3709c42013c5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224142814166.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f75f9-49df-485d-a13d-e13c4cd7dfc7",
   "metadata": {},
   "source": [
    "中文对话推理：牛肉汉堡的制作方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f3fd0-2895-458d-be0a-e8a95ce6ae99",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224143039030.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33b0af-7a37-4aef-9ebd-b3395ddd7900",
   "metadata": {},
   "source": [
    "基本数字推理：9.11和9.8哪个更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5ac49-ca89-41c9-a9e7-b3760c510174",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224143551776.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd830f-d8af-475d-806b-c6f590de77aa",
   "metadata": {},
   "source": [
    "编程能力测试：用python编写一段冒泡排序算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b403c-d8f6-4e55-8545-58b766327f99",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224144148839.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f805f0-b074-4a05-9b1b-4f12e2672101",
   "metadata": {},
   "source": [
    "后台服务器会显示运行情况以及每秒推理token速度，其显存消耗大小是和正常本地进行推理所需要的GPU空间相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a28618-180f-4c66-87cf-4ac8b3cc1f9f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224142751190.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2acc7-ef81-49ca-a78a-7aa4e763143b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224145758827.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b62e821-16b9-4ef4-b8c5-9b675c0528b4",
   "metadata": {},
   "source": [
    "需要注意的是，使用这种方式是每次单轮的对话任务，所有模型是不会保存prompt记忆的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994e103-8d7b-4c6f-a736-c5ed573ebcf0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224150423992.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e709fd-ef21-41db-8de4-13ab3ee9e45b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e4fb5-acb1-492f-bbc1-499f58aecf54",
   "metadata": {},
   "source": [
    "## 以Qwen2VL为例实现GGUF量化模型调用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71919fa-c126-44ba-aee8-ad3c59ad6946",
   "metadata": {},
   "source": [
    "通过对模型进行 GGUF 量化，可以显著降低推理过程中对显存的消耗。以下以 Qwen-2VL 这一视觉多模态大模型为例，展示模型量化的具体过程与效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73c8a9-4e57-485a-940b-ca9c3d8d7b41",
   "metadata": {},
   "source": [
    "进行本地模型量化的前提需要先在本地下载好正常参数的模型，可以通过modelscope活hugging face社区中挂载的模型进行Qwen2VL的部署安装。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f85e2-05b9-464f-bf5d-4f7457d22407",
   "metadata": {},
   "source": [
    "- 下载参考代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540096d5-f304-4606-a9e2-89573de39790",
   "metadata": {},
   "source": [
    "modelscope download --model Qwen/Qwen2-VL-7B-Instruct  --local_dir ./dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ef25c-81bd-455d-b74d-dfafab62901e",
   "metadata": {},
   "source": [
    "如下图所示，进去llama.cpp文件夹后使用内置的转化文件进行模型量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827d9dc-08cf-4204-9dcf-432710f6c5b1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241223144115895.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d70e33-9417-444e-9985-238bdd9b95fc",
   "metadata": {},
   "source": [
    "使用llama.cpp文件夹中的`convert_hf_to_gguf.py`脚本进行量化转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715eff8-fa4b-4d60-9879-9f4b454c8dd3",
   "metadata": {},
   "source": [
    "转化代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7c764-5968-4701-bda5-431ae2091162",
   "metadata": {},
   "source": [
    "python3 convert_hf_to_gguf.py /home/LLM/qwen2vl-2b/ --outfile /home/LLM/qwen2vl-gguf/qwen2vl2b.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9dee32-faae-4bb6-9cad-6df9126e770c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241223183509134.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac120a2-27ab-48c2-bfb2-1fab96709bb4",
   "metadata": {},
   "source": [
    "返回如下内容说明已经成功转化，路径可以在`home/LLM/qwen2vl-gguf/qwen2vl2b.gguf`中进行查找，这是有`outfile `参数决定的输出路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f00399-1144-419a-b94e-a8330637311c",
   "metadata": {},
   "source": [
    "INF0:hf-to-gguf:Model successfully exported to /home/LLM/qwen2vl-gguf/qwen2vl2b.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73c762-88b4-4a47-a223-6c5e36040f92",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241223183540820.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb3354-9b06-4452-90ba-f5e7f07d1bd3",
   "metadata": {},
   "source": [
    "接下来需要使用 qwen2_vl_surgery.py 将视觉编码器转换为 GGUF 格式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8564a-21e5-4d63-81da-c8fbf00203b0",
   "metadata": {},
   "source": [
    "```\n",
    "PYTHONPATH=$PYTHONPATH:$(pwd)/gguf-py python3 examples/llava/qwen2_vl_surgery.py \"/path/to/Qwen2-VL-2B-Instruct/model-dir\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e9133-e079-4831-9d69-da5b50d18a33",
   "metadata": {},
   "source": [
    "在llama.cpp文件路径下包含该脚本文件qwen2_vl_surgery.py进行视觉编码器转化，最后一个参数是转化所需权重模型文件路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18ddb4-a71a-487f-865d-711584abfc0d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224154416119.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb32b9-201a-4ac3-84a6-6eba326bdeb9",
   "metadata": {},
   "source": [
    "开始视觉编码器转化后会先加载qwen2vl模型checkpoints，根据转换脚本的超参数实现量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9a492-b03a-4480-9421-477b33de8682",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224154658348.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657546c-f2e7-4dcd-8b97-4aee9e3c0dc6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224154631609.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182221f8-a231-4c3f-aa3c-18851806edaa",
   "metadata": {},
   "source": [
    "默认转化完成的模型输出路径在llama.cpp路径下，如下图显示，转化后的视觉编码器大小为2.48G，格式同样为GGUF文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c5239-5609-4bd4-b853-33d94d60bbca",
   "metadata": {},
   "source": [
    "- 启动本地量化qwen2vl模型实现多模态推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55095be3-56b1-4e8b-a9d1-cbdbfb8b9408",
   "metadata": {},
   "source": [
    "要实现 Qwen2VL 的量化模型推理，需要通过对应的脚本进行操作，使用命令 `./llama-qwen2vl-cli`。该脚本的相对路径为：`/llama.cpp/build/bin`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650e656-dbcf-4361-8eb6-17eae1a2078a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224103842046.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c8219-2cf1-4851-b36f-dcf96e2f5900",
   "metadata": {},
   "source": [
    "通过在命令行执行以下代码可以实现本地量化qwen2vl模型实现多模态推理，该操作在脚本所在文件夹下进行执行，其中`./llama-qwen2vl-cli`为启动量化模型的脚本， `-m`后接第一步进行量化得到的权重模型，`--mmproj`后接视觉编码模型的文件地址，`-p`后接需要输入的prompt文字信息，`--image`后输入图片信息的文件地址。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5133782-f880-423c-889f-eced48d97422",
   "metadata": {},
   "source": [
    "./llama-qwen2vl-cli -m /home/LLM/qwen2vl-gguf/qwen2vl2b.gguf --mmproj qwen2vl-vision.gguf -p \"Describe this image in Chinese.\" --image \"/home/LLM/llama3_2_v/PiKaQ.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873fecbe-d723-407f-9619-7ccc6abe6375",
   "metadata": {},
   "source": [
    "- 世界知识认知测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d872a3-8df1-48ea-a0ca-05dce5833d64",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241225184801795.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553eaa0f-422f-463a-a03c-eb65ffbf6332",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224155905066.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ebe4f-b0cd-4adc-a20e-86a313a48893",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241120182031825.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150cdec-054b-4766-9f90-046cb0be75a5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224164348742.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b499014-7bb7-4a0e-8bd9-98cec7a9807f",
   "metadata": {},
   "source": [
    "- OCR光学字符识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1a23a-55e1-4df8-b7cd-05f6115e4d53",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241120182922372.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4b2e6-b59b-4156-9c19-04984874aefe",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224155320827.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb65fa-f828-4a36-b26b-ba7d8f0d3725",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241227145321286.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29d0f4-b990-43cb-82a9-16ac1a60896c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224164553941.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2343f-7217-46db-a5b3-7c40d40c3434",
   "metadata": {},
   "source": [
    "本流程使用的是qwen2vl：2b量化后的模型，由以上的测试结果来看，虽然是小参数模型的量化版本，其视觉功能基本没有受到影响，运行起来消耗显存大约为1G，是轻量化部署模型（如端侧部署）的极佳选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f5d84-3b0f-4c37-a6a6-565a2d46f9a6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241224155352059.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d15f2-22ec-46c7-b9fb-dcd653640739",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df7084-0586-4657-af2f-4d18442bbd31",
   "metadata": {},
   "source": [
    "```\n",
    "【赋范大模型技术社区】成立啦～海量硬核独家技术干货内容+无门槛技术交流，下图扫码加【助教老师】\n",
    "即可入群，社区包含：\n",
    "✅入门必备Python基础、账号注册与硬件配置方法；\n",
    "✅20+主流开源&在线大模型部署与调用方法；\n",
    "✅独家团队自研高品质技术教程&追更最新技术内容；\n",
    "✅社区交流：活跃技术氛围，技术交流&答疑；\n",
    "✅新知速递：大模型重大技术突破&最新技术信息通报；\n",
    "✅干货分享：每月2-3场硬核干货&前沿技术公开课。\n",
    "感兴趣的小伙伴抓紧时间扫码入群吧～\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbb4ad-e020-4805-b7a0-3f03fe00c195",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241225191001711.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55de6c-70f1-4cae-9a78-554e03ded630",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8303a-b936-441a-8741-11858107fec3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
