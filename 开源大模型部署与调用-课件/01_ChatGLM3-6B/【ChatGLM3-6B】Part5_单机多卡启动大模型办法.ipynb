{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14218a7-8ba2-4997-8989-df808c5b5922",
   "metadata": {},
   "source": [
    "# <center>单机多卡启动大模型办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca504362-8bb2-46ce-a80c-cbab76cdbd37",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc0a3c-8900-4cd1-b006-ee0a31d00489",
   "metadata": {},
   "source": [
    "前言："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca4ad6-ffd3-46fa-9741-369082a9bf2a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论是在单机单卡（一台机器上只有一块GPU）还是单机多卡（一台机器上有多块GPU）的硬件配置上启动ChatGLM3-6B模型，其前置环境配置和项目文件是相同的。如果大家对配置过程还不熟悉，建议回看前两节课程内容学习进行初始配置、文件部署和运行；如果跟着上一节课件实践过单机单卡的操作流程，我们建议针对本期内容的单机多卡也设置创建一个**新的虚拟环境。**这样做可以有效避免版本冲突和依赖问题，确保多卡环境具备专门优化的、独立于单卡环境的配置，简化项目的维护和调试过程。\n",
    "\n",
    "&emsp;&emsp;在实际的生产开发中，一个独立项目对应一个单独的隔离环境是一个比较标准的做法，也建议大家以后在做项目开发的时候遵循。具体的部署流程如下：\n",
    "1. 更新Conda\n",
    "2. 升级Conda到最新版本并检验\n",
    "3. 使用Conda更新软件包\n",
    "4. 创建独立的虚拟环境并进入\n",
    "5. 安装Pytorch并检验\n",
    "6. 下载大模型的项目文件\n",
    "7. 安装大模型运行所需的环境依赖\n",
    "8. 下载大模型的权重文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00ebde-ea35-4995-8e37-d07d58dc826b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本课件为单机多卡情况提供的部署指南，执行多卡环境下ChatGLM3-6B模型的启动步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe005c-e06d-463b-b406-973f95bc8aa4",
   "metadata": {},
   "source": [
    "# 创建独立虚拟环境以部署运行多卡 ChatGLM3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0dfb5-478e-4c51-9633-e1529fe2b5c2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们创建一个新的虚拟环境用来进行多卡的部署启动，以避免与现有环境中的包产生冲突，通过以下命令创建新的环境（此处设置名称为`chatglm3_multi`）,部署的完整流程如有不熟悉可以翻看上节内容：\n",
    " ```bash\n",
    " conda create --name chatglm3_multi python=3.11\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8efe11-3d59-47bb-8f97-698079f947f6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401081829490.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33022295-edf2-43c8-b7ec-9d63a5e874f8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用 `conda activate + 指定虚拟环境名称` 的方式，进入该虚拟环境。如果命令行最前面的括号中显示该虚拟环境，说明进入成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b292b0b-160d-4e92-8778-8e13eb66afc2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111045323.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955375b-630f-4f0a-ba2a-7c7cb138b29a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除此之外，大家一定要注意：如果使用远程连接，关闭了当前终端，或者是重启了电脑等情况后，再次启动ChatGLM3-6B模型服务时，需要先进入这个虚拟环境。进入指定的虚拟环境方法如下：通过 `conda env list`指令可以显示全部创建的虚拟环境，`conda activate +虚拟环境名称`就可以激活对应的虚拟环境了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b2243-a918-4b0a-8ace-dcd4338f7f7c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111048882.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cff595-7b12-40cb-89e2-7faecd074cbc",
   "metadata": {},
   "source": [
    "# 单机多卡启动ChatGLM3-6b模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82450ada-e399-446a-97c6-5e77c928995d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;单机多卡（多个 GPU）环境相较于单机单卡（一个 GPU），可以提供更高的计算能力，但同时也会存在更复杂的资源管理和更复杂的程序代码。比如我们需要考虑如何使所有的 GPU 的负载均衡，如果某个 GPU 负载过重，而其他 GPU 空闲，这会导致资源浪费和性能瓶颈，除此之外，还要考虑每个 GPU 的内存不会被过度使用及模型训练过程中GPU 之间的同步和通信。\n",
    "\n",
    "&emsp;&emsp;尽管如此，单机多卡或者多机多卡往往才是工业界实际使用的方式，单机单卡的瓶颈非常有限，所以这方面的内容还是非常有必要掌握的。而如果初次接触，我们需要做的就是：学会有效的使用简单的GPU监控工具来帮助配置一些重要的超参数，例如批大小（batch size），像出现 GPU 内存溢出（即显存不足）等情况，去考虑减小批大小等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb1313-b9ce-4334-80f6-445bf9da1356",
   "metadata": {},
   "source": [
    "## 1. 如何查看机器的GPU数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ec941-2fbd-410f-b306-3f16759b1bd1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;方式一：lspci 命令。这是最常用的方法之一，这个命令会显示与图形相关的设备信息，列出所有 PCI 设备，包括 GPU，其执行命令如下：\n",
    "```\n",
    "lspci | grep VGA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a6456-a6ab-41ae-9325-819dcf4ce0a8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111124833.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0efb5-5e7f-4e82-85ab-8c0ae3ec4433",
   "metadata": {},
   "source": [
    "&emsp;&emsp;方式二：如果系统中安装的是 NVIDIA GPU 和驱动程序，最熟知且最直观的`nvidia-smi` 命令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7867842-34a3-4dfb-808a-78cee32908f9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091054969.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3ea11-0a00-4cc1-95fc-949bbe9e7baa",
   "metadata": {},
   "source": [
    "## 2. GPU性能参数详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ab0f7-999c-420a-bd81-403a4aba79cf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;参数很多，如何理解各个数值的意义及需要关注哪些信息呢？我们首先来看上半部分的输出："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a245b-69bd-4956-93ee-4337790fc40c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091057079.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d1b2f-1017-4af0-9143-5e317bb46900",
   "metadata": {},
   "source": [
    "在这部分我们需要主要关注的信息有\n",
    ">  持续模式（Persistence-M）：开启状态时会使得GPU保持高性能状态，即使没有活跃的工作负载， 这种模式可以减少GPU从低功耗状态切换到高性能状态所需的延迟。 通常用于需要快速GPU响应的场景，如服务器环境或某些专业应用。 但是，启用此模式会增加功耗，因为GPU始终保持在较高的能耗状态。这里显示的是off的状态。这对于大多数普通用户和工作站来说是理想的设置，因为它可以在需要时提供性能，同时在空闲时节省能源。\n",
    "\n",
    "> 性能状态（P-state）：P8是最低功耗状态之一，通常在GPU空闲或负载很低时使用。P-States范围通常从P0到P12，P0表示最大性能，P12表示状态最小性能。在较高数位的状态下，GPU核心频率和内存频率会大幅降低，以节省能源。 这种状态通常在显卡没有执行任何重要任务时启用，比如桌面显示或轻量级应用程序运行时。如果需要使用GPU的全部性能，例如进行游戏或深度学习任务，GPU会自动切换到更高的性能状态（如P0或P2）\n",
    "\n",
    "> 显存利用情况（Memory-Usage）：运行任何程序都要消耗显存资源，当显存用尽的时候就会停止运行进程（俗称显存爆了），这时候就需要关闭一些程序的运行或重启程序，否则无法继续进行模型的训练或推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59b292-aef7-4e37-b8cf-2533f259aa36",
   "metadata": {},
   "source": [
    "&emsp;&emsp;再来看下半部分的输出："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea782b-447f-4be8-9ca6-78eb2167ffdf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091057078.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86edf28d-b6dc-455b-b2f4-73ed11513e1b",
   "metadata": {},
   "source": [
    "在这张图中，\"G\" 代表 \"General\"（通用）。这是指进程的类型。，除此以外，通常还会有其他类型指标。虽然图中只显示了 \"G\" 类型，但基于常见的系统监控工具和实践，以下介绍一些其他可能的类型指标：\n",
    "\n",
    "1. C - Compute：表示计算密集型进程，通常与 GPU 计算任务相关。\n",
    "2. M - Memory：指内存密集型进程，可能会大量使用 GPU 内存。\n",
    "3. E - Encode：表示编码进程，通常与视频编码任务相关。\n",
    "4. D - Decode：表示解码进程，通常与视频解码任务相关。\n",
    "5. N - Network：可能表示网络相关的 GPU 加速进程。\n",
    "6. I - I/O：表示输入/输出密集型进程。\n",
    "7. K - Kernel：可能表示 GPU 内核或驱动相关的进程。\n",
    "8. S - System：系统级进程。\n",
    "9. U - User：用户级进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014dcf7-59d4-4a48-a121-02d626f0645a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除了直接运行 nvidia-smi 命令之外，还可以加一些参数，来查看一些本机 GPU 使用的实时情况，这种方式也是在执行训练过程中最简单直观且比较常用的一种监测方式，执行命令如下：\n",
    "```bash\n",
    "watch -n 1 nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ea4a3-e849-45a7-aefb-306ecaa7df7c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091123930.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af984c48-c234-4ccb-aba0-330f6903ddfe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`-n` 参数可以自己灵活调整，后面添加的数字就是以秒为单位执行一次刷新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab200492-8f04-47a3-8ea4-65ef0bae61df",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240827172749111.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ceddbd-5794-44b8-9e79-b077f48d7d81",
   "metadata": {},
   "source": [
    "## 3. 多卡启动办法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6c6fd-5009-45a2-9fe0-89f9e5cc0539",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在 Linux 系统中想要在多GPU环境下启动一个应用服务，并且指定使用某些特定的GPU，主要有两种方式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f312edd-3991-4ce0-a921-9e96e5d5a8e0",
   "metadata": {},
   "source": [
    "1. **CUDA_VISIBLE_DEVICES环境变量**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc9568-3874-490e-8a8b-c810d08b9516",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用`CUDA_VISIBLE_DEVICES`环境变量是最常用的方法之一。这个环境变量可以控制哪些GPU对CUDA程序可见。例如，如果系统有4个GPU（编号为0, 1, 2, 3），而你只想使用编号为1和2的GPU，那么可以在命令行中这样设置：\n",
    "   ```bash\n",
    "   CUDA_VISIBLE_DEVICES=1,2 python your_script.py\n",
    "   ```\n",
    "&emsp;&emsp;这会让`your_script.py`只看到并使用编号为1和2的GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4f75a-dde9-4991-b3d2-d889de73d171",
   "metadata": {},
   "source": [
    "2. **修改程序代码**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f33e75-d406-4f39-abd8-060421667fc9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这种方式需要直接在代码中设置CUDA设备。例如，在PyTorch中，可以使用`torch.cuda.set_device()`函数来指定使用哪个GPU，除此之外，某些框架或工具（如vLLM或Ollma）提供也可能提供相关的参数或环境变量来控制GPU的使用，但都需要修改相关的启动代码。\n",
    "\n",
    "&emsp;&emsp;选择哪种方法取决于具体需求和使用的框架或工具。通常，`CUDA_VISIBLE_DEVICES`是最简单和最直接的方式，而且它不需要修改代码，这使得它在不同环境和不同应用程序之间非常灵活。如果有控制多个服务并且每个服务需要使用不同GPU的需求，那么需要根据具体情况结合使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd7401-318d-4572-888e-1dc04c03da98",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们依次尝试上述两种方式来启动ChatGLM3-6B模型服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd869f-f9d8-4e89-9b95-5d0a672871c8",
   "metadata": {},
   "source": [
    "- **根据GPU数量自动进行分布式启动**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545f0c0-4d42-4b39-bd15-6c79559aeba6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们以命令行的交互方式来进行多卡启动测试。官方提供的脚本名称是：cli_demo.py。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af060d06-6743-4ad1-bd44-47a091a96e30",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240827173214207.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba4c2c-9ada-497a-9260-dac44e823371",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在启动前，仅需要进行一处简单的修改，因为我们已经把ChatGLM3-6B这个模型下载到了本地，所以需要修改一下模型的加载路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41ffc8-e0c8-43d6-a818-13e84ae406bf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401081829506.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9de14-6752-420b-871d-8397e6a874f6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果仅修改模型权重就执行启动，该过程会自动检测可用的 GPU 并将模型的不同部分映射到这些 GPU 上。状态如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44714eb7-b6a6-43f4-be1b-a3d926d94cd8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240829161838544.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef59f8-b878-411d-812c-61fe8f439ca1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里输入`Stop` 退出启动程序，GPU资源就会立即被释放。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8aead-41bb-4e7e-b1fe-b9d0b5024e3c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240829161958242.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4854c-c3cb-41b2-abb3-b1476cc44b98",
   "metadata": {},
   "source": [
    "&emsp;&emsp;默认启动会自动使用多块GPU的资源的原因，在于`cli_demo.py`这个.py文件中的这行代码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6a286-b8d1-451a-a2ac-94ce7a76a08b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091140936.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7eeb2-bf6c-4082-9b28-7f2d899bf76a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;参数`device_map=\"auto\"`, 这个参数指示 transformers 库自动检测可用的 GPU 并将模型的不同部分映射到这些 GPU 上。如果机器上有多个 GPU，模型会尝试在这些 GPU 上进行分布式处理。其通过分析各个 GPU 的当前负载和能力来完成。负载均衡的目标是最大化所有GPU的利用率，避免任何一个GPU过载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d2909-fc69-431e-8e9d-9099016327e0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091140937.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354b375-4285-4569-a54a-e1089ec63262",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以通过如下代码，查看当前环境下的GPU情况：\n",
    "```bash\n",
    "import torch\n",
    "\n",
    "# 检查 CUDA 是否可用\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# 列出所有可用的 GPU\n",
    "if cuda_available:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # 获取当前默认 GPU\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b0d53-1337-4bd0-b3a8-77f7711cd80f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以把上述代码写在一个.py文件中，执行该文件后会输出当前机器上的GPU资源情况，方便我们对当前的资源情况有一个比较清晰的认知。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f26e58-ff99-4901-87fd-99913fbef16b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111143697.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12029857-0c59-42a1-8494-cdb18485e9ff",
   "metadata": {},
   "source": [
    "- **在代码程序中指定某几块GPU加载服务**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53b32b-2320-4cc8-98f5-4cbe106cf7ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果想要指定使用某一块GPU，那么需要这样修改代码`cli_demo.py`中的代码：\n",
    "```bash\n",
    "import torch\n",
    "\n",
    "# 设置 GPU 设备\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map=\"auto\").eval()\n",
    "model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True).eval()\n",
    "\n",
    "# 将模型移到指定的 GPU\n",
    "model = model.to(device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cb5c7-6622-46aa-a3bc-c73bae7c34f6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091414900.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90aabc6-83cc-476b-9969-74fbad542d56",
   "metadata": {},
   "source": [
    "&emsp;&emsp;修改后看下启动情况："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba672e-ef85-4053-9df2-45c6fa9b974d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091412582.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def58ad-b58b-42f8-9e75-4dc28ee84f57",
   "metadata": {},
   "source": [
    "&emsp;&emsp;更多数人的情况是：比如当前机器中有4块GPU，我们只想使用前两块GPU做此次任务的加载，该如何选择呢？这很常见，其问题主要在于：如果某块GPU已经处于满载运行当中，这时我们再使用四块默认同时运行的话大概率会提示out of memory报错，或者提示显卡不平衡imblance的warning警告。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83db11d-2073-4719-9266-9158986e4d63",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果是想在代码中指定多块卡运行该服务，需要在代码中添加这两行代码：\n",
    "```bash\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, [0,1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9f96d-97a2-4744-b02c-00f27826470c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111209936.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301614ed-95d1-44a7-9d89-69a4ee2bfc49",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后保存修改后，执行启动过程就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f45f5-53f0-4bd8-8745-abc0889846e8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111205163.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499c198-510b-4781-b585-903aa9caed13",
   "metadata": {},
   "source": [
    "- **直接使用CUDA_VISIBLE_DEVICES环境变量启动**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff5c34-6636-42e2-a4c3-0a7c72d7d714",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第二种方法就是设置 CUDA 设备环境变量。这个方法非常简单，且不涉及更改Python代码。只需要在运行 Python 脚本之前，在命令行中设置 CUDA_VISIBLE_DEVICES 环境变量。这个环境变量告诉 PyTorch 使用哪个 GPU。例如，如果想使用第二块 GPU（GPU 编号从 0 开始），就可以这样启动程序：\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=1 python cli_demo.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11c747-e1dc-4c68-8eba-d4d2e7b9b6ee",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091454025.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63078f74-96cc-4fc2-9429-8f06376f7176",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果想使用两块 GPU启动，那么可以使用逗号`，`来进行分割。\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1 python cli_demo.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad94b59-1d04-4207-b204-2a7d7c00f291",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401091454026.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125720ba-83e9-4250-b52a-4645a60cfe5b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同样的，显卡的功率消耗也会随着推理进行运算而增长。输入`stop`指令后功耗复原。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfc786-4877-418c-9632-2a884e64885c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240829162351055.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84c029-b0f9-4e6e-a4ce-8c1145621842",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
