{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32dcaff8-3e98-4aa9-8b09-bfe83df11cb5",
   "metadata": {},
   "source": [
    "# <center>PEFT主流高效微调方法介绍&实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78d814-048e-4a9f-8b1e-fbb00314c8de",
   "metadata": {},
   "source": [
    "前言\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a59cc-5a1a-4e0c-8444-1bb267a8f8dc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;微调是一种在特定于特定任务的新数据集上进一步训练（或微调）预训练模型的方法。该技术涉及根据新数据调整模型所有层的权重。它允许模型专门满足细微的任务，并且通常会为专业应用程序带来更高的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd432962-da2f-477f-b241-d624be0b0241",
   "metadata": {},
   "source": [
    "目录\n",
    "\n",
    "一 主流微调方法<br>\n",
    "&emsp;1 Full Fine-Tuning<br>\n",
    "&emsp;2 Prefix-Tuning<br>\n",
    "&emsp;3 Prompt Tuning<br>\n",
    "&emsp;4 P-Tuning v1<br>\n",
    "&emsp;5 P-Tuning v2<br>\n",
    "&emsp;6 LoRA & QLoRA<br>\n",
    "二 DeepSpeed原理浅析<br>\n",
    "&emsp;1 ZeRO-Offload<br>\n",
    "&emsp;2 ZeRO-Infinity<br>\n",
    "&emsp;3 ZeRO-1、2、3<br>\n",
    "三 基于ChatGLM3-6b进行P-Tuning v2微调实战<br>\n",
    "&emsp;1 微调环境准备<br>\n",
    "&emsp;2 微调数据下载与预处理<br>\n",
    "&emsp;3 执行P-Tuning v2微调<br>\n",
    "&emsp;4 使用微调后的模型<br>\n",
    "&emsp;5 微调效果评测<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf07914-dc49-4143-aedd-8dd27da33f98",
   "metadata": {},
   "source": [
    "## 微调是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a197a-dd10-4e3a-95f0-727c10370fb7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;微调大型语言模型 （LLM） 涉及在特定数据集上调整预训练模型，以提高特定任务的性能。此过程在一般预训练结束后开始。用户为模型提供更集中的数据集，其中可能包括特定于行业的术语或以任务为中心的交互，目的是帮助模型为特定使用案例生成更相关的响应。\n",
    "\n",
    "&emsp;&emsp;微调允许模型调整其预先存在的权重和偏差，以更好地适应特定问题。这提高了输出的准确性和相关性，使 LLM 在实际、专业的应用中比受过广泛训练的同类产品更有效。虽然微调可能是高度计算密集型的，但参数高效微调 （PEFT） 等新技术使其效率更高，甚至可以在消费级显卡上运行。微调既可以在开源 LLM（如 Meta LLaMA 和 Mistral 模型）上执行，也可以在某些商业 LLM 上执行，前提是模型的开发人员提供了此功能。例如，OpenAI 允许对 GPT-3.5 和 GPT-4 进行线上微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd04b5-b0bd-461d-a90a-eed3a723637b",
   "metadata": {},
   "source": [
    "- 微调方法分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4d775-cc25-4260-aac6-f5dad39d0dac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在微调的方法上分类可分为全参数微调（Full Fine-Tuning）和部分参数微调（Partial Fine-Tuning），其中全参数微调指的是对整个模型进行微调，该预训练模型的所有层和参数都会被更新和优化。全参数微调的方法适用于任务和预训练模型之间存在较大差异的情况，或者任务需要模型具有高度灵活性和自适应能力的情况,但是这种方法会消耗大量的计算资源和时间，对成本有较高的要求。部分微调指的是在微调过程中只更新模型的顶层或少数几层，其余的层和权重固定不变。这样可以节省大量的计算资源，但是效果会随着数据量的大小、与原模型的相似程度以及微调方法而定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ad305-6399-4e8a-8bcc-33a2741cf74e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240911173540628.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce8843-1a55-4541-b8af-e8b8007cbc16",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PEFT （Parameter-Efficient Fine-Tuning）是部分微调的一种具体实现方式，它通过引入不同的技术，进一步提高了微调效率，通常在大语言模型（如 GPT、BERT 等）的微调中广泛应用。PEFT 的目标是在保持模型性能的前提下，极大减少需要微调的参数量，避免了对整个模型进行大规模训练，从而降低资源消耗。同时，也能实现与全量参数微调相当的性能。参数高效微调方法甚至在某些情况下比全量微调效果更好，可以更好地泛化到域外场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11be9b-dc1c-43b8-813a-8e5d74776eea",
   "metadata": {},
   "source": [
    "- 使用高效微调方法的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53215ae3-82fc-4fc4-aa42-09fa593075a3",
   "metadata": {},
   "source": [
    "1. 大模型参数量过大，训练成本高。\n",
    "2. 通过自有数据提升大模型在特定领域的能力。\n",
    "3. 在特定服务中使用大模型的能力。\n",
    "4. 保护企业数据安全。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be74ecd-9091-4cf0-874c-d7011105e1e4",
   "metadata": {},
   "source": [
    "## 一、主流微调方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd740e-37ae-4cb9-8bf3-48dd692452b3",
   "metadata": {},
   "source": [
    "### 1. Prefix-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141b28a-1161-41c2-b9f2-e83a181d3e5f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Prefix-Tuning指的是在微调模型的过程中固定语言模型的参数，仅优化一个小的连续的特定任务向量(称为prefix)。prefix-tuning在充分数据下获得了与微调相当的性能，在小数据集上优于微调。以下是 Prefix-Tuning论文地址："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642c8b9-b37a-474c-b332-7f665c97307e",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2101.00190\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14729a1-68f9-4b10-bbed-9908dbae0e29",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在Prefix Tuning方法提出之前，相关工作主要集中在手动设计离散模板或自动化搜索离散模板。手动设计的模板对模型的最终性能极其敏感，哪怕仅仅增加、删除一个词或调整词的位置，都会对结果产生显著影响。而自动化搜索离散模板的过程通常成本较高，且搜索出的离散token组合可能并非最优解。此外，传统的微调范式要求针对每个下游任务对预训练模型进行独立微调，并保存相应的微调后模型权重，这不仅增加了存储空间的占用，也导致微调过程耗时较长。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba5ee0-27f5-4235-9951-69f1d97b3268",
   "metadata": {},
   "source": [
    "&emsp;&emsp;传统的微调范式Fine-turning会利用预训练模型去对不同的下游任务进行微调，对每个任务都要保存一份微调后的模型权重。比如下图展示的三个不同任务的Transformer模型，分别用来做翻译、摘要和将格式转化（table-to-text）。每个任务都有自己的微调模型，这意味着模型的所有权重都在微调过程中针对特定任务进行了更新。这种方法通常需要大量的数据和计算资源，因为整个模型都在学习任务特定的知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0205b89-5089-4252-a824-ab0909c79d81",
   "metadata": {},
   "source": [
    "&emsp;&emsp;基于上述两个因素，Prefix-tuning 就提出了一种不同的微调策略，对基于Transformers结构的模型，它会将特定的前缀添加到输入序列的开始部分，相当于任务特定的提示，可以是一组固定的词或是可训练的嵌入向量。这样做的效果相较于传统的离散token微调范式所消耗的成本更小，优化效果更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3f7f0-8e12-408c-82c4-68b91d691af5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240912112745010.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e1d40-d9cd-4f62-9522-a4da88234028",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示，并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。但是这个Prefix 并不是一些明确的单词，比如对于文本摘要任务来说，我们添加 this is summarization（明确指出这是一个摘要的任务），相反，这个prefix加的是一些隐式的Token。这里就需要了解两个概念：\n",
    "\n",
    "- Hard Prompt：也称离散Prompt，是一个实际的文本字符串（自然语言，人工可读），通常由中文或英文词汇组成；\n",
    "- Soft Prompt：也称连续Prompt，通常是在向量空间优化出来的提示，通过梯度搜索之类的方式进行优化；\n",
    "\n",
    "&emsp;&emsp;在Hoft Promot中，提示语的变化对模型最终的性能特别敏感，加一个词、少一个词或者变动位置都会造成比较大的变化。成本比较高，并且效果不太好。显然：Prefix Tuning属于Soft prompt。也就是我们学习调整的就是这部分的参数，从而达到微调的目的。与调整约3.6%LM参数的工作相比，Prefix Tuning的方法进一步减少了30倍的任务特定参数，仅调整了0.1%，同时保持表到文本任务的可比性能。\n",
    "\n",
    "&emsp;&emsp;在Soft Prompt中，LM 的模型权重被冻结，并且有单独的可学习张量与模型权重连接，并针对特定的下游任务进行训练。我们希望以最佳方式学习能够给我们带来最佳结果的提示。所有方法都适用于足够小的标记数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b20d2e-02bf-4a05-b76c-2ede38018459",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401101357838.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206c73b-2414-4d7f-adea-8fbfc596fbb4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Prefix Tuning针对不同的模型结构有设计不同的模式，以自回归的模型为例，不再使用token去作为前缀，而是直接使用参数作为前缀（粉红色部分），并只优化这部分，冻结主体部分的参数。同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40f530-ccdc-4313-9a81-d03bd4e64763",
   "metadata": {},
   "source": [
    "&emsp;&emsp;简单的说，Prefix Tuning训练一个上游前缀，它引导一个未修改的LM，因此，单个LM可以同时支持许多任务。在个性化的上下文中，任务对应于用户。Prefix Tuning将为每个用户设置一个单独的前缀，只训练该用户的数据，从而避免数据交叉污染。此外，基于前缀的体系结构使Prefix Tuning能够在单个批处理中处理来自多个用户/任务的示例，这是其他轻量级微调方法不可能实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2355d7-8ed9-4eaf-a044-964344d461f2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401101357839.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d765e-8a37-466c-b8cb-28b80983e014",
   "metadata": {},
   "source": [
    "> Encoder端增加前缀是为了引导输入部分的编码，Decoder 端增加前缀是为了引导后续token的生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103a47c-5fd5-4980-8026-4e3d0638a648",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Prefix-tuning 的优势在于它不需要调整模型的全部权重，而是通过在输入中添加前缀来调整模型的行为，这样可以节省大量的计算资源，同时使得一个单一的模型能够适应多种不同的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad6a34-d5ce-4fa3-848a-a3e3a6a40002",
   "metadata": {},
   "source": [
    "### 2. Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df639af-7ea6-4cee-829f-53b316e47629",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Prompt Tuning 方法可以看做是Prefix Tuning的简化版本，它给每个任务都定义了自己的Prompt，将真实的Tokens转化为可微的virtual token，并加入人工设计的锚字符（与任务高度相关的Token），拼接到数据上作为输出，但只在输入层加入Prompt tokens。它使用手动设计的提示，在few shot设置中适应不同的任务。以下是Prompt Tuning论文地址："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee7e6d-50d6-40b8-b89d-7844641660db",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2104.08691.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f67b6-2754-422f-8b65-901efdee163b",
   "metadata": {},
   "source": [
    "Prompt Tuning通过在输入中添加特定的提示（prompt）来引导模型生成所需的输出。这些提示是可学习的，并在微调过程中进行优化，以提高模型在特定任务上的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e7df8-fdb7-4941-b52b-be6c8375f522",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401101409787.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ca17d-72b5-44a5-afd8-8be0869ff4bb",
   "metadata": {},
   "source": [
    "如图所示，传统的微调方法需要为每个下游任务创建整个预训练模型的任务特定副本，并且推理必须在不同的批次中进行。Prompt Tuning只需要为每个任务存储一个较小的任务特定提示，同时可以使用原始的预训练模型进行混合任务推理。\n",
    "Prompt Tuning可以通过反向传播更新参数来学习prompts，而不是人工设计prompts；同时冻结模型原始权重，只训练prompts参数，训练完以后，用同一个模型可以做多任务推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b922a9c-085f-4a24-9b31-a1b8fa8d2a29",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240912115952993.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35cad29-6305-49cc-ac16-f185575c3b69",
   "metadata": {},
   "source": [
    "实验表明，随着大模型的参数量的增加，Prompt Tuning的方法会逼近全参数微调的结果。此外，Prompt token 的长度在20左右时的表现已经不错（超过20之后，提升Prompt token长度，对模型的性能提升不明显了），同样的，这个gap也会随着模型参数规模的提升而减小（即对于超大规模模型而言，即使 Prompt token 长度很短，对性能也不会有太大的影响）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d2d18-6bd4-4e95-9722-b5df06a3a4d8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;虽然Prompt Tuning和Prefix Tuning都涉及在输入数据中加入可学习的向量，但两者的策略和目的不一样：\n",
    "\n",
    "- Prompt Tuning：可学习向量（通常称为prompt tokens）旨在模仿自然语言提示的形式，它们被设计为引导模型针对特定任务生成特定类型的输出。这些向量通常被看作是任务指导信息的一部分，倾向于用更少量的向量模仿传统的自然语言提示。\n",
    "\n",
    "- Prefix Tuning：可学习前缀Prefix更多地用于提供输入数据的直接上下文信息，这些前缀作为模型内部表示的一部分，可以影响整个模型的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9c6b6-72f1-4f99-9130-fde1dd8eb861",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下面的训练例子说明了两者的区别："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82df4d1-4b59-4f2f-a444-dfd9d1e06179",
   "metadata": {},
   "source": [
    "```\n",
    "Prompt Tuning示例：\n",
    "\n",
    "输入序列: \"Prompt 1, Prompt2 | 这部电影令人振奋。\"\n",
    "\n",
    "问题: 评价这部电影的情感倾向。\n",
    "\n",
    "答案: 模型需要预测情感倾向（例如“积极”）\n",
    "\n",
    "提示: 无明确的外部提示，\n",
    " \n",
    "充当引导模型的内部提示，因为这里的问题是隐含的，即判断文本中表达的情感倾向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18a558-2fb6-4bec-8152-de2a1d895a18",
   "metadata": {},
   "source": [
    "```\n",
    "Prefix Tuning 示例：\n",
    "\n",
    "输入序列: \" Prefix1, Prefix 2 | I want to watch a movie.\"\n",
    "\n",
    "问题: 根据前缀生成后续的自然语言文本。\n",
    "\n",
    "答案: 模型生成的文本，如“that is exciting and fun.”\n",
    "\n",
    "提示: 前缀本身提供上下文信息，没有单独的外部提示\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db06d4-a597-4d3a-b056-10dae354b751",
   "metadata": {},
   "source": [
    "### 3. P-Tuning v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ab8c3-4316-4b82-81df-8345d76471b4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;P-Turning V1的核心是使用可微的virtual token（虚拟标记）替换了原来的discrete tokens（离散标记），且仅加入到输入层，并使用prompt encoder（提示编码器）对virtual token进行编码学习。以下是 P-Tuning v1 论文地址:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b0508-4e20-40b2-a7c2-c124bbc39377",
   "metadata": {},
   "source": [
    " https://arxiv.org/pdf/2103.10385.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df8c09-65c6-4e91-b0f8-2727a5cd4ea6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;P-tuning 将提示视为一组可学习的参数，这些参数通过反向传播进行更新。此方法与上述两种方法不同，更符合提示优化，但提示是向量而不是离散提示。使用提示编码器，可以是 LSTM 或多层感知器。\n",
    "\n",
    "&emsp;&emsp;Prompt Tuning更新使用的参数是静态的、可训练的虚拟标记嵌入。这些嵌入在初始化后保持固定，除非在训练过程中被更新，相对简单，因为它只涉及调整一组固定的嵌入参数。在处理多种任务时表现良好，但在处理特别复杂或需要细粒度控制的任务时受限。所以，P-Turining v1 就在输入的句子中也是加入了隐式的 virtual token，区别就是：前面的方式是直接对它进行一个学习更新，只不过不会更新大模型中的参数，只是更新我们加入的 virtual token这样一个参数，P-Turning v1 是对添加的virtual Token，又使用BiLSTM + MLP（双向长短记忆网络+多层感知机） 对其进行了一个编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476d79e-d85d-448f-9298-ea41564143f3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401101421846.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6db2a-86b8-4caf-ad8f-b358672e8c07",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240912145527195.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbd332-179e-4905-a022-3faa6b1fe161",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如上图所示的例子：提示搜索“英国的首都是[ __ ]”的一个例子。鉴于上下文（蓝色区域“英国”）和目标（红色区域“面具”），橙色区域指提示模型完成任务的关键信息。在(a)离散化提示工程方法中，提示生成器只接收离散的奖励；相反，在(b)P-tuning中，连续的提示嵌入和提示编码器可以以一种可微的方式进行优化。\n",
    "\n",
    "P-tuning将每个离散提示转换为模板。\r\n",
    "\r\n",
    "T  {[P0:i]，x，[P(i+1):j]，y，[P(j+1):k]\n",
    "\n",
    "这会将任务转换为查找并填写输入文本中的空白以找到具有最佳结果的提示。P_i 是需要学习的连续提示嵌入，h_i 是模型输入。编码器用于将 P_i 映射h_i}。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f1091-447b-4f31-a966-301bec2db921",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这种方法与提示调整非常相似，因为它们都将LLM保留为黑匣子，并且不向其中添加任何参数。不同之处在于额外的编码器，并允许我们串联离散且可学习的提示。相较于zero-shot、few-shot这种提示工程方法，P-tuining v1调整模型对较小模型的影响最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef86dc-9673-45de-b58b-4b7ac9c92f2a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240912145014912.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d51d07-c7e2-4961-bede-613f079a2cdc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Prompt Tuning和P-Tuning的共同点：\n",
    "- Prefix Tuning是将额外的embedding加在开头，看起来更像是模仿Instruction指令；而P-Tuning的位置则不固定。\n",
    "- Prefix Tuning通过在每个Attention层都加入Prefix Embedding来增加额外的参数，通过MLP来初始化；而P-Tuning只是在输入的时候加入Embedding，并通过LSTM+MLP来初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1cfbb-a9db-4d62-bd61-6d0a6a89b170",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那么Prompt Tuning和P-Tuning等方法存在两个主要的问题：\n",
    "- 缺乏模型参数规模和任务通用性：Prompt Tuning论文中表明当模型规模超过100亿个参数时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（小于1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。\n",
    "\n",
    "- 缺乏任务普遍性：尽管Prompt Tuning和P-tuning在像文本分类、阅读理解等任务中有较好的表现，尤其是在这些任务中通过设计良好的提示可以很好地引导预训练模型，但提示调优对硬序列标记任务（即序列标注需要预测一连串的标签，而非单一的标签）的有效性仍然有限。\n",
    "\n",
    "- 缺少深度提示优化，在Prompt Tuning和P-tuning中，连续提示只被插入transformer第一层的输入embedding序列中，在接下来的transformer层中，插入连续提示的位置的embedding是由之前的transformer层计算出来的，这可能导致两个可能的优化挑战，即由于序列长度的限制可调参数的数量是有限的；输入embedding对模型预测只有相对间接的影响。\n",
    "  \n",
    "- P-tuning v1有两个显著缺点：任务不通用和规模不通用。在一些复杂的自然语言理解NLU任务上效果很差，同时预训练模型的参数量不能过小。\n",
    "\n",
    "&emsp;&emsp;由于序列长度的限制，可调参数的数量是有限的，输入embedding对模型预测只有相对间接的影响。这些问题在P-tuning v2得到了改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a386a-8e8b-4dc6-b05a-ff47db73aee5",
   "metadata": {},
   "source": [
    "### 4. P-Tuning v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1d630-5150-407b-aa75-511f3a05ee90",
   "metadata": {},
   "source": [
    "&emsp;&emsp;\n",
    "P-Tuning v2 构建于 P-Tuning 和 Prefix-Tuning 技术的基础之上，核心优化策略在于引入深度提示编码（Deep Prompt Encoding）和多任务学习（Multi-task Learning）。以下是 P-Tuning v2论文地址:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfdbd4-5df2-40f1-a03d-c30874a5fb68",
   "metadata": {},
   "source": [
    " https://arxiv.org/abs/2110.07602\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a88b0-f5b1-47c1-b964-a148fdf33472",
   "metadata": {},
   "source": [
    "以下是P-tuning v1 和 v2 框架的对比。在右侧的 P-Tuning v2 中，连续提示（continuous prompt）被添加到序列的前端，并且每一层网络都引入了可训练的提示。而在左侧的 v1 模型中，提示仅插入到输入嵌入层，这会导致可训练参数受限于句子的长度。此外，P-Tuning v2 还进行了以下改进：\n",
    "\n",
    "- 移除了 Reparameterization 加速机制，提升了训练效率。\n",
    "- 引入了多任务学习优化策略，先基于多任务数据集对提示进行预训练，再适应特定的下游任务。\n",
    "- 放弃了词汇映射的 Verbalizer，改用 [CLS] 和字符标签，与传统微调方法一致，利用 CLS 或 token 的输出进行自然语言理解（NLU），增强了通用性，并适用于序列标注任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab6c9a-3327-44f2-b21c-42577b6e8a9c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401101432469.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9de96-b9ef-479f-b4ee-52b987778de3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Deep Prompt Encoding：P-Tuning v2在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这带来两个方面的好处：\n",
    "- 更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。\n",
    "- 加入到更深层结构中的Prompt能给模型预测带来更直接的影响。\n",
    "\n",
    "&emsp;&emsp;Multi-task learning：基于多任务数据集的Prompt进行预训练，然后再适配到下游任务。对于pseudo token的continous prompt，随机初始化比较难以优化，因此采用multi-task方法同时训\n",
    "练多个数据集，共享continuous prompts去进行多任务预训练，可以让prompt有比较好的初始化。\n",
    "\n",
    "&emsp;&emsp;Prefix-Tuning v2 引入了动态生成前缀的机制，而不仅仅是使用固定的前缀。这使得前缀可以更好地适应不同的输入和上下文，从而提升模型的灵活性和准确性。P-Tuning v2 通过在不同层之间共享部分前缀参数，减少了模型的总参数量和计算复杂度，从而提高了微调效率，同时减少了过拟合的风险。通过设计多任务前缀，使得一个前缀能够适应多个下游任务。这种方法尤其适合处理跨领域任务，减少了每个任务都需要独立训练前缀的开销。\n",
    "\n",
    "&emsp;&emsp;所以P-Tuning v2是一种在不同规模和任务中都可与微调相媲美的提示方法。P-Tuning v2对从330M到10B的模型显示出一致的改进，并在序列标注等困难的序列任务上以很大的幅度超过了Prompt Tuning和P-Tuning。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef60aa-6bc9-4ad3-b84a-c2c9bc5025ea",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240912154659279.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6252-336f-42a8-a39d-3d4cf2402964",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在Prefix Tuning和P-Tuning V2每一层的transformer的输入的不是上一层的输出而是随机初始化的embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24090b-4229-40b2-85d4-ad78296f9f4a",
   "metadata": {},
   "source": [
    "### 5. LoRA & QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8c4b8-67c2-44a9-9204-c0361cb915ab",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除此之外，还有比较主流的LoRA，QLoRA，其中LoRA的原理已经在上节课内容介绍过（《LORA微调原理&实战》），该方法具有节省计算资源、加快训练速度、保持模型的原有权重、可模块化和拓展的优点。QLoRA是在微调过程中同时进行微调和量化，这意味着微调可以在更小的显存要求下进行。以下是LoRA论文地址：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a9d8c-575c-4bd2-a18a-aaffbc0934d7",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2106.09685.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4032c6b-bed8-4b08-af56-312a43fdee05",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240905110121940.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368de6f8-a92c-4941-92f4-d313bd1dd22e",
   "metadata": {},
   "source": [
    "1. LoRA（Low-Rank Adaptation）\n",
    "基本概念\n",
    "LoRA通过在预训练模型的基础上添加低秩矩阵来适应特定任务，而不是微调整个模型的所有参数。具体来说，LoRA冻结了原模型的权重，只训练新增的低秩矩阵参数，从而显著减少了微调所需的参数量和计算成本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f12e4d-9fd2-47ad-a702-13e1ad55bd45",
   "metadata": {},
   "source": [
    "2. QLoRA（Quantized LoRA）\n",
    "基本概念\n",
    "QLoRA结合了量化技术和LoRA方法，通过对模型参数进行量化处理，进一步减少了存储和计算需求，同时保持了LoRA的低秩适应特性。\n",
    "\n",
    "优点\n",
    "存储高效：通过量化减少了参数存储需求。\n",
    "计算高效：结合LoRA的低秩适应，进一步降低了计算成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3334743-59f1-4c13-b43a-8afe2ab2ecb3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240911183916956.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d17b21-1b3a-4e10-901c-93d9b6c2c975",
   "metadata": {},
   "source": [
    "&emsp;&emsp;出于节约资源的目的，QLoRA可以在微调中将模型参数转换成低精度的数据类型，比如8位或4位，来大幅减少内存使用并加快计算速度。这个原理其实很简单：它将所有32位的可能值映射到一个更小范围的有限值上（例如，8位可以表示256个不同的值）。这一过程可以想象为将高精度的值围绕若干个固定点进行分组，这些固定点代表了其周围的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5b739-2992-4ab2-846c-a821dad7fa59",
   "metadata": {},
   "source": [
    "&emsp;&emsp;QLoRA 的核心优势在于将 4-bit 量化技术 与 LoRA 微调方法相结合。通过量化模型的权重到 4-bit 表示，它能够大幅减少模型的存储和计算需求，而不会显著损害模型的性能。并且QLoRA 能够在压缩的同时保持与全精度模型相当的性能。进而够进一步降低显存需求，提升大模型微调的效率，特别是在处理超大规模模型时具有显著的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71537310-2847-403b-8782-ce5b525e874b",
   "metadata": {},
   "source": [
    "### 6. Adapter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f7590-b838-4252-a2f7-76bb84b5c2cf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;2019年谷歌的研究团队设计了Adapter结构，将其嵌入Transformer的结构里面，在训练时，固定住原来的预训练参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），以下是Adapter Tuning论文地址："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29f5bd-8d98-469d-a5e7-477097bc5276",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1902.00751"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6b8a8-6ef8-44f3-a767-819218ff0923",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240911173851826.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3b301-4407-4998-9d64-bafb1087da0a",
   "metadata": {},
   "source": [
    "他们将 Adapter 设计为这样的结构：\n",
    "\n",
    "&emsp;&emsp;首先是一个 down-project 层将高维度特征映射到低维特征\n",
    "然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征\n",
    "同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8326aa-44f3-4efc-b1fd-36e73f19f643",
   "metadata": {},
   "source": [
    "&emsp;&emsp;与传统的微调方法不同，Adapter Tuning 仅需要更新适配器中的参数，模型的大部分预训练参数保持不变。这大大减少了需要训练和存储的参数数量。因为适配器模块占用的参数量很小，可以为每个任务保存不同的适配器，而不需要保存完整的微调模型。\n",
    "\n",
    "&emsp;&emsp;Adapter Tuning 特别适合那些需要在大规模预训练模型上进行任务适应的场景，例如跨任务、跨领域的应用，或需要频繁更新和切换任务的环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c7f69-8199-4bff-8240-7cf277f83a11",
   "metadata": {},
   "source": [
    "## 二、DeepSpeed原理浅析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bb733-ea95-4134-916c-99bc9312f72b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果大家在尝试训练大型推荐系统或微调像 llama-7B 这样相对较小的LLM的时候 ，可能会遇到令人沮丧的 OOM 错误。\n",
    "为什么 7B 参数模型（假设 fp32 或每个参数权重 4 字节）不适合具有 7B*4=28GB 内存的 GPU？除了参数之外，每个设备还需要访问梯度（与参数大小相同）和优化器状态，例如，如果使用 ADAM 优化器（参数大小的两倍），则需要梯度的一阶和二阶矩。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6b540-3843-4b1a-8cf8-706c4b546536",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在实际训练、微调大模型的时候，由于大模型的参数量很大，其训练效率和所消耗的资源、时间是不可忽视的指标，在实际训练大语言模型的时候一般是要配备多GPU的集群，但实际的机器利用率往往只能达到其最大效率的一半左右。这也就是说，只是一味的堆料并不能有效地带来模型训练效率的提升。同样，即使系统具有更高的吞吐量，也并不能保证所训练出的模型具有更高的精度或更快的收敛速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593dc2d-888c-4991-ae94-b5d2099f3715",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Deepspeed是微软的大规模分布式训练工具。专门用于训练超大模型。其主要解决了训练万亿参数模型的两个基本挑战：显存效率和计算效率。因此，DeepSpeed 可以扩展至在显存中放下最巨大的模型但不会牺牲速度的优势。\n",
    "\n",
    "DeepSpeed已经在Github上的地址：https://github.com/microsoft/DeepSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed649d3-9fb7-4ef3-8dda-c0d1db1960af",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在使用方法上DeepSpeed是一个开源深度学习优化库，专门设计来提高大型模型训练的效率和扩展性。这个库采用了一系列先进技术，如模型并行化、梯度累积、动态精度缩放和混合精度训练等，来实现快速训练。除此之外，DeepSpeed还搭载了一套强大的辅助工具集，涵盖分布式训练管理、内存优化以及模型压缩等功能，帮助开发者更有效地处理和优化大规模的深度学习任务。值得一提的是，DeepSpeed是基于PyTorch构建的，因此对于现有的PyTorch项目，开发者可以轻松地实施迁移。此库已在众多大规模深度学习应用中得到验证，涉及领域包括但不限于语言模型、图像分类和目标检测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2825cda-a952-42fc-aac2-6ca1eb5276a7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240911192834334.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b94ad-97e3-4d92-b37f-1d75e25d384b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;DeepSpeed的参数非常多，最核心的技术就是ZeRO（Zero Redundancy Optimizer ），其中ZeRO-Offload是将一部分计算（参数、梯度、优化器状态）和内存管理任务从 GPU 卸载到 CPU，从而大大降低了 GPU 的内存消耗，允许在较小的 GPU 内存配置下训练大规模模型。其中 ZeRO 可以在当前一代 GPU 集群上训练具有 1000 亿个参数的深度学习模型，吞吐量是当前最佳系统的三到五倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89885f6-55c0-4740-9890-79d384dfe912",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111522917.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd4f1a-305b-4c6a-a2b5-beda9f2c32db",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240912190949270.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dc2bc-b959-4231-90f0-72997063eb12",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ZeRO-Infinity 是 DeepSpeed 的一种扩展技术，专门用于在超大规模模型（如数千亿或万亿参数的模型）上实现高效训练和推理。它构建在 ZeRO (Zero Redundancy Optimizer) 系列优化技术的基础上，通过整合 CPU 内存、NVMe 存储和 GPU 显存，最大限度地扩展训练和推理能力，从而支持更大规模的模型而无需昂贵的硬件升级。\n",
    "- 显存（GPU Memory）：用于前向和反向传播的关键计算，存储当前正在处理的模型参数和激活值。\n",
    "- CPU 内存（Host Memory）：用于存储暂时不需要的梯度、优化器状态和部分模型参数，当 GPU 内存不足时，这些数据可以动态地移到 CPU 内存。\n",
    "- NVMe 存储：当 CPU 内存也不足时，ZeRO-Infinity 可以进一步将数据交换到 NVMe 存储设备，利用高速固态硬盘来扩展模型的训练能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc8c65-c774-4e0a-856c-870b2b91e61a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111531658.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a4807-99c2-4af9-bd07-9de0e71d76cc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除了实现高效推理以外，DeepSpeed还提高了模型的训练速度。不管大模型还是小模型的训练，训练的效率一定是框架需要重点关注的，需要在保证精确性的前提下，保证它快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f97497-43c2-4210-9080-0829165cfa45",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111534986.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bed5ca-50e8-4d59-9f10-50e77e3e6cc8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并行模型："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db96707-5bfb-4ffc-8290-a6c183e176ae",
   "metadata": {},
   "source": [
    "- **数据并行** Data Parallelism（DP）：整个模型会复制到所有GPU上，输入数据会被分割成多个batch到不同的GPU上。也因为每个GPU都在处理不同的数据子集，所以在独立执行前向传播后计算的损失（loss）也会有所不同，接着每个GPU根据其计算出的损失执行反向传播，计算梯度。当所以GPU计算完成后，求平均。这个平均梯度代表了整个数据集上的平均梯度。使用这个平均梯度更新模型的参数。从而确保所有GPU上的模型都保持同步。数据并行性无助于减少每个设备的内存占用：具有超过 10 亿个参数的模型即使在具有 32GB 内存的 GPU 上也会出现内存不足的情况。\n",
    "\n",
    "- **张量并行** Tensor MP：对模型做横向切分，也就是层内的切分，每一层的计算被分割成几个较小的部分，每部分独立在不同的GPU上进行计算。比如最大层是一个MLP层，有非常大的计算，但一张卡放不下，就需要切分成两个小的分别放在两张卡上计算。由于细粒度计算和昂贵的通信，张量并行无法有效地扩展到单个节点之外。模型并行框架经常需要广泛的代码集成，这些代码集成可能是特定于模型架构的。\n",
    " \n",
    "- **流水线并行** Pipeline MP：把模型的不同层分在不同的GPU上，比如12层的模型，前6层分在一个GPU上，后六层分在一个GPU上。像我们常用的Transformer结果，它会分成一个个Block，所以一般不同的Block会分布不同的层中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480ac98-4dd8-46ac-96ba-030c908ff4e4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111623249.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc47b6a-64e2-45fe-bcdb-a78c386d8db2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ZeR0实际上分为三个版本：ZeR0-1只会对优化器状态做切分。ZeR0-2会对优化器状态和梯度做切分。ZeR0-3是对优化器状态、梯度和模型参数做切分,并分布在多个设备或节点上，最大化利用计算资源。\n",
    "- φ：假设有一个模型，这个模型由φ个参数，也就是由φ个浮点数组成的模型，每个参数如果以fp16的形式存放，一个参数是32float，也就是4bit，所以这里就是2φ。\n",
    "- 梯度，同样是2φ的显存占用。\n",
    "- 优化器状态就是K倍的φ，优化器的状态根据实现的形式是不一样的，这里选择12进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5696a-7c97-4cc8-aba1-4919c285ccad",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在Baseline（数据并行模式）中，这120GB显存是每张卡都要占用的，所以现在最大的H100这种80G的显存都放不下。但是使用ZeRO-1则占用31.4G显存，ZeRO-2占用16.6G显存，ZeRO-3占用1.9G显存.\n",
    "\n",
    "ZeRO 消除了内存冗余，并使集群的全部聚合内存容量可用。启用所有三个阶段后，ZeRO 可以在 1024 个 NVIDIA GPU 上训练万亿参数模型。具有Adam  16 位精度优化器的万亿参数模型需要大约 16 TB 的内存来保存优化器状态、梯度和参数。 16TB 除以 1024 就是 16GB，这对于 GPU 来说完全在合理范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4ac12-9d2b-4942-b0d8-5d078c8766f9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111552218.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66921667-be47-4633-85d2-f717406c3854",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在实际计算过程中，GPU1 ~ GPU3 计算显存空间的使用会根据 GPU0 的可使用显存空间来确定，这就造成了一个问题：在显存使用上，GPU0 = GPU1 = GPU2 = GPU3，对 GPU1~GPU3 来说是一种巨大的浪费。而且，这种浪费随着模型精度、参数的增加愈发明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be080ea-78d7-49c8-9feb-3854cebbe954",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/image-20230704120416817.png\"width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c75728-7a7e-4fdd-9e25-af93129dc1e8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;什么是micro_batch_size？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e1376d-6333-4eaa-9536-77048a87cf25",
   "metadata": {},
   "source": [
    "Pipeline会把输入进来的mini_batch 分成设备个 micro_batch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b18f0-53d4-48ae-abad-0324b9f66e42",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111635104.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dbe48-b60e-4e74-9b73-2e03c6dcacae",
   "metadata": {},
   "source": [
    "&emsp;&emsp;理想的计算加载方式应该是将模型加载在每个GPU上，减少模型对单个GPU的占用依赖，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6632a3b-89e8-4145-85b7-4e8914b05a48",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/image-20230704122333555.png\"width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f8a71-9578-43d8-823e-33d1ae6a2195",
   "metadata": {},
   "source": [
    "&emsp;&emsp;DeepSpeed 就是实现这样的加载，结合Deepspeed框架的优化特性，充分发挥每块GPU的计算和显存潜能，从而提高整体的训练效率和资源利用率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c430d5-dd9a-4237-bb4e-7f00c27b27eb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在理解了DeepSpeed原理后，我们尝试进行模型加载并观察其内存消耗情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae37b6-b982-4f07-a2bd-76b686a3fec4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于我们的本地运行环境，如果采用 DeepSpeed 在4块3090上加载ChatGLM3-6B模型，加载情况如下：\n",
    "\n",
    "- 原始模型大小 chatglm-6B FP16 -> 单卡显存 $Mem_{model}=12GB$\n",
    "- 在 $n_{GPU} = 4$ 的情况下采用 zero++ 方式计算过程中，模型会先加载到 内存中，占用内存大小 $Men_{load} =  n_{GPU} * Mem_{model} = 4 * 12 ~= 48GB$\n",
    "- 内存加载完毕后再分布到各个显存上，遵循 “对内存中 $n_{GPU}$ 个模型进行截取，而不是 一个模型进行分割”；\n",
    "- 计算公式如下：\n",
    "\t$$Men_{load}=Mem_{model} * n_{GPU}$$\n",
    "\n",
    "- 加载130B FP16  $n_{GPU} = 4$ 时，$Men_{load}=Mem_{model} * n_GPU=130*2*4=1040GB$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7a94a-a88f-49fb-b347-cbd4ba7e43ce",
   "metadata": {},
   "source": [
    "> 可见，DeepSpeed 的 分布截取 会占用大量重复内存，造成资源上的冲击和浪费，一种优化方法是加载到虚拟内存中作为缓存，对一个模型进行分割而不是逐个截取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc7aa2-f9c8-4eb3-80e4-190c04216f0e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;DeepSpeed+Zero的结合方式就可以实现全参数的微调。当然，使用DeepSpeed进行full finetuning，对于显存要求较高，且训练较慢。但这个无疑是一个比较好的办法，因为DeepSpeed ZeRO-2主要用于训练，因为它的功能对推理没有用。但是当DeepSpeed发展到ZeRO-3后，也可用于推理，因为它允许在多个GPU上加载大型模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4031f78-ab00-4441-887b-447c6d1d811c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在微调过程中，参数配置和优化对于模型性能和训练效率至关重要。合理的参数设置不仅可以加速模型的收敛，还可以提高模型的表现。特别是当我们使用高级的训练框架如DeepSpeed时，更需要对每个参数有深入的理解和精细的调整。DeepSpeed训练过程中涉及的主要参数和分类如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e1411-f7da-4e6d-a05c-b3f7dc25433b",
   "metadata": {},
   "source": [
    "|para| n |GPU| times/acc |cost|micro-bs|bs|1 0000| times |\n",
    "|:----|:----|:----|:----|:----|:----|:----|:----|:----|\n",
    "|4-4-16 | 4U | 21554MB | 4 |50s / step | 64 |256| 39 step/epoch | 32.5min |\n",
    "|4-4-12 | 4U | 21554MB | 3 |40s / step | 48 |192| 52 step/epoch | 34.6h |\n",
    "|4-4-8  | 4U | 21554MB | 2 |25s / step | 32 |128| 78 step/epoch | 32.5h |\n",
    "|2-2-16 | 4U | 18254MB | 8 | 25s / step | 32 |128| 78 step/epoch  | 32.5h |\n",
    "|2-2-32 | 4U | 18254MB | 16 |50s / step | 64 |256| 39 step/epoch  | 32.5h |\n",
    "\n",
    "$$micro-bs=TRAIN\\_BATCH\\_SIZE*GRA\\_ACC\\_STEPS$$\n",
    "$$bs=micro-bs*nGPU$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2a74a-7fa6-4502-b265-e7d8ca28733f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中，para列中的4-4-16所在位置参数分别表示per_device_train_batch_size、per_device_eval_batch_size、gradient_accumulation_steps。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4f488-3891-4467-a8af-e1444bae19f3",
   "metadata": {},
   "source": [
    "> 注：10000条数据在当前para下完成一个epoch需要步数；10000/48/4=52 step/epoch； 10000/32/4=78 step/epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d7f09-ec2d-4a08-8e5f-768f72f9f2b2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ZeRO的易操作性：只需更改几行代码即可使PyTorch 模型能够使用DeepSpeed 和ZeRO。与当前的模型并行库相比，DeepSpeed 不需要重新设计代码或模型重构。它也不会限制模型维度（例如注意力头的数量、隐藏大小等）、批量大小或任何其他训练参数。对于多达 60 亿个参数的模型，即可以方便地使用数据并行（由 ZeRO 提供支持），而无需模型并行，而相比之下，对于超过 13 亿个参数的模型，标准数据并行会耗尽内存。 ZeRO 的第二阶段和第三阶段将进一步增加仅通过数据并行性即可训练的模型大小。此外，DeepSpeed 支持 ZeRO 支持的数据并行性与模型并行性的灵活组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f971d6f-3cbd-4542-8c1a-3e2173aca04c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240913103205276.png\"width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a890a-5317-4ef6-98ae-f780b8d8ebf2",
   "metadata": {},
   "source": [
    "## 三、基于ChatGLM3-6b进行P-Tuning v2微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e0e90-c4bc-4362-8f61-329cee2a8955",
   "metadata": {},
   "source": [
    "### 3.1 微调环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee21ea-7d5c-4cc7-a4c3-bab5d68c134c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本次实验环境配置：\n",
    "- 操作系统：Ubuntu 22.04；\n",
    "- GPU：3090双卡，总共48G显存；\n",
    "- CPU：AMD 5900X；\n",
    "- 存储：64G内存+2T SSD数据盘；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea76cad-09e0-4c1d-a587-96eaa94567a7",
   "metadata": {},
   "source": [
    "官方硬件需求：显存：24GB及以上（推荐使用30系或A10等sm80架构以上的NVIDIA显卡进行尝试） 内存：16GB RAM: 2.9 /16 GB GPU RAM: 15.5/16.0 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d4992-96e3-4b04-b6d4-2a94498ca56e",
   "metadata": {},
   "source": [
    "本教程P-Tuning V2微调方法是使得原有的ChatGLM3-6b模型**具备专业的广告生成能力**。项目地址：https://github.com/THUDM/ChatGLM3/tree/main/finetune_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2e5c5-2d21-43b3-9822-44bb0804ef45",
   "metadata": {},
   "source": [
    "首先我们需要完成微调所需的相关依赖的安装，在文件相对路径/chatglm3/finetune_demo下可以找到`requirement.txt`文件，里面包含了所有的依赖以及对应版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8739cf0-6c7b-46c6-8249-3536516b9bd9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240905150634867.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217111e-a387-4f84-bbb0-982458f6e14d",
   "metadata": {},
   "source": [
    "在微调环境准备阶段，需要先检查代码的运行地址，确保运行地址处于 finetune_demo 中。 并确保已经安装了 requirements.txt中全部依赖。\n",
    "使用以下命令安装依赖：\r\n",
    "\r\n",
    "pip install -r requirements.txt\r\n",
    "\r\n",
    "这条命令会读取 `requirements.txt` 文件，并安装其中列出的所有依赖包。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e6d5f-c4a7-4d4f-99c5-5255b46be3c8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240903104308880.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc367df-d7eb-4047-857b-7a1732bbe0d9",
   "metadata": {},
   "source": [
    "- 其中mpi4py这个依赖，如果在正常状态`pip install`方法不能直接拉取，需要`conda install`的方式下载，会同时拉去前置的openmpi依赖。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3fbea-872d-4b56-86bd-a8c3a8114aac",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240903115334033.png\" width=50%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47e6ca-0535-43c4-997a-816924491f40",
   "metadata": {},
   "source": [
    "### 3.2 微调数据下载与预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a455a-daf1-44fc-a127-95dbaf6cb894",
   "metadata": {},
   "source": [
    "&emsp;&emsp;官网为**广告生成能力项目**提供了一个微调示例：AdvertiseGen 数据集，里面包含两个文件分别是训练集和测试集。可以进入Tsinghua Cloud：https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1 下载并上传到`finetune_demo/data`路径下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0dfdb6-aac3-444a-be3e-eb9bbec29d77",
   "metadata": {},
   "source": [
    "- 模型路径大家要使用绝对路径。因为LoRA仅存储adapter部分参数，而adapter配置 json文件记录了预训练模型的路径，用于读取预训练模型权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f5f45-3f6c-4d11-9c30-76d12cfd54f2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240903104941760.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa247e80-a98a-42ef-816a-c38e0acca0e9",
   "metadata": {},
   "source": [
    "一种更便捷的方式就是在服务器终端使用`wget`命令来进行下载。同时下载到的AdvertiseGen数据集是一个.tar.gz的压缩文件，需要解压才可使用：\n",
    "```bash\n",
    "wget - O AdvertiseGen https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5b197-8ead-448a-b1cf-46c88838f40c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401101712121.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81affaa6-2de1-4d7e-af37-e9c41dd4dd7f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该数据集任务为根据输入（content）生成一段广告词（summary），其数据格式如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47db974-b4ca-4d35-8b5c-f85a6191f157",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240902143658551.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a3071-1c31-48e8-98ba-80d74a0ea0bb",
   "metadata": {},
   "source": [
    "```json\n",
    "{\"content\": \"类型#上衣*风格#街头*图案#创意*衣样式#卫衣\", \"summary\": \"在这件卫衣上，BRAND-white集合了女性化的柔美还有不变的街头风采，<UNK><UNK>的向日葵花朵美丽的出现在胸前和背后，犹如暗<UNK>闪光的星星一般耀眼又充满着<UNK>的生命力，而后品牌标志性的logo<UNK>出现，呈现出将花束固定的效果，有趣极了，穿的不仅是服饰。更是新颖创意的载体。\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a210284-2f20-4447-bd32-e922b82e7b74",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们需要修改成单轮对话的数据微调格式。可以直接在数据集所在的文件夹新建一个python脚本文件用来转化，代码内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7bb07-9306-4ca0-b948-4838acc8c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    " \n",
    "def process_and_save_json(input_filepath, output_filepath):\n",
    "    # 记录开始时间\n",
    "    start_time = time.time()   \n",
    "    with open(input_filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    " \n",
    "            user_data = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": data[\"content\"]\n",
    "            }\n",
    "            assistant_data = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": data[\"summary\"]\n",
    "            }\n",
    "            outfile = open(output_filepath, 'a', encoding='utf-8')\n",
    "            json.dump({\"conversations\": [user_data, assistant_data]}, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "    # 记录结束时间\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    " \n",
    "    print(\"耗时:\", elapsed_time, \"秒\")\n",
    " \n",
    "input_filepath = 'train.json'  # 传入处理前的json\n",
    "output_filepath = 'train2.json'  # 处理后的保存位置\n",
    " \n",
    "process_and_save_json(input_filepath, output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419fa0f-6969-4773-8d46-e7ad83ba93b5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;执行后，数据格式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753df96c-0ca1-4510-8ff2-b199724d1e4b",
   "metadata": {},
   "source": [
    "```json\n",
    "{\"conversations\": [{\"role\": \"user\", \"content\": \"类型#裙*裙长#半身裙\"}, {\"role\": \"assistant\", \"content\": \"这款百搭时尚的仙女半身裙，整体设计非常的飘逸随性，穿上之后每个女孩子都能瞬间变成小仙女啦。料子非常的轻盈，透气性也很好，穿到夏天也很舒适。\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb894186-3521-4a43-957c-35a0d7740b28",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/37eb19d70817262358e43ab7e3f98ba.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29623413-1079-4288-9b39-f4aca2ee003e",
   "metadata": {},
   "source": [
    "- 转化结束后新文件名称是`xx2.json`文件，注意文件名称的修改或调用地址的确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118933a3-b388-4dba-8f65-cf3a20baab46",
   "metadata": {},
   "source": [
    "### 3.3 执行P-Tuning v2微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6914e-ea15-43c1-8c52-56e2da62f349",
   "metadata": {},
   "source": [
    "在正式微调之前可以，先通过配置文件了解以下对应的参数。P-Tuning v2的配置文件在相对路径`chatglm3/finetune_demo/configs`中，适当调节参数可以达到更合适的微调效果以及适配对应具体的显卡能力："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06ae4e-0104-4ec4-b5b5-44e321f5c570",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240913103543806.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee8d7c-c6ea-4c89-a925-bbe4afbd8a06",
   "metadata": {},
   "source": [
    "具体的参数含义如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa03ab-1354-4728-b49d-511b02949143",
   "metadata": {},
   "source": [
    "**data_config**部分\r\n",
    "\r\n",
    "- train_file: 训练数据集的文件路径。\r\n",
    "\r\n",
    "- val_file: 验证数据集的文件路径。\r\n",
    "\r\n",
    "- test_file: 测试数据集的文件路径。\r\n",
    "\r\n",
    "- num_proc: 在加载数据时使用的进程数量。\r\n",
    "\r\n",
    "- max_input_length: 输入序列的最大长度。\r\n",
    "\r\n",
    "- max_output_length: 输出序列的最大长度。: 输出序列的最大长度。: 输出序列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e17d51-209e-4f20-b416-1ea3c3a9b905",
   "metadata": {},
   "source": [
    "**training_args** 部分\r\n",
    "\r\n",
    "- output_dir: 用于保存模型和其他输出的目录。\r\n",
    "- max_steps: 训练的最大步数。\r\n",
    "- max_steps: 训练的最大步数。\r\n",
    "- per_device_train_batch_size: 每个设备（如 GPU）的训练批次大小。\r\n",
    "- dataloader_num_workers: 加载数据时使用的工作线程数量。\r\n",
    "- remove_unused_columns: 是否移除数据中未使用的列。\r\n",
    "- save_strategy: 模型保存策略（例如，每隔多少步保存一次）。\r\n",
    "- save_steps: 每隔多少步保存一次模型。\r\n",
    "- log_level: 日志级别（如 info）。\r\n",
    "- logging_strategy: 日志记录策略。\r\n",
    "- logging_steps: 每隔多少步记录一次日志。\r\n",
    "- per_device_eval_batch_size: 每个设备的评估批次大小。\r\n",
    "- evaluation_strategy: 评估策略（例如，每隔多少步进行一次评估）。\r\n",
    "- eval_steps: 每隔多少步进行一次评估。\r\n",
    "- predict_with_generate: 是否使用生成模式进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484d50c-5ec6-4291-a37a-82ce904d574c",
   "metadata": {},
   "source": [
    "**generation_config** 部分\r\n",
    "\r\n",
    "- max_new_tokens: 生成的最大新 token 数量。\r\n",
    "\r\n",
    "**peft_config** 部分\r\n",
    "\r\n",
    "- peft_type: 使用的参数有效调整类型（如 LORA）。\r\n",
    "- task_type: 任务类型，这里是因果语言模型（CAUSAL_LM）。\r\n",
    "- P-TuningV2 参数：\r\n",
    "  - num_virtual_tokens: 虚拟 token 的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e095f-87d5-4f17-83d6-57f03143b9cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05315fe1-6272-4d19-bec2-6f20a96ab83c",
   "metadata": {},
   "source": [
    "通过在命令行输入以下代码执行可以在单机多卡/多机多卡环境下运行，这是使用 deepspeed 作为加速方案的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d6efd-9102-4cc4-8d73-b5ff566bd940",
   "metadata": {},
   "source": [
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --standalone --nnodes=1 --nproc_per_node=8  finetune_hf.py  data/AdvertiseGen/  /home/util/ARIS/chatglm3/chatglm3-6b  configs/ptuning_v2_.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342374f-6877-4506-8d5d-18f3910202d0",
   "metadata": {},
   "source": [
    "-  **OMP_NUM_THREADS=1**：这是一个环境变量，控制OpenMP库在多线程运算中的并行线程数。将其设置为1表示每个进程只使用一个线程进行计算，避免过多的线程争用资源，可能会提升分布式计算的效率。\n",
    "\n",
    "- **torchrun**： 是用于启动分布式PyTorch训练的工具。它用于替代之前的 `torch.distributed.launch`，更现代化和易于使用。它会自动初始化分布式环境并管理多进程训练。\n",
    "\n",
    "- **standalone**：这个表示在**单机模式（standalone mode）**下运行，即所有训练进程都在一台机器上进行。它不需要额外的集群配置，是常用于小规模分布式训练的简化模式。\n",
    "\n",
    "- **nnodes=1**：指定训练的节点数。在分布式训练中，节点通常指的是一台物理或虚拟机。这里 `nnodes=1` 表示只有一个节点参与训练。\n",
    "\n",
    "- **nproc_per_node=8**：表示在每个节点上启动 8 个进程。由于是单节点，意味着这台机器将运行 8 个并行的训练进程。通常，`nproc_per_node` 与机器上的 GPU 数量对应，这里应该是有 8 个 GPU，每个进程使用一张 GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ab1e5-2247-421e-b711-7dbdb9518355",
   "metadata": {},
   "source": [
    "通过以下代码可以在单机单卡环境下运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b2b1b-43a3-4a5c-9915-eb976f21f306",
   "metadata": {},
   "source": [
    "```\n",
    "python finetune_hf.py  data/AdvertiseGen/    /home/util/ARIS/chatglm3/chatglm3-6b   configs/ptuning_v2_.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a758676-91c2-41a7-b7b2-ef510eac07b9",
   "metadata": {},
   "source": [
    "**从保存点开始训练**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029786c3-8827-4b1d-9287-658119e608d3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果按照上述方式进行训练，每次微调都会从头开始。如果你想从训练一半的模型开始微调，可以加入第四个参数，这个参数有两种传入方式:\n",
    "\n",
    "- yes, 自动从最后一个保存的 Checkpoint开始训练\n",
    "- XX, 断点号数字 例如`500`则从序号为500 的Checkpoint开始训练。\n",
    "\n",
    "以下是一个从最后一个保存点继续微调的示例代码的示例："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6cfec-9663-40ce-ae75-f771b19ede4c",
   "metadata": {},
   "source": [
    "```\n",
    "python finetune_hf.py  data/AdvertiseGen/   /home/util/ARIS/chatglm3/chatglm3-6b   configs/ptuning_v2_.yaml yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8883ad-a385-45a1-8822-5b4b093570aa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`checkpoint`中存储的是训练过程中保存的模型状态，包括模型参数、优化器状态、当前epoch等信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa56bb-d98c-48a9-bbbf-9cb40bb836d9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401111417256.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a72e92-06b1-4961-9678-32e985facefe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不同的训练参数，会产生不同数量的`checkpoint`，比如在脚本中，`SAVE_INTERVAL` 设置为1000，这说明每1000个训练步骤保存一次模型。如果`MAX_STEP`设置为3000，就应该有3个checkpoints被保存，这个也很好计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e813b-4dc8-48db-92fe-9397019fddfc",
   "metadata": {},
   "source": [
    "### 3.4 使用微调后的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1df644-5b11-42eb-bcda-1cfd5611ba8c",
   "metadata": {},
   "source": [
    "可以在 finetune_demo/inference_hf.py 中使用微调后的模型，仅需要一行代码就能进行模型的推理测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc24504-8b79-4b92-b6e6-4fc1f4f08740",
   "metadata": {},
   "source": [
    "```\n",
    "python inference_hf.py your_finetune_path --prompt your prompt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cfc89-780e-4967-b73c-f01eb4747124",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240903164858312.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86277e6-d073-4701-b027-873a5dc8c6c9",
   "metadata": {},
   "source": [
    "也可以在`/chatglm3/finetune_demo`文件夹下的finetune_hf.py启动微调后的模型推理，不过需要先修改文件`adapter_config.json`中调用的地址才能正确运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffa7f9-d571-4fae-b354-4884e27de360",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240913145528625.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcfe3c-3a28-42b3-8aea-3c439aa25c69",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240913145711113.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53601599-c16c-441d-b771-8bc60576904f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8aafcc9-f8df-4ef6-8733-1611b3384cef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
