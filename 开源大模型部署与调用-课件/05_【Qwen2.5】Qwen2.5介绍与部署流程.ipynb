{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b84baa4-b28f-4cab-8958-c847a81f5d19",
   "metadata": {},
   "source": [
    "# <center>Qwen 2.5 介绍与部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36b16e-73ff-4086-9bca-7dad4c72c298",
   "metadata": {},
   "source": [
    ">前言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd53a21-35cd-45cd-8254-df6ccd2f1443",
   "metadata": {},
   "source": [
    "\n",
    "在国产大模型领域，Qwen 系列一直稳居前列，其出色的性能使其在多项评测中名列前茅。作为阿里巴巴的一项重要研发成果，Qwen 系列的开源版本在业内备受瞩目，且长期以来在各大榜单上表现优异。2024年9月，阿里重磅推出了全新升级的 Qwen2.5 系列模型，涵盖了不同参数规模的版本，以满足多样化的应用需求。此外，Qwen2.5 系列还推出了各具特色的强化版本，进一步提升了模型在特定任务上的表现。接下来，让我们一同深入了解 Qwen2.5 系列的具体技术特点及其在实际应用中的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a89be0-10a2-459e-8b9f-66f094c0ee82",
   "metadata": {},
   "source": [
    ">目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b797802-4240-443b-8dd8-302fd405067c",
   "metadata": {},
   "source": [
    "一 Qwen 2.5 模型介绍<br>\n",
    "&emsp;1.1 基本参数介绍<br>\n",
    "&emsp;1.2 线上体验办法<br>\n",
    "二 ModelScope本地部署流程<br>\n",
    "三 ModelScope本线上部署流程<br>\n",
    "四 Ollama框架部署流程<br>\n",
    "&emsp;4.1 Ollama基本信息介绍<br>\n",
    "&emsp;4.2 使用Ollama实现Qwen2.5下载流程<br>\n",
    "&emsp;4.3 Ollama文件管理<br>\n",
    "五 在Windows环境下使用Ollama框架部署流程<br>\n",
    "六 使用vLLM框架部署流程<br>\n",
    "&emsp;6.1 vLLM基本介绍与安装<br>\n",
    "&emsp;6.2 使用vLLM进行推理部署<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0253b6-e034-46b2-acc0-61572fe86b13",
   "metadata": {},
   "source": [
    "# 1. Qwen 2.5模型介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5404834-a077-4b6a-8f55-57decf703f36",
   "metadata": {},
   "source": [
    "## 1.1基本参数介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb120f-d5ff-4607-9101-19a027f6d498",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023181450068.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36847312-2ff1-457c-a8b7-22d2dd6bb14d",
   "metadata": {},
   "source": [
    "通义千问是由阿里巴巴的通义千问团队研发的一系列大规模语言和多模态模型。该模型能够执行多种任务，包括自然语言理解、文本生成、视觉理解、音频理解、工具调用、角色扮演和智能体操作等。语言和多模态模型均在大规模、多语言和多模态的数据上进行预训练，并在高质量语料上进行后续训练，以使其与人类的偏好保持一致。同时发布开源和闭源两大版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ed235-3efb-4e15-adcc-a908a859ae43",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023180644425.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57ef7d-afba-4db3-87ea-0eaa52dfe589",
   "metadata": {},
   "source": [
    "2024年9月19日最新发布的模型包括语言模型 Qwen2.5，这个系列涵盖了多种尺寸的大语言模型、多模态模型、数学模型以及代码模型，构建了一个完善的模型体系，能够为不同领域的应用提供强有力的支持。不论是在自然语言处理任务中的文本生成与问答，还是在编程领域的代码生成与辅助，或是数学问题的求解，Qwen2.5 都能展现出色的表现。每种尺寸的模型均包含基础版本、指令跟随版本和量化版本，共推出了100多个模型，充分满足了用户在各类应用场景中的多样化需求。具体版本内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97717a1-845b-41dd-8cf2-0f9d5d142a29",
   "metadata": {},
   "source": [
    "- Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B;\n",
    "- Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B;\n",
    "- Qwen2.5-Math: 1.5B, 7B, 以及72B。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b21ab-3803-473f-bb6b-73cb0cbca5f8",
   "metadata": {},
   "source": [
    "相比于 Qwen2 系列，Qwen2.5 带来了以下全新升级：\n",
    "\n",
    "- 全面开源：考虑到用户对 10B 至 30B 范围模型的需求以及移动端对 3B 模型的兴趣，此次不仅继续开源 Qwen2 系列中的 0.5B/1.5B/7B/72B 四款模型，Qwen2.5 系列还新增了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有较强的竞争力，例如 Qwen2.5-32B 的整体表现超越了 Qwen2-72B，而 Qwen2.5-14B 则领先于 Qwen2-57B-A14B。\n",
    "\n",
    "- 更大规模、更高质量的预训练数据集：预训练数据集的规模从 7T tokens 扩展至 18T tokens。\n",
    "\n",
    "- 知识储备升级：Qwen2.5 的知识涵盖面更广。在 MMLU 基准测试中，Qwen2.5-7B 和 72B 的得分分别从 Qwen2 的 70.3 提升至 74.2，以及从 84.2 提升至 86.1。此外，Qwen2.5 在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中也有明显提升。\n",
    "\n",
    "- 代码能力增强：得益于 Qwen2.5-Coder 的突破，Qwen2.5 在代码生成能力上大幅提升。Qwen2.5-72B-Instruct 在 LiveCodeBench（2305-2409）、MultiPL-E 和 MBPP 中的得分分别为 55.5、75.1 和 88.2，明显优于 Qwen2-72B-Instruct 的 32.2、69.2 和 80.2。\n",
    "\n",
    "- 数学能力提升：引入了 Qwen2-math 的技术后，Qwen2.5 在数学推理表现上也有快速提升。在 MATH 基准测试中，Qwen2.5-7B/72B-Instruct 的得分分别从 Qwen2-7B/72B-Instruct 的 52.9/69.0 上升至 75.5/83.1。\n",
    "\n",
    "- 更符合人类偏好：Qwen2.5 生成的内容更贴近人类的偏好。具体来说，Qwen2.5-72B-Instruct 在 Arena-Hard 测试中的得分从 48.1 大幅提升至 81.2，而 MT-Bench 的得分也从 9.12 提升至 9.35，相较于之前的 Qwen2-72B 有显著提升。\n",
    "\n",
    "- 其他核心能力提升：Qwen2.5 在指令跟随、生成长文本（从 1K 升级到 8K tokens）、理解结构化数据（如表格）以及生成结构化输出（特别是 JSON）方面都有明显进步。此外，Qwen2.5 能够更好地响应多样化的系统提示，支持用户为模型设置特定角色或自定义条件。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcd115-8afa-45bb-b67f-34bb5ea1c72d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023181729012.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e7bc6-f258-48df-8d51-4d50fa8f31e6",
   "metadata": {},
   "source": [
    "就 Qwen2.5 语言模型而言，所有模型均在最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。与 Qwen2 相比，Qwen2.5 获得了显著更多的知识（MMLU：85+），并在编程能力（HumanEval 85+）和数学能力（MATH 80+）方面实现了大幅提升。此外，新模型在指令执行、生成长文本（超过 8K tokens）、理解结构化数据（如表格）以及生成结构化输出，特别是 JSON 方面也取得了显著进展。总体而言，Qwen2.5 模型对各种系统提示表现出更强的适应性，增强了角色扮演的实现和聊天机器人的条件设置功能。与 Qwen2 相似，Qwen2.5 语言模型支持高达 128K tokens，并能够生成最多 8K tokens 的内容，同时保持对包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等在内的 29 种以上语言的支持。关于模型的基本信息已在下表中提供。\n",
    "\n",
    "专业领域的专家语言模型，如用于编程的 Qwen2.5-Coder 和用于数学的 Qwen2.5-Math，相较于前身 CodeQwen1.5 和 Qwen2-Math 实现了实质性改进。具体来说，Qwen2.5-Coder 在包含 5.5T tokens 编程相关数据上进行了训练，使得即使是较小的编程专用模型在编程评估基准测试中也能表现出与大型语言模型相媲美的竞争力。同时，Qwen2.5-Math 支持中文和英文，并整合了多种推理方法，包括链式推理（CoT）、程序推理（PoT）和工具集成推理（TIR）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbdafc-8c2b-4cbc-bd2b-0d68e77f565e",
   "metadata": {},
   "source": [
    "> 72B表现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc5630-53a8-4664-b37b-e0af2c64858a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022172404290.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e1a38-76d7-4857-bf55-44ac08b3e245",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022172413811.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e14d8-82e6-4542-8815-ee9bdd22cca8",
   "metadata": {},
   "source": [
    "\n",
    "Qwen2.5 的一项重要更新是重新推出了 Qwen2.5-14B 和 Qwen2.5-32B。这些模型在各种任务中表现优异，超越了同等或更大规模的基线模型，如 Phi-3.5-MoE-Instruct 和 Gemma2-27B-IT。Qwen2.5 系列在模型规模和能力之间取得了良好平衡，提供了与一些更大型模型相当甚至更优的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee736b5b-a583-4eab-9d8b-23d9f53ad2a4",
   "metadata": {},
   "source": [
    ">14/32B表现："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21ba88-2b2f-4e42-892c-046b11b6c9cf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022172427354.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504667b0-cc42-4dc5-9387-233e06e1c141",
   "metadata": {},
   "source": [
    "近年来，小型语言模型（SLMs）出现了明显的转向趋势。尽管历史上小型语言模型的表现一直落后于大型语言模型（LLMs），但二者之间的性能差距正在迅速缩小。值得注意的是，即使是只有约 30 亿参数的模型，现在也能够取得高度竞争力的结果。附带的图表显示了一个重要的趋势：在 MMLU 中得分超过 65 的新型模型正逐渐变得更小，这凸显了语言模型知识密度增长速度的加快。特别值得一提的是，Qwen2.5-3B 成为这一趋势的典型例子，凭借约 30 亿参数实现了令人印象深刻的性能，展现了相较于前辈模型的高效性和能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fa6c4-15eb-4311-b6f8-c0987670b0fc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/qwen2.5-small.jpg\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed7e7d-ce47-4046-98ec-d591d61a2cf5",
   "metadata": {},
   "source": [
    "Qwen2.5 还推出了专门针对数学问题的模型——Qwen2.5-Math。其数学能力得分十分亮眼，这得益于其大规模的训练数据和专门的数学模型设计。与其他顶尖模型相比，Qwen2.5 不仅在数学推理上表现优异，还在编程能力上展现出强大的竞争力。该模型在数学推理方面进行了特别优化，支持中文和英文，并整合了多种推理方法，包括：思维链（Chain of Thought, CoT）：帮助模型在解决复杂问题时进行逐步推理。工具集成推理（Tool-Integrated Reasoning, TIR）：增强模型在解决数学问题时的灵活性和准确性。\n",
    "\n",
    "Qwen2.5-Math-72B-Instruct 的整体性能超越了 Qwen2-Math-72B-Instruct 和 GPT4-o，甚至是非常小的专业模型如 Qwen2.5-Math-1.5B-Instruct 也能在与大型语言模型的竞争中取得高度竞争力的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf48d53-afee-4fb0-9dfd-3b79e37d3acf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/2024-08-qwen2.5-math-allsize.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fd83c-6e4d-426a-a7ff-76db225fd781",
   "metadata": {},
   "source": [
    "- 更多信息可以访问Qwen官网：https://qwenlm.github.io/zh/blog/qwen2.5-llm/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7ca73-b048-4cee-9089-f6453be307ea",
   "metadata": {},
   "source": [
    "## 1.2 线上体验办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551494d-08e4-47e7-9779-76a813abd6f1",
   "metadata": {},
   "source": [
    "在 Hugging Face 平台上，用户可以方便地进行该模型的线上体验。这种在线测试不仅可以帮助用户直观地了解模型的性能，还能够根据具体任务需求进行评估。因此，建议大家在正式使用该模型之前，先通过这种方式进行测试，以确保模型的能力和特性符合自身的需求。在测试过程中，用户可以尝试不同类型的输入和任务，从而更好地评估模型在实际应用中的表现。这一过程有助于发现模型的优势与局限，使用户在后续的应用中做出更为明智的选择。通过线上体验，还可以及时获取最新的功能更新和使用技巧，从而优化工作流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5feae-8501-468e-be93-f797d2f35461",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/Qwen/Qwen2.5-72B-Instruct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3527c615-a900-4cc3-bcbe-e0db6c85e43f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023183708271.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9935830-4c63-40ca-9c8e-24aba0e4e94c",
   "metadata": {},
   "source": [
    "# 2. ModelScope本地部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850e617-2964-430c-b87e-68ddbdb0caaa",
   "metadata": {},
   "source": [
    "- **Step 1. 创建conda虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdde940-356c-4df3-874a-e3ffdc96f927",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Conda创建虚拟环境的意义在于提供了一个隔离的、独立的环境，用于Python项目和其依赖包的管理。每个虚拟环境都有自己的Python运行时和一组库。这意味着我们可以在不同的环境中安装不同版本的库而互不影响。根据官方文档信息建议Python版本3.10以上。创建虚拟环境的办法可以通过使用以下命令创建：\n",
    "\n",
    "```bash\n",
    "# GLM4 是你想要给环境的名称，python=3.11 指定了要安装的Python版本。你可以根据需要选择不同的名称和/或Python版本。\n",
    "\n",
    "conda create -n qwen2_5 python=3.11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1d79f-01ed-474e-800a-586110cb0ab4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022152113279.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14acfb8-c4bc-4e37-a9d7-1a085c39b5fe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;创建虚拟环境后，需要激活它。使用以下命令来激活刚刚创建的环境。如果成功激活，可以看到在命令行的最前方的括号中，就标识了当前的虚拟环境。虚拟环境创建完成后接下来安装torch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b688c-0c25-4227-81ab-e2cfee62051c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023112135370.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84753f59-314c-4de4-a8f4-1c7882755ee6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果忘记或者想要管理自己建立的虚拟环境，可以通过conda env list命令来查看所有已创建的环境名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb1dae-e0ba-4b73-8e01-14b5b21b494c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143512354.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b38ab-5d50-4c52-b181-aebbfb576f9d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果需要卸载指定的虚拟环境则通过以下指令实现：\n",
    "```\n",
    "conda env remove --name envname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806b128-5e0b-4f92-9843-42616750c562",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143643575.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725a328-24f9-41bb-98cf-079f8555b52a",
   "metadata": {},
   "source": [
    "- 需要注意的是无法卸载当前激活的环境，建议卸载时先切换到base环境中再执行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56f905-3a56-45bc-9c63-9c9baca694f7",
   "metadata": {},
   "source": [
    "- **Step 2. 查看当前驱动最高支持的CUDA版本**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bfb37-78dc-4b7c-b5a8-e84b041acd45",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们需要根据CUDA版本选择Pytorch框架，先看下当前的CUDA版本：\n",
    "```\n",
    "nvidia -smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbcc76-96cf-4520-ae31-ede81b95f330",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924161818464.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8771a-ea0b-49b9-8763-3c47ca6b77a2",
   "metadata": {},
   "source": [
    "- **Step 3. 在虚拟环境中安装Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19853c78-85b7-405b-ad8b-415fa46cbe1e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进入Pytorch官网：https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439f183-91b7-4717-bc71-2860c40b4976",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240103163206436.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be40524-4a86-43d1-8d79-516e2db2cf17",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当前的电脑CUDA的最高版本要求是12.2，所以需要找到不大于12.2版本的Pytorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d693661-171e-4e3a-97e7-870f723454d0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041455184.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a2e36-34e3-4786-8c1a-b13c900ca910",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直接复制对应的命令，进入终端执行即可。这实际上安装的是为 CUDA 12.1 优化的 PyTorch 版本。这个 PyTorch 版本预编译并打包了与 CUDA 12.1 版本相对应的二进制文件和库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9684bf8-fd51-423e-aa14-43eaf96522d0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022153422264.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02566b-9c51-4352-8e4b-5460116920d6",
   "metadata": {},
   "source": [
    "- **Step 4. 安装Pytorch验证**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94771747-0215-436e-833e-4db1a5fd937b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;待安装完成后，如果想要检查是否成功安装了GPU版本的PyTorch，可以通过几个简单的步骤在Python环境中进行验证：\n",
    "```bash\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f192154-9ef1-4dcc-801b-83130ed4d147",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154248388.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95579b-9ba9-434f-90df-6c218f368754",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果输出是版本号数字，则表示GPU版本的PyTorch已经安装成功并且可以使用CUDA，如果显示ModelNotFoundError，则表明没有安装GPU版本的PyTorch，或者CUDA环境没有正确配置，此时根据教程，重新检查自己的执行过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfc4b1-e9f8-4adc-adeb-209dd8e26a40",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154338394.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826decb-11e2-429a-9d0e-3764163fe789",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然通过pip show的方式可以很简洁的查看已安装包的详细信息。pip show <package_name> 可以展示出包的版本、依赖关系（展示一个包依赖哪些其他包）、定位包安装位置、验证安装确实包是否正确安装及详情。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccce74-716a-4ec9-863d-87bdcd204d5d",
   "metadata": {},
   "source": [
    "- **Step 5. 安装必要的依赖包**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd351ca-b2fe-419d-be83-449528c65e39",
   "metadata": {},
   "source": [
    "Transfomers是大模型推理时所需要使用的框架，官方给出的建议版本是`Transfomers>=4.37.0`，通过以下指令可以下载最新版本的Transfomers："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f544e-00a2-4c79-8347-f2a5a37e0c1f",
   "metadata": {},
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e4c66-0aaf-4ce4-9ea6-cdadce63f874",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154907510.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2e4e4-5251-462e-a138-fea69e41c15f",
   "metadata": {},
   "source": [
    "安装完成后可以通过以下命令检查是否安装：\n",
    "```\n",
    "pip show transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024b473-4352-4b4a-b736-8d5b2fadd31f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154927444.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116306ec-e019-443a-a1aa-9da0f1278e78",
   "metadata": {},
   "source": [
    "接下来需要安装下载工具modelscope以及接下来要下载脚本的依赖accelerate，通过以下代码进行对应工具的部署：\n",
    "```\n",
    "pip install modelscope\n",
    "pip install accelerate>=0.26.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa225a8a-877e-43d2-a96b-48549e5a95cb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022160253024.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb381fe-e55f-45b5-a875-c6941c87bbd5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022162830644.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6401f-6d13-4250-b449-8e3d293da961",
   "metadata": {},
   "source": [
    "- **Step 6.1 使用下载脚本安装**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcb4a5-8cea-47ef-80f0-c12eadb3946e",
   "metadata": {},
   "source": [
    ">这是一个自动安装并进行运行测试的脚本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3456b2-f644-4cf9-8d2e-0af4c7a5d4d7",
   "metadata": {},
   "source": [
    "通过mkdir命令创建一个存放Qwen2.5项目文件的文件夹\n",
    "\n",
    "```\n",
    "mkdir qwen2_5 #文件的具体名称可以自定义\n",
    "cd qwen2_5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfc519-3eb2-47e9-9432-c93c88ab5a68",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022155134521.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee084050-a801-4e1a-83c7-62acc09b5e06",
   "metadata": {},
   "source": [
    "在命令行种可以通过vim的方式创建或编辑文件，通过`vim download.py`创建一个python文件，将以下代码复制，然后保存退出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2baee54-4b76-4da7-8f46-40d81aff80b7",
   "metadata": {},
   "source": [
    "```python\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\r\n",
    "\r\n",
    "model_name = \"qwen/Qwen2.5-7B-Instruct\"\r\n",
    "\r\n",
    "model = AutoModelForCausalLM.from_pretrained(\r\n",
    "    model_name,\r\n",
    "    torch_dtype=\"auto\",\r\n",
    "    device_map=\"auto\"\r\n",
    ")\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
    "\r\n",
    "prompt = \"Give me a short introduction to large language model.\"\r\n",
    "messages = [\r\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n",
    "    {\"role\": \"user\", \"content\": prompt}\r\n",
    "]\r\n",
    "text = tokenizer.apply_chat_template(\r\n",
    "    messages,\r\n",
    "    tokenize=False,\r\n",
    "    add_generation_prompt=True\r\n",
    ")\r\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\r\n",
    "\r\n",
    "generated_ids = model.generate(\r\n",
    "    **model_inputs,\r\n",
    "    max_new_tokens=512\r\n",
    ")\r\n",
    "generated_ids = [\r\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n",
    "]\r\n",
    "\r\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16b058-c408-4559-b6ee-c2cda0998c80",
   "metadata": {},
   "source": [
    "编辑好脚本之后通过`python download.py`开始执行这个文件，如下图所示开始自动下载大模型启动所需的项目文件与权重文件以及分词器文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4a15b-cb87-40e0-8032-109d6c338aaf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022160452593.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b868bb-6e50-4b94-a462-4bcaaf2ef0b5",
   "metadata": {},
   "source": [
    "文件安装完毕后会启动对话测试（再次运行该文件会直接加载本地权重文件后直接进行运行推理），出现以下文本返回信息说明文件下载完整且可以启动："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d12e63-fa6b-4167-9a7c-5e40eeafae9b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022163008952.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3ed62-56c3-4914-a66a-2f6b466f6ff5",
   "metadata": {},
   "source": [
    "通过后台监控可以看到推理该对话占用将近15G的显存："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8769f9d-f623-4e8d-895b-e14c4202a504",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022162938337.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d709573-8424-4acc-b669-10bfd5b712de",
   "metadata": {},
   "source": [
    "文件完整性可以通过以下文档进行检查："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e79a2-7136-4570-9a70-055f4c56dfe4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022164607284.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf75106-c8ad-4684-adca-aab6865a527a",
   "metadata": {},
   "source": [
    "- **Step 6.2 从Model Scope下载GLM4-9b-chat模型权重**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5efc0-424d-4bf6-8951-4203958e42f3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Model Scope下载路径如下：\n",
    "https://www.modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08b5ab-d537-4298-afe2-d0d77c82dc07",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Model Scope支持多种下载模式：SDK下载、Git下载、命令行下载（下载完整模型、下载单个文件 后直接加文件名即可）、手动下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5504c-8412-43ef-81ed-7b0527750aa6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023143452005.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c20c44-490c-4e67-9cb5-672eccc37298",
   "metadata": {},
   "source": [
    "这是一种快捷的本机下载方式，首先通过命令`pip install modelscope`安装好达摩社区的安装包，然后通过以下指令可以下载模型的权重文件，这种方法无需翻墙，无需下载额外的插件。\n",
    "```\n",
    "modelscope download --model Qwen/Qwen2.5-7B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cdd61-e7cd-42a0-a384-a5d6fe4414ff",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924184742785.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9e193-faf5-479d-b302-7891515a1193",
   "metadata": {},
   "source": [
    "这种命令可以支持下载单个的文件，通过以下指令可以单独下载某文件,用于补充或更新(这里用readme文件为例)：\n",
    "```\n",
    "modelscope download --model Qwen/Qwen2.5-7B-Instruct README.md --local_dir ./dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07045d-813a-4dc0-baed-00be2d42e1b1",
   "metadata": {},
   "source": [
    "下载完成后全部的权重文件的内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8b96d-58e0-46c4-8477-98f34db792fd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022164940062.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed93df7-4fa6-4e50-b9e6-a3e11d7c397e",
   "metadata": {},
   "source": [
    "- **Step 6.3 手动方式下载**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177690a-a133-4351-9286-ad15e986248f",
   "metadata": {},
   "source": [
    "在model scope中也可以实现手动下载，虽然相较于命令行下载的方法较繁琐，但是特殊情况下的有效安装方法。在模型下载页面可以通过手动点击文件对应的下载按钮即可实现下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbdd26-158c-42fa-ad57-d3db88f6721f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925161540765.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d088f2-e492-491c-b9d9-a3e0dec9fff9",
   "metadata": {},
   "source": [
    "下载好的权重文件和命令行下载方式的一样，不过一般如果是在主机-服务器环境下使用这种方式下载的话需要xftp这样的传输工具在上传到服务器使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241abe3-6b3d-4648-88b5-92252609a72d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022164940062.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12163d-8ab1-4076-90f8-13d4cf22caa5",
   "metadata": {},
   "source": [
    "# 3.ModelScope线上部署办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff09af-3492-4d25-b601-cf51d937bc1b",
   "metadata": {},
   "source": [
    "##### **ModelScope在线算力与在线环境获取指南**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c1394-476a-4e1b-875d-ba48a42acd64",
   "metadata": {},
   "source": [
    "&emsp;&emsp;登录魔搭社区：https://www.modelscope.cn/home ，点击注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffa10b-6ef9-4a27-8d0d-2b0f17001a8f",
   "metadata": {},
   "source": [
    "<img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171441920.png\" alt=\"image-20240417144137477\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d8e0a-9567-4874-98e7-f4302ab74f87",
   "metadata": {},
   "source": [
    "输入账号密码完成注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984ab1d-7300-45ab-a0b1-eaccdeaa883c",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171443072.png\" alt=\"image-20240417144344970\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af093f-5d4f-4407-885f-081228fa26cd",
   "metadata": {},
   "source": [
    "注册完成后，点击个人中心，点击绑定阿里云账号："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b1cf7-8ec1-4390-8292-5f30af3da541",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171444204.png\" alt=\"image-20240417144458827\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b03d2f-9cf4-4dfd-8709-e22e6e1f9a7e",
   "metadata": {},
   "source": [
    "在跳转页面中选择登录阿里云，未注册阿里云也可以在当前页面注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab954509-7416-412e-89e1-af63bded75d2",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171447094.png\" alt=\"image-20240417144756962\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7781c-94e6-4663-98ee-bb4fb94f78da",
   "metadata": {},
   "source": [
    "绑定完成后，点击左侧“我的Notebook”，即可查看当前账号获赠算力情况。对于首次绑定阿里云账号的用户，都会赠送永久免费的CPU环境（8核32G内存）和36小时限时使用的GPU算力（32G内存+24G显存）。这里的GPU算力会根据实际使用情况扣除剩余时间，总共36小时的使用时间完全足够进行前期各项实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33204dbc-d438-4b9a-8fb0-8405a9087dbc",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171509070.png\" alt=\"image-20240417150915758\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b99c19-fe86-4192-a6a4-46ff533b8176",
   "metadata": {},
   "source": [
    "接下来启动GPU在线算力环境，选择方式二、点击启动："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2dfd3-7072-4173-849c-ccddd986da31",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171509570.png\" alt=\"image-20240417150937257\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3014d94-fd86-4e49-ba68-147f8ccd1bcb",
   "metadata": {},
   "source": [
    "稍等片刻即可完成启动，并点击查看Notebook："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b567d7-7c76-4981-9235-6bf79ae727e3",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171510775.png\" alt=\"image-20240417151001464\" style=\"zoom: 33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92caef42-1460-44b6-b7d8-f5f25d38539b",
   "metadata": {},
   "source": [
    "即可接入在线NoteBook编程环境："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d4e56-f240-49b5-b61b-07c8d34f2ddd",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171511445.png\" alt=\"image-20240417151156357\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50b9fd-18c3-4dcb-b0e0-240a48922db7",
   "metadata": {},
   "source": [
    "当前NoteBook编程环境和Colab类似（谷歌提供的在线编程环境），可以直接调用在线算力来完成编程工作，并且由于该服务由ModelScope提供，因此当前NoteBook已经完成了CUDA、PyTorch、Tensorflow环境配置，并且已经预安装了大模型部署所需各种库，如Transformer库、vLLM库、modelscope库等，并且当前NoteBook运行环境是Ubuntu操作系统，我们可以通过Jupyter中的Terminal功能对Ubuntu系统进行操作："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568f328-ab25-4af8-805f-4463f9b90081",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171516840.png\" alt=\"image-20240417151622754\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff8947-d46e-40a1-9a66-482c1cefc3fb",
   "metadata": {},
   "source": [
    "进入到命令行界面："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6cb1c2-8115-4764-a86a-bc6a6209c772",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171516374.png\" alt=\"image-20240417151651288\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9ad45-5218-40d1-817b-9a46141cabae",
   "metadata": {},
   "source": [
    "通过`pip show +name`的方式可以查看ModelScope已经安装好的依赖资源，可以看到已经安装好了我们所需的各种依赖资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad1f2b-2f51-4d09-8ba6-47c29fd61258",
   "metadata": {},
   "source": [
    "<center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022175754507.png\" alt=\"image-20240417151651288\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c46e51-8449-455d-9643-fe5835cf8933",
   "metadata": {},
   "source": [
    "输入nvidia-smi，查看当前GPU情况："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd36d03-0c88-4cf5-9a76-86b76d562be0",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171518227.png\" alt=\"image-20240417151818137\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5d234-7d86-48b6-a71c-294384bb25df",
   "metadata": {},
   "source": [
    "继续在命令行中进行操作，和本地下载流程一样，在创建部署新文件之前我们先为之创建一个新文件夹，随后进入该文件夹中进行下载操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e93728-3364-4fbd-bd1a-123cd6b8d293",
   "metadata": {},
   "source": [
    "```\n",
    "mkdir qwen2-5\n",
    "cd qwen2-5\n",
    "modelscope download --model Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cd181-7bd8-4a69-b887-47af53122c55",
   "metadata": {},
   "source": [
    "<center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022180401531.png\" alt=\"image-20240417151818137\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d3928-6f60-4e87-898b-83ca828e1365",
   "metadata": {},
   "source": [
    "下载完成之后，其完整的文件信息也如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-0ca7-45db-92bc-44e205a1a953",
   "metadata": {},
   "source": [
    "<center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022180544609.png\" alt=\"image-20240417151818137\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798e06f-b0d7-4dcc-886c-f61b6e12e479",
   "metadata": {},
   "source": [
    "当然，这里需要注意的是，哪怕当前在线编程环境已经做了适配，但并不一定满足所有ModelScope中模型运行要求，既并非每个拉取的Jupyter文件都可以直接运行。不过无论如何，ModelScope Notebook还是为初学者提供了非常友好的、零基础即可入手尝试部署大模型的绝佳实践环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165e908-8f9c-4c99-a02d-ba066507e746",
   "metadata": {},
   "source": [
    "# 4. Ollama框架部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f9560-5eb5-4aa4-b29c-96f776060bcd",
   "metadata": {},
   "source": [
    "### 4.1 Ollama基本信息介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d8dc2-5b47-446a-83e9-020eb4912f64",
   "metadata": {},
   "source": [
    "Ollama 是一个开源的大语言模型服务工具，专注于简化本地模型的创建、管理和部署流程。它可以帮助开发人员和数据科学家轻松地在本地或私有环境中使用大语言模型，而不必依赖于云服务，从而保证了数据隐私和灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75779c1-d9a4-48ef-aa9b-7ac609af7881",
   "metadata": {},
   "source": [
    "Ollama 非常适合需要在本地运行大语言模型的开发者和企业，如：\n",
    "\n",
    "- 开发和测试：在本地快速创建和测试新的语言模型。\n",
    "- 隐私保护：在本地部署模型，适用于有严格隐私需求的企业。\n",
    "- 多模型管理：轻松管理和部署多个模型，适合有大量模型管理需求的团队。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f38bfd-e734-4a0c-8ecf-e7dbf39bcf9a",
   "metadata": {},
   "source": [
    "在官网可以看到Ollama支持的模型列表 https://ollama.com/library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebffd2-12dc-4d3e-a547-ee7364ff2f93",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023151401629.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade24e7-7463-43f2-a19f-a2c93769975d",
   "metadata": {},
   "source": [
    "每个模型下面有支持的功能和参数型号以及基本的模型描述，点击进入对应模型可以看到下载所需占用的内存大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d0297-f355-41e1-87ae-ae8e5d19ea78",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023151317687.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224cde6-4f20-47a7-b931-772000d2109b",
   "metadata": {},
   "source": [
    "### 4.2 使用Ollama实现LLM下载流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a534b2-ae5c-45a8-bc31-9e9919af52d7",
   "metadata": {},
   "source": [
    "使用Ollama方法下载的前两步骤与正常方法下载相同（本地部署流程的前五步），即创建虚拟环境和依据硬件配置Pytorch环境，部署好流程后可以继续下面的步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f57b77-7e7a-4c81-a3c2-5696bc4dc794",
   "metadata": {},
   "source": [
    "Ollama安装硬件要求：\n",
    "\n",
    "- Windows：3060以上显卡+8G以上显存+16G内存，硬盘空间至少20G显卡\n",
    "- \r\n",
    "Mac：M1或M2芯片 16G内存，20G以上硬盘空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be712616-a16e-4e1f-8d27-5457155e2081",
   "metadata": {},
   "source": [
    "下载Ollama的指令如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e346d0-821c-4a9e-904d-54ee6a079d3b",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c24756-4e89-4b71-a413-12ea27ebc9ed",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011114550703.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260688f-8796-450e-a89f-b3eea893f35c",
   "metadata": {},
   "source": [
    "下载完成后检测,如果返回版本号则说明成功下载：\n",
    "```\n",
    "ollama -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67dc65-b33a-4f30-9d0f-a8175c50a184",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011114741149.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069c25f-6c4a-4dd3-90ce-60c6bf26b385",
   "metadata": {},
   "source": [
    "通过指令**ollama help**可以查看该系统可执行的命令："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc4287-f417-429d-be48-db7143e2c512",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011144419942.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468af67a-12d2-467f-95a9-c1b3856fe923",
   "metadata": {},
   "source": [
    "通过以下指令可以，检查ollama可运行的模型列表，可以看见之前下载的Llama 3-8b模型的信息呈现在列："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368bf59-5948-46e0-9ebd-f8fba6f801f0",
   "metadata": {},
   "source": [
    "```\n",
    "ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18efba2-0ccf-4be7-8f97-afac8d93dac0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011115126172.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c721f-4be2-4b8c-b8e0-783f7892e59f",
   "metadata": {},
   "source": [
    "**下载模型**\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369b3ab-51be-4c06-af5e-f12c4cfb0435",
   "metadata": {},
   "source": [
    "在终端中执行命令 `ollama run qwen2.5`或`ollama run qwen2.5：7b` ，即可下载 qwen2.5：7b 模型。模型下载完成后，会自动启动大模型，进入命令行交互模式，直接输入指令，就可以和模型进行对话了对应参数的模型的下载方式可以通过在Ollama官网查看到下载指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425973b2-87a8-406a-bdba-aa121307314a",
   "metadata": {},
   "source": [
    ">以下是进行qwen2.5:72b模型进行下载&运行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603daf7-dc03-4844-9647-e4690c44cb9b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023151317687.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b4019-0742-4563-a7bc-4bebc1654f58",
   "metadata": {},
   "source": [
    "下载70B的模型只需要一行指令即可完成部署："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd6707-e96c-4e2f-9017-84183bbafc3c",
   "metadata": {},
   "source": [
    "```\n",
    "ollama run qwen2.5:72b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32a9a2-b7fa-432c-aee1-674d386fb60a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022112005646.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f39781-2e33-4512-bbb3-f5cdeaf69348",
   "metadata": {},
   "source": [
    "完成下载后会直接进入模型启动状态，如果退出或刷新界面，再次输入指令`ollama run qwen2.5:72b`即可启动对应模型。\n",
    "可以看到推理所需的计算资源相对均匀的分布在每张工作显卡上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9f168-da10-4864-ab59-d3e325c48b7d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022112658854.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1f38b-5ee5-4b45-b1fa-cdc4b1f1dd9d",
   "metadata": {},
   "source": [
    "对该模型进行数学能力测试后可以看到，它无论在应对简单的数学陷阱题，还是中等难度的数学问题时，都能作出准确且逻辑清晰的回答。这表明该模型的指令微调（instruct）效果显著，具备良好的数学理解和推理能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f13b5-3fcf-454d-a881-7c95eeee95c6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022113049403.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb38cbb7-2d78-4aa2-874e-3510253bd6ee",
   "metadata": {},
   "source": [
    ">使用 /bye 退出正在进行的 Ollama进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebc3bf-359c-4706-8733-4e2aa7ebddc2",
   "metadata": {},
   "source": [
    "### 4.3 如何卸载安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b961a4-8277-4040-8a7c-2ea895bdf061",
   "metadata": {},
   "source": [
    "直接在你的安装目录下，删除ollama文件夹即可。所有下载的数据和大模型文件都在里面，Ollama 的默认安装目录通常是用户主目录下的 .ollama 文件夹。例如，在 Unix 系统（如 macOS 和 Linux）上，默认安装目录为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed637c80-6be2-44d9-8916-ebe5027def41",
   "metadata": {},
   "source": [
    "~/.ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7dae46-fbb6-4b48-b919-3cb35650f3c1",
   "metadata": {},
   "source": [
    "或可以通过指令 `ollama rm modlename`的方式来移除对应的大模型文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f5bb1-3fa5-4af8-a51d-54769a2e139f",
   "metadata": {},
   "source": [
    "# 5. 在Windows环境下进行Ollama方式下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70facb5-01bf-4947-89a0-b9c79bfd35f9",
   "metadata": {},
   "source": [
    "Ollama同样支持在Windows环境下进行部署下载，使用此方法下载意味着你可以无需调整任何硬件设备直接在你的主机上实现Llama 3.2模型的安装部署和推理使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7de515-3629-433a-8086-391d1b17f2bb",
   "metadata": {},
   "source": [
    "首先要下载Windows版的Ollama文件，进入官网选择下图的内容进行下载："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6d89-2ebf-4ad4-a165-13f75d1e80a4",
   "metadata": {},
   "source": [
    "https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4511bd-c5e9-4325-b5f4-0ae3e23008b9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241015180846073.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462ad80-71a1-4701-920f-a8649a689ff9",
   "metadata": {},
   "source": [
    "点击Download for window即可实现下载安装包，文件大小为664MB。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b253785-5d60-48b8-b368-193e0d356bac",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241015181037931.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff54183-6689-42a2-b228-2b3b4ebe8b78",
   "metadata": {},
   "source": [
    "下载完成后在对应文件夹双击打开安装包完成安装流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53779058-f24c-4530-84d0-0c794b7c4030",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241015184929499.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131ddaf-7527-4aa8-a37d-3aeba7f55e00",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241016102814619.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d494da4-bb1d-4f40-a8e4-50efa04afca3",
   "metadata": {},
   "source": [
    "框架和模型默认路径都是C盘，为了主机内存管理方便，可以通过以下步骤来实现模型下载地址的切换。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5261319-bee1-4a68-b346-d987355466d3",
   "metadata": {},
   "source": [
    "1.在桌面下方搜索栏输入高级系统设置并进入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6bb38-116e-4a48-ad76-1aa9e457c663",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241016103219333.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58b8d9-2533-4cea-ac94-21b3e7604904",
   "metadata": {},
   "source": [
    "在系统设置中找到`高级`的标签，点击下方的环境变量选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e5085-bca8-4efd-89c3-2fa003f496b6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241016103304360.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71f762-a0e1-4e8c-bc37-19a51211746d",
   "metadata": {},
   "source": [
    "在系统变量栏点击新建，变量名设置为OLLAMA_MODELS,变量值设置为你想要的路径位置，这里设置为D:\\Ollama\\models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24815544-0753-4505-8a3a-959abbfa8af2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241016103947874.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848d21a-6e86-4cd9-920a-a3e154abfba6",
   "metadata": {},
   "source": [
    "设置好路径之后可以在命令行中进行检查，通过以下命令实现："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1fcf6-8862-4d8e-86f6-8003667b4b7c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241016104050213.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff51430-bef4-439b-ad33-3699472a87f5",
   "metadata": {},
   "source": [
    "在完成Ollama部署之后，系统会自动启动ollama的程序，界面如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081136c-f736-487e-aec2-03cbfc02d8c5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241016104224705.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ba5c1-ea31-423c-a60b-744d331b1416",
   "metadata": {},
   "source": [
    "在Ollama官网支持的模型界面可以找到对应可推理的模型的部署代码，以下以Qwen2.5：0.5B为例进行演示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d2098-da17-466e-885c-eb47d1b5975f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025104913395.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e54b6-47ed-42a8-a67d-5eee0f38a2af",
   "metadata": {},
   "source": [
    "在Windows平台运行和在Linux系统中进行的指令一样，执行`ollama run qwen2.5:0.5b`即可实现该模型的下载和运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cceb821-ee26-43d4-a420-9ed96b5bd7f6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025104940753.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d1f66-3c45-4c45-a3e2-8db60ad06147",
   "metadata": {},
   "source": [
    "同样的，在完成模型下载之后会直接开启模型的推理任务，可以通过命令行的方式进行模型对话交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a46a05-c386-460a-bb8a-63905a8ed258",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025105307601.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35146b7-fd57-465f-855a-5ac9c37b5d29",
   "metadata": {},
   "source": [
    "再次启动Ollama可以直接在Windows的命令行中实现，通过win+R键启动cmd命令，执行`ollama run qwen2.5:0.5b`便可以开启对话，实例所演示的是qwen2.5系列最小的模型，通过测试仅在资源有限的纯CPU环境下也可以流畅运行，这展现出未来AIPC的一种可能方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bef676-f735-4a3c-a21b-3604324a0e97",
   "metadata": {},
   "source": [
    "# 6. 使用vLLM框架进行推理部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4266d66-c0d3-45fa-95df-5caec6e6f3ac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先来看一下什么是vLLM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8cad3-d007-4381-a57c-76f7c73fd644",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM 是一个用于大型语言模型推理和服务的快速且易于使用的库，简单理解就是一个大模型的推理框架。这是一个用于快速 LLM 推理和服务的开源库。它提供的吞吐量比 HuggingFace Transformers 高 24 倍，而无需更改任何模型架构。vLLM在延迟方面也有显著优势。\n",
    "GLM4的官方集成了vLLM开源框架来实现部署和加速推理。\n",
    "\n",
    "&emsp;&emsp;vLLM论文地址：https://arxiv.org/pdf/2309.06180\n",
    "\n",
    "&emsp;&emsp;vLLM官方地址：https://docs.vllm.ai/en/latest/index.html\n",
    "\n",
    "&emsp;&emsp;vLLM支持模型列表\n",
    "https://docs.vllm.ai/en/latest/models/supported_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d05c06-f7c7-4779-9f73-05053441c526",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401292226013.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5923bc-de24-463b-b020-8510cc1f113e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM有几个优化点值得我们去关注： 其一是*PagedAttention* 高效管理注意力键值内存，其二是vLLM支持多GPU和多节点推理，适用于大规模部署场景。其三，vLLM支持多种量化方案，如GPTQ、AWQ和FP8，优化了CUDA内核以提高推理效率。最后，它还可以为大语言模型构建API服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08396b-0bf0-4a1b-8f9d-6f496692bdd6",
   "metadata": {},
   "source": [
    "### 6.1 vLLM安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3157567-c901-4bcb-ba80-bca8e410b2f1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202402011332935.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45f1c5-6644-42ad-8538-ad786b0c87a1",
   "metadata": {},
   "source": [
    "因为与HuggingFace的无缝集成，才导致这个工具非常易用。所以vLLM与其他的工具包使用方式一样，要使用vLLM框架集成GLM4模型，首要做的事情就是安装vLLM这个依赖包。这里有提供安装教程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6261926-08df-48ff-8c89-f1db2597a907",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM框架对Python版本和CUDA版本的要求比较严格：\n",
    "\n",
    "- Python: 3.8 ~ 3.11\n",
    "- torch >= 2.0\n",
    "- cuda 11.8 or 12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e05ac-249e-4e06-a649-b0f5bc772a4d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM 对 torch 版本要求较高，且越高的版本对模型的支持更全，效果更好。如果不满足上述情况，会在安装过程中报错，导致无法安装，所以需要严格按照上述要求检查自己的环境。如确实无法满足，只能放弃使用vLLM框架推理加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1662b-a78f-41e8-a48d-ddd9b5238972",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先验证当前的开发环境，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9d612-e0b0-4654-b122-cb1ef46481db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# 打印 Python 版本\n",
    "print(\"Python 版本:\", sys.version)\n",
    "\n",
    "# 打印 PyTorch 版本\n",
    "print(\"PyTorch 版本:\", torch.__version__)\n",
    "\n",
    "# 打印 CUDA 版本（如果 CUDA 可用）\n",
    "if torch.cuda.is_available():\n",
    "    cuda_version = torch.version.cuda\n",
    "    print(\"CUDA 版本:\", cuda_version)\n",
    "else:\n",
    "    print(\"CUDA 不可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bab29b-9883-4d33-b082-31d518ec8434",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924175008707.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e019b-ce7f-4ee7-a723-5b64590dda32",
   "metadata": {},
   "source": [
    "Qwen2.5系列所需的vLLM版本需要大于 `vLLM>=0.4.0`，在实际部署安装的时候可以通过`pip install vllm -U`命令来下载最新的版本，或者可以通过`pip install vllm==0.4.3`来确定下载指定的版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdf57f-e288-4ce2-8dea-23814d4ff620",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241024103106902.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870489f9-1708-438f-823c-481448ea7db2",
   "metadata": {},
   "source": [
    "安装完成后可以通过以下命令查看下载是否成功，以及相应的版本信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f9384-1fa7-460e-82f3-bc611f6b5183",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241024103043928.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a671b-c3d8-4692-a46d-a09f66f09aea",
   "metadata": {},
   "source": [
    "## 6.2 使用vLLM进行推理部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bf983-f5da-439d-9bbc-3ebdcf9740f4",
   "metadata": {},
   "source": [
    "如果你此前没有下载Qwen2.5：7B的模型，终端运行下面的代码会自动从huggingface拉取模型（这里需要魔法才能拉取成功）："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc7b4b-b614-4ae9-84a3-395001b3279f",
   "metadata": {},
   "source": [
    "python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81c31d-449a-470e-b40a-b591a1747cb5",
   "metadata": {},
   "source": [
    "默认情况下，vLLM 会从 Hugging Face 下载模型。但如果你希望使用 ModelScope 上的模型，可以通过设置环境变量来切换源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8106e-0e5c-4d87-b99b-2ddabef37178",
   "metadata": {},
   "source": [
    "export VLLM_USE_MODELSCOPE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2696c-d8a9-4301-bbfb-89b9eb5d1261",
   "metadata": {},
   "source": [
    "如果已经通过其他方式完成下载（如：ModelScope本地化下载的方式），通过魔搭社区在本地下载qwen-2__5-7B-instruct的模型，直接加载本地模型启动一个API服务器来运行Qwen-2__5-7B-Instruct 的服务即可。你需要修改上面的代码--model 后的内容为你本地模型的存储路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d4233-0ef5-49fd-b5ff-34b48500ed36",
   "metadata": {},
   "source": [
    "python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2___5-7B-Instruct --model /home/LLM/qwen2_5/hub/qwen/Qwen2___5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50590e8c-102d-477e-ba46-267f2c3c79da",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025113325433.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56757f9-cf91-49d5-b8ec-8514d4e9229f",
   "metadata": {},
   "source": [
    "在加载完全部的权重模型之后会开启openai风格的推理流程，在对应的网址端口就可以进行API调用对话了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0887bd-c9f9-4938-8f27-268d4cc72e6f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241024105131797.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc715429-f731-4046-b5a6-5d824edac9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='cmpl-9122c54c70ee43a88ba86ef5a1f5c062', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='我是一个人工智能助手，由阿里云开发和运营，旨在为用户提供高效、便捷的信息查询和问题解答服务。我的主要功能包括但不限于：\\n\\n1. **快速搜索信息**：能够迅速从海量数据中检索和提供所需信息，涵盖科技、文化、教育、生活等多个领域。\\n2. **智能问答**：能够理解和回答各种问题，无论是专业知识、日常生活常识还是技术细节，都能提供准确的答案或指导。\\n3. **语言翻译**：支持多种语言之间的翻译，帮助用户跨越语言障碍进行沟通交流。\\n4. **语音识别与合成**：能够将文字转为语音，或者将语音转换为文字，提供更加自然、人性化的交互体验。\\n5. **个性化推荐**：根据用户的历史行为和偏好，提供个性化的信息推荐和服务。\\n6. **操作助手**：在使用智能设备或应用程序时提供辅助指导，帮助用户更高效地完成任务。\\n\\n我致力于通过不断学习和进步，为用户提供更加智能、贴心的服务，帮助解决日常生活和工作中遇到的问题。无论是学习、工作还是娱乐，我都是您可靠的助手。', role='assistant', function_call=None, tool_calls=None), stop_reason=None)], created=1717747911, model='Qwen2-7B-Instruct', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=230, prompt_tokens=21, total_tokens=251))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个人工智能助手.\"},\n",
    "        {\"role\": \"user\", \"content\": \"介绍一下你自己.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c3e94-7c42-453f-a489-e69fe2c790d6",
   "metadata": {},
   "source": [
    "\n",
    "以下方法可用于在开发模式下使用 vLLM 框架进行模型推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fb51a6-b115-42bc-99eb-8abb6680be96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:27:04.283915Z",
     "iopub.status.busy": "2024-10-25T06:27:04.283593Z",
     "iopub.status.idle": "2024-10-25T06:27:09.105354Z",
     "shell.execute_reply": "2024-10-25T06:27:09.104788Z",
     "shell.execute_reply.started": "2024-10-25T06:27:04.283894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db33387-9906-4d40-bb24-19e36ac13be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:27:13.722910Z",
     "iopub.status.busy": "2024-10-25T06:27:13.722594Z",
     "iopub.status.idle": "2024-10-25T06:27:13.725799Z",
     "shell.execute_reply": "2024-10-25T06:27:13.725306Z",
     "shell.execute_reply.started": "2024-10-25T06:27:13.722891Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f7a314-0b37-41da-a278-72850e865a94",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-25T06:37:02.710567Z",
     "iopub.status.busy": "2024-10-25T06:37:02.710228Z",
     "iopub.status.idle": "2024-10-25T06:37:35.408761Z",
     "shell.execute_reply": "2024-10-25T06:37:35.408244Z",
     "shell.execute_reply.started": "2024-10-25T06:37:02.710546Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-25 14:37:03 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 10-25 14:37:19 model_runner.py:255] Loading model weights took 14.2409 GB\n",
      "INFO 10-25 14:37:19 gpu_executor.py:84] # GPU blocks: 5136, # CPU blocks: 4681\n",
      "INFO 10-25 14:37:22 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-25 14:37:22 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-25 14:37:35 model_runner.py:1117] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\", max_model_len=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a72bbc1-d4c3-4256-95b9-2fb589fa8a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:37:37.166756Z",
     "iopub.status.busy": "2024-10-25T06:37:37.166403Z",
     "iopub.status.idle": "2024-10-25T06:37:37.777553Z",
     "shell.execute_reply": "2024-10-25T06:37:37.777054Z",
     "shell.execute_reply.started": "2024-10-25T06:37:37.166737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  6.71it/s, est. speed input: 36.91 toks/s, output: 107.38 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Josh. I'm 15 and a rising 10th grader\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' the head of state and head of government of the United States. The president also'\n",
      "Prompt: 'The capital of France is', Generated text: ' located in which part of the country?\\nThe capital of France is Paris, which'\n",
      "Prompt: 'The future of AI is', Generated text: ' here. Companies, from startups to established businesses, are adopting AI in their operations'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef020e5-4757-44e1-8227-63ef3ee5f4fe",
   "metadata": {},
   "source": [
    "或者我们可以通过创建一个运行脚本来实现单轮对话，在命令行通过`vim run.py`来创建一个vllm启动脚本，将以下代码复制到文件中，进行必要的修改后保存退出："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ff0d2-45a2-41e1-9f90-c378c79cfd32",
   "metadata": {},
   "source": [
    "```python\n",
    "from vllm import LLM, SamplingParams\r\n",
    "from transformers import AutoTokenizer\r\n",
    "import os\r\n",
    "import json\r\n",
    "\r\n",
    "# 自动下载模型时，指定使用modelscope。不设置的话，会从 huggingface 下载\r\n",
    "os.environ['VLLM_USE_MODELSCOPE']='True'\r\n",
    "\r\n",
    "def get_completion(prompts, model, tokenizer=None, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):\r\n",
    "    stop_token_ids = [151329, 151336, 151338]\r\n",
    "    # 创建采样参数。temperature 控制生成文本的多样性，top_p 控制核心采样的概率\r\n",
    "    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)\r\n",
    "    # 初始化 vLLM 推理引擎\r\n",
    "    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)\r\n",
    "    outputs = llm.generate(prompts, sampling_params)\r\n",
    "    return outputs\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    # 初始化 vLLM 推理引擎\r\n",
    "    model='/home/LLM/qwen2_5/hub/qwen/Qwen2___5-7B-Instruct' # 指定模型路径\r\n",
    "    # model=\"qwen/Qwen2-7B-Instruct\" # 指定模型名称，自动下载模型\r\n",
    "    tokenizer = None\r\n",
    "    # 加载分词器后传入vLLM 模型，但不是必要的。\r\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) \r\n",
    "\r\n",
    "    text = [\"你好，帮我介绍一下什么时大语言模型。\",\r\n",
    "            \"可以给我将一个有趣的童话故事吗？\"]\r\n",
    "    # messages = [\r\n",
    "    #     {\"role\": \"system\", \"content\": \"你是一个有用的助手。\"},\r\n",
    "    #     {\"role\": \"user\", \"content\": prompt}\r\n",
    "    # ]\r\n",
    "    # 作为聊天模板的消息，不是必要的。\r\n",
    "    # text = tokenizer.apply_chat_template(\r\n",
    "    #     messages,\r\n",
    "    #     tokenize=False,\r\n",
    "    #     add_generation_prompt=True\r\n",
    "    # )\r\n",
    "\r\n",
    "    outputs = get_completion(text, model, tokenizer=tokenizer, max_tokens=512, temperature=1, top_p=1, max_model_len=2048)\r\n",
    "\r\n",
    "    # 输出是一个包含 prompt、生成文本和其他信息的 RequestOutput 对象列表。\r\n",
    "    # 打印输出。\r\n",
    "    for output in outputs:\r\n",
    "        prompt = output.prompt\r\n",
    "        generated_text = output.outputs[0].text\r\n",
    "        print(f\"Prompt: {pr```ompt!r}, Generated text: {generated_text!r}\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47e1af-f0b8-4584-90a8-9d9d80dd48d2",
   "metadata": {},
   "source": [
    "在命令行使用`python run.py`来启动这个脚本，在模型加载完成后会返回推理结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925590b-7a58-4f0a-ac19-f96f037890a3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025161057315.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a14c5-e683-4019-a52d-e54a40439368",
   "metadata": {},
   "source": [
    "可以通过直接在 Python 环境中调用 vLLM 进行推理。通过以下方式在 Python 中启动 vLLM 并指定所需的大模型，该方法更为简便高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e4d61-2d10-400a-8af2-9ae748755003",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025162708674.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33e17c-e0b3-4a9d-bc1f-90061ce763b4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241025162609237.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f787f66-5e49-41c1-ae34-30f928c844ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
