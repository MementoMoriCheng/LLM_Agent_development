{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641fd304-96bb-4c7f-b019-a0cb0a476357",
   "metadata": {},
   "source": [
    "# <center>GLM4-9b模型基本介绍 & Ubuntu环境下安装办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e8aa5-a677-4196-b414-d9a9f1264067",
   "metadata": {},
   "source": [
    "目录\n",
    "\n",
    "一 GLM4系列介绍<br>\n",
    "&emsp;1 GLM4生态介绍<br>\n",
    "&emsp;2 GLM-4-9B模型性能介绍<br>\n",
    "二 Ubuntu环境GLM4-9b安装办法<br>\n",
    "&emsp;1 环境准备<br>\n",
    "&emsp;2 项目文件下载<br>\n",
    "&emsp;3 权重文件下载<br>\n",
    "三 GLM-4-9b基本启动方式<br>\n",
    "&emsp;1 快速启动demo<br>\n",
    "&emsp;2 命令行启动办法<br>\n",
    "四 vLLM推理框架介绍与启动办法<br>\n",
    "&emsp;1 Transfomer推理框架介绍<br>\n",
    "&emsp;2 vLLM推理框架介绍<br>\n",
    "&emsp;3 vLLM推理GLM4-9b模型实战<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc4bee-a575-4416-b361-30cb620037cb",
   "metadata": {},
   "source": [
    "## 1. GLM4生态介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943af8c1-fa4e-40ae-8e5a-57c10066efef",
   "metadata": {},
   "source": [
    "- 整体生态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ca3fd-56bc-452c-9ccb-ea81f8f46098",
   "metadata": {},
   "source": [
    "2024 年 1 月，智谱举行开发者大会“DevDay”，发布了新一代对标 OpenAI GPT-4 的基座大模型 GLM-4。GLM-4在以中文为主的应用场景中实际性能可逼近GPT-4的95%，在部分中文对齐的测试中，甚至超过GPT-4，逼近GPT-4-turbo模型。同时，GLM-4支持128K的超长上下文对话，支持多模态功能，支持更快的推理、更多并发，大大降低了推理成本。\n",
    "\n",
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202401171835518.png\" alt=\"img\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e39fa6-3d1c-49ae-b295-d810556d690d",
   "metadata": {},
   "source": [
    "智谱 可以说是国内最早研发大模型的企业之一。智谱成立于2019年6月11日，由清华大学知识工程实验（KEG）技术成果转化而来，团队的核心人员来自清华，是此次大模型浪潮中清华系参与的代表。作为最早参与中国大模型研究的创业团队之一，智谱是最早研发出千亿参数大模型的公司之一，早在2022 年 8 月，智谱发布了双语千亿级超大规模预训练模型 GLM-130B，支持中英双语，在多个公开评测集上性能超过GPT-3。2023 年 ChatGPT 爆发后，智谱AI仅用了两个月的时间就成功复刻 ChatGPT，开发出了 ChatGLM。\n",
    "\n",
    "今天我们所学习的GLM4和此前ChatGLM3模型所不同的是，它最开始并没有采用开源模式，而是采用了OpenAI和Google大模型的在线大模型模式，即模型无需本地部署，而是通过联网的方式调用智谱算力中心的模型进行推理或微调，用户通过API-KEY进行身份验证，同时根据实际使用模型不同、以及不同的Token数量进行计费。智浦AI大模型Maas开放平台：https://maas.aminer.cn/\n",
    "\n",
    "![](https://fufanshare.oss-cn-beijing.aliyuncs.com/Data_Analysis_AI/Ch1_GLM/zhipu_web.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c20be-bc21-4806-8d37-eb4b941bd559",
   "metadata": {},
   "source": [
    "在线上版本中我们可以在官网的[【开发资源-使用指南】](https://maas.aminer.cn/dev/howuse/introduction)看到目前上线的大模型。\n",
    "\n",
    "![](https://fufanshare.oss-cn-beijing.aliyuncs.com/Data_Analysis_AI/Ch1_GLM/zhipu_model_show.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73086a98-d0e2-40a5-aa17-f0e8b4590b51",
   "metadata": {},
   "source": [
    "这里不难看出，整体模型情况都在向OpenAI对齐：  \n",
    "GLM-3-turbo对标GPT3.5  \n",
    "cogView-3对标DALL.E-3  \n",
    "Embedding-2对标text-Embedding-ada-2  \n",
    "由此可见，对于我们用户来说我们就有了“平替”选择，而且对于一套生态体系的熟悉可以快速迁移到另一套大模型的生态体系，大大缩减了我们的学习使用门槛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653fd80-e29b-4ea0-8a3c-11c60bc1ae55",
   "metadata": {},
   "source": [
    "- 闭源模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df481d48-1252-489f-82c3-9b1728279f1e",
   "metadata": {},
   "source": [
    "在24年6月5日，GLM-4-9B是智谱AI推出的最新一代预训练模型GLM-4系列的开源模型版本。其中包括了base、chat、vision和长文本四个模型，GLM-4-9B 模型具备了更强大的推理性能、更长的上下文处理能力、多语言、多模态和 All Tools 等突出能力。在数据评分上超越了Llama-3-8B的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee61b29-58e4-47aa-acc0-bc7b9bde9e2b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://fufanshare.oss-cn-beijing.aliyuncs.com/Data_Analysis_AI/Ch2_OpenSourceLLM/GLM4.jpg\" width=90%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac117aa4-3794-4ae6-8bd4-fd289bb3b480",
   "metadata": {},
   "source": [
    "作为国内一流的大模型产品，具有丰富的功能应用和社区生态，随着ChatGLM模型系列从ChatGLM-1发展到ChatGLM-4，我们观察到了模型架构和设计理念的持续演进。这一演变过程体现了几个关键的技术进步：\n",
    "1. 模型深度的扩展：ChatGLM-4的模型深度已经扩展到40层，这显著提升了模型的表示能力，使其能够捕捉更复杂的数据特征。\n",
    "2. 词汇表的动态调整：随着版本的迭代，词汇表的大小也在不断优化，以适应不同的模型复杂性和实际应用场景的需求。\n",
    "3. 归一化技术的革新：从传统的LayerNorm到先进的RMSNorm，归一化方法的改进不仅提升了模型的训练效率，也增强了模型的稳定性。\n",
    "4. 注意力机制与多层感知机（MLP）的优化：在这两个关键领域进行的调整和优化，进一步提高了模型的计算效率和数据处理能力。\n",
    "这些技术的迭代和优化，展现了ChatGLM系列模型在追求更高性能、更高效率以及更好地适应实际应用需求方面的不懈努力。通过这些改进，ChatGLM模型系列在自然语言处理领域中的表现越来越出色，为各种复杂的语言任务提供了强大的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558ba06-ed59-4c41-b7ac-f27eb9bcb451",
   "metadata": {},
   "source": [
    "官方传送门：\n",
    "\n",
    "Github代码仓库：Github：https://github.com/THUDM/GLM-4  \n",
    "模型：  \n",
    "huggingface：https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7  \n",
    "魔搭社区：https://modelscope.cn/organization/ZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e498046-2738-4f97-a673-49976a4357ab",
   "metadata": {},
   "source": [
    "## 2. GLM-4-9B模型性能介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c41816-3f22-4926-ba82-8496d157cd65",
   "metadata": {},
   "source": [
    "基座模型典型任务评分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307241db-148d-4edf-80e9-bff3173423c4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925175211912.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0039b-bdad-46d7-8e19-855838594965",
   "metadata": {},
   "source": [
    "其中，这些测评项目重心如下：\n",
    "- MMLU是一个包括自然语言处理、常识推理、数学、科学等的多任务评估基准，旨在测试语言模型在多种任务上的理解能力。\n",
    "- C-EVAL 是一个专门用于评估中文语言模型的基准。它包括各种中文任务，如阅读理解、文本生成和情感分析等。\n",
    "- GPQA 测试模型的推理能力和知识整合能力。强调模型在不确定性和模糊性条件下的表现。\n",
    "- GSM8K 是一个用于评估语言模型在解决小学数学问题能力的基准。它包含大约 8,000 道数学题，涵盖了各种主题，包括代数、几何、时间、货币等。\n",
    "- MATH 是一个用于评估数学问题解决能力的基准，该基准包含各种数学问题，如代数、几何和微积分等。\n",
    "- HumanEval 是一个用于评估代码生成和编程能力的基准，通常用在测试语言模型生成可执行代码的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0c55a-fc4d-4041-a77c-fbebbbdd29f8",
   "metadata": {},
   "source": [
    "**多模态能力**\n",
    "\n",
    "基于GLM基座的开源多模态模型GLM-4V-9B。这一模型采用了与CogVLM2相似的架构设计，能够处理高达1120 x 1120分辨率的输入，并通过降采样技术有效减少了token的开销。为了减小部署与计算开销，GLM-4V-9B没有引入额外的视觉专家模块，采用了直接混合文本和图片数据的方式进行训练，在保持文本性能的同时提升多模态能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c2cca-3756-4d78-8380-fed384c48562",
   "metadata": {},
   "source": [
    "**检索能力**\n",
    "\n",
    "GLM-4-9B模型具有较强的检索能力，以下是在1M上下文长度进行大海捞针实验，可以发现是可以100%得到信息检索。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf55ad-de33-4cd2-b74f-a91ed5293485",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924150137851.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa8765-df2d-4969-985b-ebd802e7af31",
   "metadata": {},
   "source": [
    "**多语言能力**\n",
    "\n",
    "GLM-4-9B模型支持26种语言，包括了中文、英语、法语、日语、韩语等主流语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701310c8-3e47-4fc1-90df-a9ff0988e276",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924150202325.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb703873-5aae-4e5e-942c-921ebed34230",
   "metadata": {},
   "source": [
    "**“All Tools”**函数调用能力\n",
    "\n",
    "![](https://fufanshare.oss-cn-beijing.aliyuncs.com/Data_Analysis_AI/Ch2_OpenSourceLLM/GLM4-alltools.png)\n",
    "\n",
    "模型能够理解和使用一系列外部工具（比如代码执行、联网浏览、画图、文件操作、数据库查询、API 调用等）来辅助回答问题或完成任务。  \n",
    "在 1 月 16 日的 Zhipu DevDay 上，GLM-4 模型全线升级了 All Tools 能力，模型可以智能调用网页浏览器、代码解释器、CogView 来完成用户的复杂请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2817f-5eed-447b-a7b2-4d8474d23a89",
   "metadata": {},
   "source": [
    "“All Tools”原理：自主理解用户意图→规划复杂的指令→顺序调用一个或多个工具(例如支持嵌入式Python解释器、Web浏览器、文本到图像模型)→完成复杂的任务(如支持用户定义的函数、API和外部知识库)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937deab-c34f-4873-a60a-c9758ce66cf0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924150228619.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844019d8-b72f-4973-ab47-608f00277db6",
   "metadata": {},
   "source": [
    "## 3. Ubuntu环境下GLM4-9b-chat安装办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bffdc-d9e2-4bb2-971b-b3b1eb84cf70",
   "metadata": {},
   "source": [
    "对于部署GLM4-9b-chat模型，官方给出的最低硬件要求是Python版本大于等于3.10，内存不小于32GB，推荐的环境是在Linux操作系统环境下部署，同时使用大于8GB显存并且支持CUDA或ROCM且支持`BF16`推理的GPU设备（`FP16`精度无法训练，推理小概率出现问题），以下是官方测试运行所使用的硬件信息仅供参考："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3915fb-704f-4cbc-96d5-f4e47959a872",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925151714324.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82326deb-28d0-4a39-b2c2-c2068407e239",
   "metadata": {},
   "source": [
    "- **Step 1. 创建conda虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b6c09-10e7-4d3b-a072-27f6db4f5098",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Conda创建虚拟环境的意义在于提供了一个隔离的、独立的环境，用于Python项目和其依赖包的管理。每个虚拟环境都有自己的Python运行时和一组库。这意味着我们可以在不同的环境中安装不同版本的库而互不影响。根据官方文档信息建议Python版本3.10以上。创建虚拟环境的办法可以通过使用以下命令创建：\n",
    "\n",
    "```bash\n",
    "# GLM4 是你想要给环境的名称，python=3.11 指定了要安装的Python版本。你可以根据需要选择不同的名称和/或Python版本。\n",
    "\n",
    "conda create -n glm4 python=3.11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8ae9d-c0f5-497a-8db3-ee5822ff545a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924162412814.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a1c7-4a4c-489c-a39b-b1ee95b548d6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;创建虚拟环境后，需要激活它。使用以下命令来激活刚刚创建的环境。如果成功激活，可以看到在命令行的最前方的括号中，就标识了当前的虚拟环境。虚拟环境创建完成后接下来安装torch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1961157-e0ed-4693-a850-3c5bcbaf7498",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924162442522.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1966bc-db59-42b7-ad31-dfe5241d7f20",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果忘记或者想要管理自己建立的虚拟环境，可以通过conda env list命令来查看所有已创建的环境名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff9c0a-1113-4051-8fab-849bdbbf513a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143512354.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc26da-53e8-4e6b-850f-dfa3a696c7c9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果需要卸载指定的虚拟环境则通过以下指令实现：\n",
    "```\n",
    "conda env remove --name envname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047231f7-a14e-4986-851d-739bcb5fd181",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143643575.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e243123-f2c2-488e-afdc-304ed1e14aca",
   "metadata": {},
   "source": [
    "- 需要注意的是无法卸载当前激活的环境，建议卸载时先切换到base环境中再执行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c936aa-1ade-4d3f-ab6f-73b9749324aa",
   "metadata": {},
   "source": [
    "- **Step 2. 查看当前驱动最高支持的CUDA版本**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b91d1f-04a4-4f13-a78c-8c10114b16fd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们需要根据CUDA版本选择Pytorch框架，先看下当前的CUDA版本：\n",
    "```\n",
    "nvidia -smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b5ef8-8b04-455d-afad-ad082c38981b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924161818464.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdc590-c2bf-41d8-8abb-4d591c46beb5",
   "metadata": {},
   "source": [
    "- **Step 3. 在虚拟环境中安装Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a615d-dc1c-4af8-8f37-41e25b3f3a3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进入Pytorch官网：https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4245740-61d9-409e-af00-42c990e9c25f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240103163206436.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36c516-17a2-4d1f-9f6d-0e7990e0f68e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当前的电脑CUDA的最高版本要求是12.2，所以需要找到不大于12.2版本的Pytorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87d9f5-a6e0-40de-add9-b71a357d9dfb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041455184.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bbee7-5942-480c-88b5-d7f6e30786a0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直接复制对应的命令，进入终端执行即可。这实际上安装的是为 CUDA 12.1 优化的 PyTorch 版本。这个 PyTorch 版本预编译并打包了与 CUDA 12.1 版本相对应的二进制文件和库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3c620-5cad-4d8b-9ba7-51c96bbec249",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924163535040.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162bd94e-402c-488b-b0a6-d85782081220",
   "metadata": {},
   "source": [
    "- **Step 4. 安装Pytorch验证**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6801e4-1c03-4bf6-9f79-c09e5332296d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;待安装完成后，如果想要检查是否成功安装了GPU版本的PyTorch，可以通过几个简单的步骤在Python环境中进行验证：\n",
    "```bash\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ad078-d449-4571-bc01-570609a08ba9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924164006735.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf9f60-71a3-4de1-8c06-cc93a9d51d15",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果输出是版本号数字，则表示GPU版本的PyTorch已经安装成功并且可以使用CUDA，如果显示ModelNotFoundError，则表明没有安装GPU版本的PyTorch，或者CUDA环境没有正确配置，此时根据教程，重新检查自己的执行过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41d082-095a-40a8-a003-1b24ac6283f4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924164131600.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca41272-1a93-4b70-9b63-22b3243df117",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然通过pip show的方式可以很简洁的查看已安装包的详细信息。pip show <package_name> 可以展示出包的版本、依赖关系（展示一个包依赖哪些其他包）、定位包安装位置、验证安装确实包是否正确安装及详情。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df523d-a754-4afa-a5c8-d1d7ce8d0354",
   "metadata": {},
   "source": [
    "- **Step 5. 下载GLM4的项目文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf0ac1-1cd3-4f48-b9a9-a74f2931a38f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;GLM4-9b-chat的代码库和相关文档存储在 GitHub 这个在线平台上。GitHub 是一个广泛使用的代码托管平台，它提供了版本控制和协作功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af3e95-6681-41e2-9f3f-1aac7f7a6344",
   "metadata": {},
   "source": [
    "&emsp;&emsp;要下载GLM4-9b-chat的项目文件，需要进入GLM4的Github：https://github.com/THUDM/GLM-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aaaa80-2cad-4eac-b7db-9678f573df22",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925153722227.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00152b14-ce8b-4327-8285-42548363eb97",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在 GitHub 上将项目下载到本地通常有两种主要方式：克隆 (Clone) 和 下载 ZIP 压缩包。\n",
    "\n",
    "&emsp;&emsp;克隆 (Clone)是使用 Git 命令行的方式。我们可以克隆仓库到本地计算机，从而创建仓库的一个完整副本。这样做的好处是我们可以跟踪远程仓库的所有更改，并且可以提交自己的更改。如果要克隆某一个仓库，可以使用如下命令：\n",
    "\n",
    "```bash\n",
    "git clone <repository-url>   # 其中 <repository-url> 是 GitHub 仓库的 URL。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12199b-b1a3-4587-af31-06a28b609898",
   "metadata": {},
   "source": [
    "&emsp;&emsp;推荐使用克隆 (Clone)的方式。对于GLM4这个项目来说，我们首先在GitHub上找到其仓库的URL。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c0e51-cff8-4362-b5b6-a7c817c63d6c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924184100868.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdea07-7953-4162-888f-73e331f787b0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在执行命令之前，先安装git软件包。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0d145-0791-4bad-91d0-6a49db9204ae",
   "metadata": {},
   "source": [
    "```bash\n",
    "sudo apt install git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26971b70-6307-4663-8e74-301d444d2abc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924165452009.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d6335-bb4b-4e63-b5ab-9bd185f8ff3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后通过mkdir命令创建一个存放GLM4-9b-chat项目文件的文件夹。\n",
    "```\n",
    "mkdir GLM4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a52c3e-fabd-4047-a573-cf0a681937f1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924165401180.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9c4cc-cd96-4a85-8e5d-04a91e45e840",
   "metadata": {},
   "source": [
    "&emsp;&emsp;执行克隆命令，将Github上的项目文件下载至本地。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6e70e-eca5-46ee-ad88-b09818546d6e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924165728527.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32b4b8-3ec3-4572-a641-b34d4a6b9b00",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果克隆成功，本地应该会出现如下文件内容："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06611c-1811-4db3-a819-e92a31c5af6c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925154501679.png\" width=120%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ea498-fe0f-48b6-9d3c-2691d9461dc9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除了直接通过git clone的方式拉取代码至本地，也可以直接下载压缩包。这是更简单的下载方式，不需要使用 Git，适合那些不打算使用 Git 版本控制的用户。在 GitHub 仓库页面上，通常会有一个“Download ZIP”按钮，我们可以点击这个按钮下载仓库的当前状态的压缩包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5439b-e33d-4778-aa1b-41d9af3aa778",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925154639917.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fbb2a-fda1-42f3-a194-d0a18dab776c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下载后，只需解压缩该文件即可访问项目文件。压缩包中存放的是项目文件和git方法下载的内容一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e316a-4b1b-4d9a-bce6-7d295625eb6f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925154825491.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4d300-171a-4c8a-812c-bd20baf37e82",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过这种方式下载的项目文件，需要xftp这样的工具在上传到服务器使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc016a-cf9e-4825-86fe-c11fac601857",
   "metadata": {},
   "source": [
    "&emsp;&emsp;pip 是 Python 的一个包管理器，用于安装和管理 Python 软件包。允许从 Python Package Index（PyPI）和其他索引中安装和管理第三方库和依赖。一般使用 pip 来安装、升级和删除 Python 软件包。除此之外，pip 自动处理 Python 软件包的依赖关系，确保所有必需的库都被安装。在Python环境中，尽管我们是使用conda来管理虚拟环境，但conda是兼容pip环境的，所以使用pip下载必要的包是完全可以的。\n",
    "\n",
    "&emsp;&emsp;我们建议在执行项目的依赖安装之前升级 pip 的版本，如果使用的是旧版本的 pip，可能无法安装一些最新的包，或者可能无法正确解析依赖关系。升级 pip 很简单，只需要运行命令如下命令：\n",
    "\n",
    "```bash\n",
    " pip install --upgrade pip \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cff2f-5e61-4f7c-b72c-7a72223c01e3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240926110524950.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702654cd-6a41-40be-8980-e8e3a4c2827f",
   "metadata": {},
   "source": [
    "- **Step 7. 使用pip安GLM4-9b-chat运行的项目依赖**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b4dc7-d745-48d4-8bb0-2d5eefec5d2e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一般项目中都会提供 `requirements.txt`这样一个文件，该文件包含了项目运行所必需的所有 Python 包及其精确版本号。使用这个文件，可以确保在不同环境中安装相同版本的依赖，从而避免了因版本不一致导致的问题。我们可以借助这个文件，使用pip一次性安装所有必需的依赖，而不必逐个手动安装，大大提高效率。命令如下：\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938efc6-720b-4e77-ac8e-3e454e2b0bc4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925111325501.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcf91a-4a63-469b-8433-c99f0cb3352a",
   "metadata": {},
   "source": [
    "- **Step 8. 从Model Scope下载GLM4-9b-chat模型权重**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e0a22-5a32-4ba2-be73-b9853107cb44",
   "metadata": {},
   "source": [
    "&emsp;&emsp;经过Step 5的操作过程，我们下载到的只是GLM4-9b-chat的一些运行文件和项目代码，并不包含GLM4-9b-chat这个模型。这里我们需要进入到 Model Scope 下载。Model Scope 是一个国内平台的资源丰富的模型库，开发者可以上传和共享他们训练好的机器学习模型。这些模型通常是经过大量数据训练的，并且很大，因此需要特殊的存储和托管服务。\n",
    "\n",
    "&emsp;&emsp;不同于GitHub，GitHub 仅仅是一个代码托管和版本控制平台，托管的是项目的源代码、文档和其他相关文件。同时对于托管文件的大小有限制，不适合存储大型文件，如训练好的机器学习模型。相反，Model Scope 专门为此类大型文件设计，提供了更适合大型模型的存储和传输解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd7050-905a-452f-b7e2-1305854f3f6f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Model Scope下载路径如下：\n",
    "https://www.modelscope.cn/models/ZhipuAI/glm-4-9b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62cd75-b4fd-4d5b-a363-9658891b16aa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Model Scope支持多种下载模式：SDK下载、Git下载、命令行下载（下载完整模型、下载单个文件 后直接加文件名即可）、手动下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba106b2-0ef8-4b39-ae6f-d33c4fc679f2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925160924132.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712c52a-1841-47b3-8c39-8eb616400bae",
   "metadata": {},
   "source": [
    "- **Step 9. 命令行方式下载**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f7fc7-b521-4556-b1d0-45432cf9f0d1",
   "metadata": {},
   "source": [
    "这是一种快捷的本机下载方式，首先通过命令`pip install modelscope`安装好达摩社区的安装包，然后通过以下指令可以下载模型的权重文件，这种方法无需翻墙，无需下载额外的插件。\n",
    "```\n",
    "modelscope download --model ZhipuAI/glm-4-9b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fff33-8ce0-470f-b051-7ca9ff374929",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924184742785.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475aeb70-d668-4ea8-8993-b6e6bcdc0202",
   "metadata": {},
   "source": [
    "这种命令可以支持下载单个的文件，通过以下指令可以单独下载某文件,用于补充或更新(这里用readme文件为例)：\n",
    "```\n",
    "modelscope download --model ZhipuAI/glm-4-9b README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f3199-e79b-43fe-b9cd-ff76b65582ea",
   "metadata": {},
   "source": [
    "下载完成后全部的权重文件的内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97293ed-b0f9-4e9f-a580-7a212824ef8b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925101231811.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2fe9d0-4698-4ca6-870c-19be19a49876",
   "metadata": {},
   "source": [
    "- **Step 10. 手动方式下载**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca4ec8-10cf-4c3d-a8ba-4572a81f8cda",
   "metadata": {},
   "source": [
    "如同在git hub上下载文件，在model scope中也可以实现手动下载，虽然相较于命令行下载的方法较繁琐，但是特殊情况下的有效安装方法。在模型下载页面可以通过手动点击文件对应的下载按钮即可实现下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e233f43-709c-47bc-ad7a-4d82a3117c81",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925161540765.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48868de9-220a-40c7-888f-b0fce122f17d",
   "metadata": {},
   "source": [
    "下载好的权重文件和命令行下载方式的一样，不过一般如果是在主机-服务器环境下使用这种方式下载的话需要xftp这样的传输工具在上传到服务器使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc1356-99b1-4698-bf48-6c4e4ffe68a7",
   "metadata": {},
   "source": [
    "- **Step 11. Git LFS方式下载**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892daf7-24fc-4d07-8152-7b49d3011229",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Git Large File Storage（Git LFS）是一种用于处理大文件的工具，在 Hugging Face 下载大模型时，通常需要安装 Git LFS，主要的原因是：Git 本身并不擅长处理大型文件，因为在 Git 中，每次我们提交一个文件，它的完整内容都会被保存在 Git 仓库的历史记录中。但对于非常大的文件，这种方式会导致仓库变得庞大而且低效。而 Git LFS， 就不会直接将它们的内容存储在仓库中。相反，它存储了一个轻量级的“指针”文件，它本身非常小，它包含了关于大型文件的信息（如其在服务器上的位置），但不包含文件的实际内容。当我们需要访问或下载这个大型文件时，Git LFS 会根据这个指针去下载真正的文件内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e422689-6428-4709-9ff7-7f53aae5f36a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;实际的大文件存储在一个单独的服务器上，而不是在 Git 仓库的历史记录中。所以如果不安装 Git LFS 而直接从 Hugging Face 或其他支持 LFS 的仓库下载大型文件，通常只会下载到一个包含指向实际文件的指针的小文件，而不是文件本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a99d0-dbae-4f2f-ad08-8adf6b3e00a8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所以，我们需要先安装git-lfs这个工具。命令如下：\n",
    "\n",
    "```bash\n",
    "sudo apt-get install git-lfs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90923a8-9e52-4906-8a81-525a6d94c6c7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925162818015.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716639c-13a7-4534-863e-835f692f7c23",
   "metadata": {},
   "source": [
    "&emsp;&emsp;安装完成后，需要初始化 Git LFS。这一步是必要的，因为它会设置一些必要的钩子。Git 钩子（hooks）是 Git 提供的一种强大的功能，允许在特定的重要动作（如提交、推送、合并等）发生时自动执行自定义脚本。这些钩子是在 Git 仓库的 `.git/hooks` 目录下的脚本，可以被配置为在特定的 Git 命令执行前后触发。钩子可以用于各种自动化任务，比如：\n",
    "\n",
    "1. **代码检查：** 在提交之前自动运行代码质量检查或测试，如果检查失败，可以阻止提交。\n",
    "2. **自动化消息：** 在提交或推送后发送通知或更新任务跟踪系统。\n",
    "3. **自动备份：** 在推送到远程仓库之前自动备份仓库。\n",
    "4. **代码风格格式化：** 自动格式化代码以符合团队的代码风格标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0aa9f8-f0eb-4ac0-a92a-bd962a8c9207",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而初始化git lfs，会设置一些在上传或下载大文件是必要的操作，如在提交之前检查是否有大文件被 Git 正常跟踪，而不是通过 Git LFS 跟踪，从而防止大文件意外地加入到 Git 仓库中。（pre-commit 钩子）或者在合并后，确保所有需要的 LFS 对象都被正确拉取（post-merge）等。初始化命令如下：\n",
    "\n",
    "```bash\n",
    "git lfs install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bd5ad-6f79-4853-aa86-dc33606771d7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925162901832.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc603ac-2304-4b9e-8dae-c20cc4ec842d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直接复制Model Scope上提供的命令，在终端运行，等待下载完成即可。\n",
    "```bash\n",
    "git clone https://www.modelscope.cn/ZhipuAI/glm-4-9b.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b591ec1-16c2-4b06-8258-9bf1a127e788",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在Hugging face上也可以实现这种方式的下载，其他步骤一致，只有最后一步略有改变，同样是在终端运行，等待下载完成即可。\n",
    "```bash\n",
    "git clone https://github.com/THUDM/GLM-4.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0a166-9cad-4fd1-9b55-fd108624b0df",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924165728527.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fee405-4fcb-4029-bf23-ad300e2ef956",
   "metadata": {},
   "source": [
    "下载好的文件内容和其他的方式所得无异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f095d-f20b-4ac2-904d-342d49bd1325",
   "metadata": {},
   "source": [
    "- **Step 12. 启动模型前，校验下载的文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba0e22-daff-4ca7-abcf-2db1c3a4587a",
   "metadata": {},
   "source": [
    "完成项目文件和权重文件的下载之后，在建立的GLM-4文件夹中会出现项目文件和权重文件，其中分别如下。\n",
    "权重文件："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201f2e8-0bdd-48d3-ab01-622abe01a5a6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925101231811.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212a7b6-1b8e-4693-a84e-dcda450babaf",
   "metadata": {},
   "source": [
    "项目文件的信息通过打开GLM-4-main可以查看，项目文件列表："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ee090-00cc-40c3-b72f-51408ce59b01",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925154501679.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39218b00-f543-40d4-b2bd-f8aa545f0568",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就已经把glm-4-9b模型部署运行前所需要的文件全部准备完毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947b571-b49c-465d-948a-00eb77b1e294",
   "metadata": {},
   "source": [
    "## 4. GLM-4-9b基本启动方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbdcab-61e8-4eab-8df9-a88199897732",
   "metadata": {},
   "source": [
    "&emsp;&emsp;GLM-4-9b模型提供了许多应用demo在项目文件中供开发者尝试使用。其中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6b68b-c230-4361-893f-dfbdf323eab0",
   "metadata": {},
   "source": [
    "base_demo中包含了使用transformers和vLLM后端的交互代码、OpenAI API后端交互代码、Batch推理代码。\n",
    "\n",
    "composite_demo中包含了GLM-4-9b完整功能展示代码，其中包括了All Tools功能、长文档解读、多模态能力。\n",
    "\n",
    "fintune_demo中包括了PEFT高效微调的代码以及SFT全量微调的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81a465-5e08-4121-81f6-21a40e88cb44",
   "metadata": {},
   "source": [
    "###  4.1 快速启动测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0680b-6d1b-4bb2-9cd0-535bf7545b16",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这种方式使得大模型回答一些预设好的问题，可以用来快速判断模型启动正常。对于这种启动方式，官方提供的脚本名称是：trans_batch_demo.py。通过vim 命令打开该文件先进行一些修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2c876-4492-4de9-832c-68a79207b877",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925113436431.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333b08c-a65c-4b95-9068-b3dae8c21386",
   "metadata": {},
   "source": [
    "通过`vim trans_batch_demo.py`方法打开文件，修改模型的调用路径为权重文件的绝对路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98cb49-ed54-45da-8bcf-11ed908cbedb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925114212893.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ba943-ff1f-4974-b78d-789f9683d9f2",
   "metadata": {},
   "source": [
    "在文件中可以看到预设好的与大模型对话的问题，其中包括了由不同role组成的Promt。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c09b74-9838-44b3-a0cf-5b3358382618",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925114101975.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b4722-60e1-4b7d-9f88-139f591d9a8e",
   "metadata": {},
   "source": [
    "通过`python trans_batch_demo.py`的方式启动文件，可以看到大模型一本正经的分析弱智吧经典问题以及对招呼的回应。（展示多轮对话能力）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254cde8-8e22-4efa-acf9-0dcdf3458a1b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925113436431.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6743b4-fb00-4ab1-b3bb-d5d4065cfab4",
   "metadata": {},
   "source": [
    "###  4.2 基于命令行的交互式对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5422f19-a08c-4e69-9b64-7a99c6ef15ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这种方式可以为非技术用户提供一个脱离代码环境的对话方式。对于这种启动方式，官方提供的脚本名称是：trans_cli_demo.py。首先我们通过vim 命令打开该文件先进行一些修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783619b9-f154-4e92-8f18-1e4def34956b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925164610668.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667d3d6-d4ab-411d-95ee-b86145e92114",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在启动前，我们仅需要进行一处简单的修改，因为我们已经把GLM-4-9b这个模型下载到了本地，所以需要修改一下模型的加载路径（原代码会下载权重模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bc1fb-8e8b-4eb1-ab1a-b78b8a84a034",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925115555680.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7426441-8c0f-41d0-8dea-84f5bdba6e4f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在文件下方可以修改大模型问答的长度、随机度，通过调节参数max_length、top_p、temperature可以进行输出的调节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d5ecc-71e9-429d-96c0-4145781c47a9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925115513595.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b914d45-63c0-4d74-b18a-a8950f62af92",
   "metadata": {},
   "source": [
    "&emsp;&emsp;修改完成后在命令行输入`python trans_cli_demo.py`即可启动，如果启动成功就会开启交互式对话，如果输入`ctrl + z` 可以退出该运行环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26814a36-fa27-4dc7-ac14-2f4ef0bcd3c8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925115904394.png\" width=150%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b169638-036b-42a2-b025-1d14c89c34b8",
   "metadata": {},
   "source": [
    "GLM-9b模型作为一个开源模型具有不错的CoT链式思考能力，针对数学问题也能做出不错的推理判断："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0c077-906e-4b08-9d20-07dc81e0b1d3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925120500403.png\" width=130%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb7750-ef80-4642-9e01-05f74b044ecf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240925121321038.png\" width=130%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eef331-e42c-49fa-8037-da8f150b9206",
   "metadata": {},
   "source": [
    "###  4.3 基于Gradio的前端对话启动方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ef86d-a605-4216-8dd9-45d8059697b6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这种方式可以提供一种可以在前端交互的大模型响应方式，官方提供的脚本名称是：trans_web_demo.py\n",
    "文件所在位置如下：。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ce423-f92e-4f3f-bef2-be279df1e916",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929105903454.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5836e-c723-4253-b458-f352de4c4237",
   "metadata": {},
   "source": [
    "如果你也是使用主机-服务器的方式在局域网内实现操作，需要先在服务器的命令行输入以下操作查看其在局域网中的地址：\n",
    "```\n",
    "ifconfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d0a8f-49b8-47f4-8d17-2acff0f5844e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927140346136.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f585464-07c0-4e2d-97ee-6f690adfaff9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过vim 命令打开trans_web_demo.py文件进行一些修改。在代码最底部找到文件输出地址信息，将代码修改成对应的服务器地址（如果是本地调用则0.0.0.0即可），并确认好端口不冲突。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d93356-fc8a-4b70-8cda-7a8b7b87b885",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927140504964.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298618ef-830d-4ba3-ba33-4f17194bf959",
   "metadata": {},
   "source": [
    "修改完成后通过命令 python trans_web_demo.py启动项目，在主机端打开浏览器输入服务器的地址和端口号实现web端的demo操作："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af9088-41bd-497b-8d66-6e7471374a13",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927140218190.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287b14c-d4bd-46e5-9042-71a747a065ee",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927140137108.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c3018-8a41-4b1b-9a76-053d9a3a47b4",
   "metadata": {},
   "source": [
    "启动成功后效果如下，在左下方输入好信息点击submit即可上传并实现对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5eb67-a6b5-43f5-90f9-fe686031d1d2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927133805583.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588146e-8782-478b-bc5d-a756bd40b4d3",
   "metadata": {},
   "source": [
    "对于这个前端界面还可以进行一些简单修改，在启动文件中可以找到以下的代码根据需求进行修改："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30ede2-f4d7-4de9-9777-ea1484822849",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929111930487.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9147970-0ad2-4538-88f7-caa331b91eb5",
   "metadata": {},
   "source": [
    "在trans_web_demo中可以添加这两行的代码实现模型的量化以减少推理时所需的显存消耗。这个参数指示加载模型时尽量减少 CPU 内存的使用。它通常通过分步加载模型参数或在需要时才加载部分数据来实现。这有助于在内存受限的环境中提高模型加载的效率。\n",
    "```\n",
    "quantization_config=BitsAndBytesConfig(load_in_4bit=True),            \r\n",
    "\r\n",
    "low_cpu_mem_usage=True,\n",
    "```\n",
    "\n",
    "量化后进行的模型推理大概只消耗9G的显存应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec48b2f-b996-46c1-8f4d-2dc0ec78812f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927171835824.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dec2f-42dc-449b-9e65-7586049f3d1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. 大模型开发工具Transformers库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b821a76-54d3-44aa-a676-6a2de783be0a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在上述运行的demo文件里导入的库往往都有这样两行代码：\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba26e82-500c-4cdc-a2ae-609516e2c0b5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里导入的Transformers，是 Hugging Face 提供的开源库，用于构建、训练、推理和部署大规模语言模型及其他类型的深度学习模型，特别是在自然语言处理（NLP）领域中。该库支持数百种预训练模型，并提供了一系列便捷的 API，极大简化了模型的使用和管理过程。可以允许用户下载和训练模型，最初是用于机器学习领域，而随着大模型的爆发，Hugging Face 也是迅速占领了大模型的市场，将功能扩展到包括多模态、计算机视觉和音频处理等其他用途的模型。\n",
    "\n",
    "> Github地址：https://github.com/huggingface/transformers\n",
    "\n",
    "> Hugging Face地址：https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86f910-a196-4398-89f4-beebc5c3b487",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311139436.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546c0ad-a6ff-4698-a90f-243ba74975e2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其价值在于：\n",
    "\n",
    "- 🤗 Transformers 提供了数以千计的预训练模型，支持 100 多种语言的文本分类、信息抽取、问答、摘要、翻译、文本生成。它的宗旨是让最先进的 NLP 技术人人易用。\n",
    "\n",
    "- 🤗 Transformers 提供了便于快速下载和使用的API，让你可以把预训练模型用在给定文本、在你的数据集上微调然后通过 model hub 与社区共享。同时，每个定义的 Python 模块均完全独立，方便修改和快速研究实验。\n",
    "\n",
    "- 🤗 Transformers 支持三个最热门的深度学习库： Jax, PyTorch 以及 TensorFlow — 并与之无缝整合。你可以直接使用一个框架训练你的模型然后用另一个加载和推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028a233-c813-4c86-b21b-5737468d3c7c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;简单理解，就是Transformers库，集成了大量的预训练模型，将其内部的调用方式已经做好了封装，所以才有我们看到的 模型加载、模型推理这种仅仅通过调用某个接口就可以实现。实际上调用大模型的推理是比较复杂的，只不过Transformers库在`暗处`给我们写好了而已，让我们仅仅关注如何使用，而不需要去了解复杂的原理和代码实现，从而降低模型的使用门槛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32382dd-ab09-409b-9e7e-c01db9134145",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Transformers集成了非常多的模型，在 Hugging Face：https://huggingface.co/docs/transformers/index 中可以找到详细的说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1c404-913a-4ab7-9691-2bca50e926db",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311156981.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a735704-4b03-4405-a280-165367e108c5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但是千万不要觉得使用大模型非常简单，简单的只是能够借助Transformers等库，来快速和大模型实现交互，而实际的生产，这种交互仅仅是最基础的向外提供的服务，理解大模型的推理底层过程还是非常重要的。这是由于所有的框架本质都是一个原理，所以当遇到一个新的框架应该如何去把我们使用的大模型与之做好集成，这是我们在以后的开发工作中，经常要做的事情，而且是必须要掌握的内容，这是进行上层应用开发的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce5fff-9338-45af-b34f-a8215e71a98a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来为大家介绍另一种开源框架vLLM，它具有更加优化的推理部署结构，以及如何将GLM4-9b-chat模型集成到这种推理加速及显存优化的开源框架中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57025d-33f8-4af0-ad48-185cd3356758",
   "metadata": {},
   "source": [
    "## 6. vLLM推理框架介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678c528-8a30-4df9-a0f6-5b325fd014ef",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先来看一下什么是vLLM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0621df7-3d01-444c-a233-992390ebea70",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM 是一个用于大型语言模型推理和服务的快速且易于使用的库，简单理解就是一个大模型的推理框架。这是一个用于快速 LLM 推理和服务的开源库。它提供的吞吐量比 HuggingFace Transformers 高 24 倍，而无需更改任何模型架构。vLLM在延迟方面也有显著优势。\n",
    "GLM4的官方集成了vLLM开源框架来实现部署和加速推理。\n",
    "\n",
    "&emsp;&emsp;vLLM论文地址：https://arxiv.org/pdf/2309.06180\n",
    "\n",
    "&emsp;&emsp;vLLM官方地址：https://docs.vllm.ai/en/latest/index.html\n",
    "\n",
    "&emsp;&emsp;vLLM支持模型列表\n",
    "https://docs.vllm.ai/en/latest/models/supported_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d2fb1-0b57-4830-a8df-24c90217ed52",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401292226013.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f3e8c-cf6b-4a9c-8ecb-3007afac5f12",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM有几个优化点值得我们去关注： 其一是*PagedAttention* 高效管理注意力键值内存，其二是vLLM支持多GPU和多节点推理，适用于大规模部署场景。其三，vLLM支持多种量化方案，如GPTQ、AWQ和FP8，优化了CUDA内核以提高推理效率。最后，它还可以为大语言模型构建API服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b57cf-e3a3-4ebd-a7f7-0e7ca1b6bed4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927190011014.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b92833-04e2-4ec1-85f5-a27e87ea6ff6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;与其他的推理框架相比可见vLLM在性能、功能和易用性方面都有显著优势，特别是在高吞吐量和低延迟的推理任务中表现突出。是一种非常易上手的推理框架，适合需要高性能和高可扩展性的应用场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b1018-6913-426b-b4eb-c3723e71f821",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927150608387.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326469f-5141-47d9-8ca3-555ba7152d6d",
   "metadata": {},
   "source": [
    "- 大模型生成原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e6dd5-18bc-4c68-b181-ba68def866e7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;大模型的语言模型是通过前面已生成的内容来预测下一个字的概率。举个例子，我们有一句话，从第一个字开始，一直往后预测每个字。公式可以理解为：第一个字的概率 P(x₁)，第二个字的概率 P(x₂) 是基于第一个字的，第三个字 P(x₃) 是基于前两个字，依此类推。\n",
    "\n",
    "&emsp;&emsp;在实际过程中，如果我们已经生成了一个句子 x₁、x₂...一直到 xₜ，接下来要预测下一个字 xₜ₊₁。模型会计算一些中间结果 K 和 V 向量，并把它们缓存起来。这样在预测下一个字时，不需要重新计算 x₁ 到 xₜ 的结果，只需要算 xₜ₊₁ 的部分。\n",
    "\n",
    "&emsp;&emsp;接着，利用这些缓存继续预测后续的字，直到生成完句子，或者遇到结束符号（比如 <eos>）。这就是我们所说的 KV Cache，它缓存了中间的 K 和 V 向量，帮助加速后续预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77080379-a15e-4f09-a309-4901f1a6cb5a",
   "metadata": {},
   "source": [
    "- 存在的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07429905-d621-481c-ad38-6bcaa3b4e2d8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在 LLM 推理过程中，每个请求都需要维护一个键值（KV）缓存，用来存储模型生成文本时的上下文信息。随着请求数量的增加，KV 缓存的大小会快速增长，消耗大量的 GPU 内存。\n",
    "\n",
    "&emsp;&emsp;在 LLM 推理时，通常为用户提供多种解码算法选择，比如贪婪解码、采样解码和束搜索（beam search）。这些解码算法对内存管理的复杂性有着不同的影响。比如，在并行采样中，多个输出可以共享相同的输入提示的 KV 缓存；而在束搜索中，不同的候选序列的 KV 缓存可以部分共享，这就要求内存管理系统能够动态调整内存的分配。\n",
    "\n",
    "&emsp;&emsp;LLM 服务的输入和输出长度是变化的，这就要求内存管理系统能够适应不同长度的提示。在解码过程中，随着请求的输出长度增长，所需的 KV 缓存内存也会增加，可能会耗尽可用的内存，从而影响新请求或现有任务的正常运行。kv缓存具有两个特性分别是大（在LLaMA-13B 中，单个序列最多占用 1.7GB。），以及动态性，它的大小取决于序列长度，而序列长度是高度可变且不可预测的。 因此，有效管理 KV 缓存是一项重大挑战。由于碎片和过度预留，现有系统浪费了 60% – 80% 的内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46d49e-a9c1-4700-b5d4-4af1cbd1acd7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcc71c-0f8a-4f18-a605-a37b76314e72",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在模型进行推理的时候，一般占用的GPU的内存分配情况如下，（以Nvidia A100显卡为例），其中：\n",
    "\n",
    "（1） Parameters 保留权重等参数，是静态的，这部分无法优化；\n",
    "\n",
    "（2） KV Cache 是 Transformer 的 attention 机制引入的中间缓存；\n",
    "\n",
    "（3） Others 是临时激活函数使用，占用比例较小，优化空间不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a7213-c079-486e-b6cf-3be9cac35539",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929130502935.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ef8b0-70ea-40d4-b077-194f272b65f9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这些数据里面最值得优化的就是KV Cache部分，同时KV Cache 是影响推理吞吐量的瓶颈，如果 KV Cache 管理不好，导致一次推理输出的数量太少，就会导致推理速度降低。Cache占用只占全部的30%，但是其中的60-80%都被浪费了；原因在lm推理时会按照可生成的最长序列长的分配显存，这会导致三种类型的浪费（相同的 token 在不同位置时可能会有不同的 KV 缓存）：\n",
    "\n",
    "（1）预留浪费（reserved）：为每个请求预留最大可能序列长度的内存，然而实际请求的长度可能远小于最大长度；\n",
    "\n",
    "（2）内部碎片（internal fragmentation）：内存分配的低效率还会导致内存碎片，进一步降低内存的可用性；\n",
    "\n",
    "（3）外部碎片（external fragmentation）：显存之间存在间隔碎片，不足以分配给下一个文本生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987d994-7643-4622-9483-23a191e3ef5d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929133725112.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e7666-9cb0-498b-bc64-ffe987dc1151",
   "metadata": {},
   "source": [
    "- PagedAttention原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da81b1-fcb6-488b-931a-744b6fd583a6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;针对这一问题进行优化，vLLM提出了新的注意力算法机制即PagedAttention，\n",
    "通过使用一种高效的内存管理系统——KV 缓存管理器（KV Cache Manager），通过分页技术对 KV 缓存进行管理，从而提升内存利用率，减少内存浪费，并支持更复杂的解码算法。这种方法允许在非连续的物理内存中存储连续的键和值，使内存管理更加灵活，能够更有效地应对 LLM 服务中的内存管理挑战。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736dfbc4-4452-419b-b3a5-7dea49da043b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240926144946206.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff084362-8369-480d-9a0e-974c6eb8039c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PagedAttention灵感来自操作系统中虚拟内存和分页的经典思想。与传统的注意力算法不同，PagedAttention 允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention 将每个序列的 KV 缓存划分为多个块，每块包含固定数量的token。这种方法允许在不连续的显存空间中存储连续的键和值，从而提高显存利用率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36245c13-ead8-4425-8b87-618208ab2b6d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927190040432.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2af4db-ca2d-42c1-89aa-fe7bc0a2ddeb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于这些块不需要在内存中是连续的，所以可以像在操作系统的虚拟内存中一样以更灵活的方式管理键和值：可以将块视为页面，将token视为字节，将序列视为进程。序列的连续逻辑块通过块表映射到非连续的物理块。物理区块在生成新token时按需分配。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50b697-6c59-4c9f-83e8-65e9175010ea",
   "metadata": {},
   "source": [
    "- KV Cache Manager "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e844556-1be4-4305-825f-b27d64d69d1c",
   "metadata": {},
   "source": [
    "下面是一个例子说明 KV Cache Manager 和 PagedAttention 的工作机制：KV Cache Manager 负责以分页的方式高效管理 KV Cache。它的作用类似操作系统中的虚拟内存的分页管理技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff7eef-195a-4c82-bc80-5f84fe298a6f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929141013812.png\" width=90%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413e1fc-015b-495e-a199-b178ad5ae9ce",
   "metadata": {},
   "source": [
    "\n",
    "（1）一个逻辑 KV 块最大可以存储 4 个 token。以输入 “Four score and seven years ago our” 为例，总共包含 7 个 token。因此，逻辑 KV 块 Block 0 填充前 4 个 token，对应于物理 KV 块的 Block 7；剩下的 3 个 token 填充逻辑 KV 块 Block 1，对应物理 KV 块的 Block 1，但未填满，因此对应的 Block Table 中的 #filled 写入 3。\n",
    "\n",
    "（2）当自回归生成下一个 token “fathers” 时，物理 KV 块的 Block 1 也同时被写入，同时对应的 Block Table 中的 #filled 从 3 更新为 4。\n",
    "\n",
    "（3）当自回归生成下一个 token “brought” 时，流程与之前同理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06833c9a-a4a0-47ac-8af6-a26fbb11198b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过这种方式可以实现物理块和逻辑块的映射，由此可以动态地增长 KV Cache 内存，而无需预先为其所有位置分配内存。并且通过页级内存共享，KV Cache Manager 支持在不同请求之间共享 KV Cache，进一步减少内存使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1936fa-5e48-4e40-becc-802a9aec4e6a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928165912727.png\" width=90%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2f3dd-873f-4ae4-b632-ae5cbdd801e9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;\n",
    "PagedAttention 具有另一个关键优势：高效的内存共享。例如，在并行采样中，从同一提示符生成多个输出序列。在这种情况下，可以在输出序列之间共享提示的计算和内存。在并行采样时，不同序列可以共享prompt的注意力计算和显存，通过逻辑块映射到同一物理块来实现内存共享。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5306a-8823-45e1-b0a9-2afd91a25ed5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;常见的解码场景--共享前缀：是指在多个用户请求中反复出现的文本序列，这些序列通常位于请求的开头部分。在大型语言模型的应用中，例如机器翻译或聊天机器人，用户经常提供包含指令和示例输入输出的详细描述。这些描述被连接到实际任务输入，以形成完整的请求提示（prompt）。由于这些共享前缀的重复性，模型可以有效利用已生成的上下文，从而提高推理效率，减少计算资源的浪费。这样的策略不仅加速了处理速度，还能显著提高用户请求的响应质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948660a9-cf4f-46db-8092-ab8c81b65b7f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM 系统通过以下方式优化共享前缀的处理：\n",
    "\n",
    "（1）预存储 KV 缓存：vLLM 可以预先存储共享前缀的 KV Cache，这样在处理包含该前缀的请求时，就无需重新计算这部分的 KV Cache，从而减少冗余计算。\n",
    "\n",
    "（2）映射逻辑块到物理块：当用户输入的提示包含共享前缀时，vLLM 可以将这些逻辑块映射到预先缓存的物理块。这样，只有用户特定任务的输入部分需要在模型中进行处理。\n",
    "\n",
    "（3）提高效率：通过这种方式，vLLM 能够更高效地利用内存和计算资源，因为共享前缀的计算只需进行一次，而不是在每个请求中重复进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ee60e-bbd5-42d3-a787-d6f5a61d98ac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM是一个通过 PagedAttention 实现高效内存管理的高吞吐量大语言模型（LLM）服务系统。其核心算法受操作系统启发，将虚拟内存和写时复制等成熟技术应用于高效管理 KV 缓存以及处理 LLM 服务中的各种解码算法。在官方论文实验表明，vLLM 在吞吐量方面比最先进的系统提高了 2-4 倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a5e29-8bb5-4d7c-a911-66ddbd51cadf",
   "metadata": {},
   "source": [
    "### 6.1vLLM安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d770595-47ec-40be-b61d-21fcf6b5af02",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202402011332935.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61461cc-b06a-4fdb-8e23-4485cee5499b",
   "metadata": {},
   "source": [
    "因为与HuggingFace的无缝集成，才导致这个工具非常易用。所以vLLM与其他的工具包使用方式一样，要使用vLLM框架集成GLM4模型，首要做的事情就是安装vLLM这个依赖包。这里有提供安装教程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3aa4f-6193-492e-af23-f6d9f74056e8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM框架对Python版本和CUDA版本的要求比较严格：\n",
    "\n",
    "- Python: 3.8 ~ 3.11\n",
    "- torch >= 2.0\n",
    "- cuda 11.8 or 12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd1044-0d10-41d2-9a0a-4f50a3c3c39f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM 对 torch 版本要求较高，且越高的版本对模型的支持更全，效果更好。如果不满足上述情况，会在安装过程中报错，导致无法安装，所以需要严格按照上述要求检查自己的环境。如确实无法满足，只能放弃使用vLLM框架推理加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16534ce8-9265-47a9-ba14-83cbd384d0ca",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先验证当前的开发环境，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1c88b-53ea-40eb-8389-ac2a13f2257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# 打印 Python 版本\n",
    "print(\"Python 版本:\", sys.version)\n",
    "\n",
    "# 打印 PyTorch 版本\n",
    "print(\"PyTorch 版本:\", torch.__version__)\n",
    "\n",
    "# 打印 CUDA 版本（如果 CUDA 可用）\n",
    "if torch.cuda.is_available():\n",
    "    cuda_version = torch.version.cuda\n",
    "    print(\"CUDA 版本:\", cuda_version)\n",
    "else:\n",
    "    print(\"CUDA 不可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ede4c7-500c-48ba-8f57-7219207c3675",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924175008707.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7dd32f-1ed8-4ad6-8c94-cf2fd09bc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf828ea7-0b4d-4725-9807-704a5691cb1f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924183219498.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06fec4-5082-4239-91e6-25167e630bad",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过`apt install nvidia-cuda-toolkit` 安装的是 Ubuntu 仓库中可用的 CUDA Toolkit 版本，这可能不是最新的，也可能不是特定需要的版本。主要用于本地 CUDA 开发（如果想直接编写 CUDA 程序或编译 CUDA 代码）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c604d2f-542a-4a78-a114-d5ac2dda9ecb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924180757523.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2746e9e-c3a3-4709-a0f9-8af239795932",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果想安装指定版本的CUDA-Toolkit，如何操作呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a115b-0650-4b27-8960-4d55f1f755f2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要进入NVIDIA官网：https://developer.nvidia.com/cuda-toolkit-archive  ，找到需要下载的Cuda版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd767b30-317d-477d-a9b2-76c0a7393040",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041412434.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52600b69-fb69-44bf-9e08-7b470122f8cb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据当前情况依次选择操作系统、版本等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ab1eb-2101-40e8-99c2-d21681112e8a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041412435.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e05db0-b418-4df6-a1a3-5e6f0f73d62e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后根据当前官方给出的代码，在终端执行即可安装。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bfa3e-9d91-4e41-bc61-780b0c95cee1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041412436.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0df7c1-abb2-4caa-b3f4-f6ef72a6ee03",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但其实，通常不需要预先手动安装 CUDA ，因为我们目前使用的 PyTorch 等框架在安装过程会处理这些依赖。当我们通过 Conda/pip等方式安装 PyTorch 时会指定的 CUDA 版本，该 CUDA 版本就会与当前的Pytorch版本相兼容，预编译并打包了与 CUDA 版本相对应的二进制文件和库。所以除非有特定的需求或要进行 CUDA 级别的开发，才可能需要手动安装 CUDA Toolkit。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3332781-6fb1-4fa8-a2f1-5aef6f58093b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM官方提供了安装方法，即：\n",
    "```bash\n",
    "# Install vLLM with CUDA 12.1.\n",
    "pip install vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defdcc38-7add-466a-8cee-a7c2b49749c6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924181336712.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19416472-513e-42ac-9ea2-ed72241e8a82",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过我们今天执行demo所需的vLLM版本要指定，执行以下代码安装（如果安装不合适的版本通过指令 pip uninstall vllm\r\n",
    "卸载\n",
    "）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7718bd-8275-4398-acd7-494abb0b0d10",
   "metadata": {},
   "source": [
    "```\n",
    "pip install vllm==0.4.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057bce89-c187-4daf-b74f-1083c371f26e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果在后续操作中出现如下报错，这大概率是由于vLLM和transformers版本不匹配导致tokenizer包无法识别。这时候就需要调整它们的版本实现兼容（一般的项目文件在requirements中会写好兼容的依赖）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51b3a4-4c8c-48fa-93f4-10880f428113",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240927181628463.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d4afa-e579-41fb-bf19-f4bea8717179",
   "metadata": {},
   "source": [
    "### 6.2 vLLM推理LLM实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acefbd-a89a-4b41-967e-120cae34c935",
   "metadata": {},
   "source": [
    "#### 6.2.1 快速启动Openai风格端口的方法 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20e574-d477-44ec-b4a5-20bf5e2b1fc1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在命令行中键入以下指令即可实现vLLM框架的Openai风格启动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86bb20-71d8-414d-b242-0fdf4b0e7dfc",
   "metadata": {},
   "source": [
    "python -m vllm.entrypoints.openai.api_server \\\r\n",
    "--model /home/01_llm/GLM-4 \\\r\n",
    "--served-model-name glm4-9b-chat \\\r\n",
    "--dtype auto \\\r\n",
    "--trust-remote-code \\\r\n",
    "--port 8000 \\\r\n",
    "--host 0.0.0.0 \\\r\n",
    "--gpu-memory-utilization 1 \\\r\n",
    "--max-model-len 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b13e9c-ecf1-470e-8b7d-136ecb00f446",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928104237915.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3010d-cc3e-46fb-bf85-fe9973470c37",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如显示以下信息说明推理启动成功，通过指令 nvidia-smi 可以查看到该进程的资源消耗情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ab3e4-b5e4-4158-b234-5ac6c86e1135",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928105411629.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a5b93-cfd5-4c29-ad14-acb467168e42",
   "metadata": {},
   "source": [
    "#### 6.2.2 vLLM启动命令行对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45843946-1b68-49ec-b70a-2190b9a24d57",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在basic demo文件夹中，可以找到以下文件vllm_cli_demo.py："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce14559-5fe7-4d00-a87b-847ef5ec6589",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929144929536.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3922935-bcdf-42b0-8d8e-96b00e32d794",
   "metadata": {},
   "source": [
    "&emsp;&emsp;依旧通过vim vllm_cli_demo.py方法编辑模式打开文件，修改模型的启动路径，并根据可运行的GPU数量将张量并行尺寸进行修改。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5d888e-e130-4eff-af48-875bc94870ac",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928102059812.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78130195-c743-4e3f-9db9-ea0523aa00fc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;修改并保存后退出，通过命令python vllm_cli_demo.py启动程序便可实现vLLM方法推理的命令行交互办法，可以在下图右侧看到vllm推理框架将计算资源均匀的分配给每个GPU。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33a0f535-cc53-4c27-b0f4-f3094f5eee01",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928154850770.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11713e5b-8087-4f8c-9553-68c53725e9f7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;也可以通过建立一个新的python文件来创建属于你自己的对话脚本，复制以下内容代码并进行必要的修改保存,在本例中命名为vllm_model.py。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32e1bc-06f0-4272-a4e9-31506af3d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 自动下载模型时，指定使用modelscope。不设置的话，会从 huggingface 下载\n",
    "# os.environ['VLLM_USE_MODELSCOPE']='True'\n",
    "\n",
    "def get_completion(prompts, model, tokenizer=None, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):\n",
    "    stop_token_ids = [151329, 151336, 151338]\n",
    "    # 创建采样参数。temperature 控制生成文本的多样性，top_p 控制核心采样的概率\n",
    "    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)\n",
    "    # 初始化 vLLM 推理引擎\n",
    "    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化 vLLM 推理引擎\n",
    "    model='/home/01_llm/GLM-4' # 指定模型路径\n",
    "    # model=\"THUDM/glm-4-9b-chat\" # 指定模型名称，自动下载模型\n",
    "    tokenizer = None\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) # 加载分词器后传入vLLM 模型，但不是必要的。\n",
    "\n",
    "    text = [\"给我介绍一下大型语言模型。\",\n",
    "           \"健康饮食推荐。\"]\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": \"你是一个有用的助手。\"},\n",
    "    #     {\"role\": \"user\", \"content\": prompt}\n",
    "    # ]\n",
    "    # 作为聊天模板的消息，不是必要的。\n",
    "    # text = tokenizer.apply_chat_template(\n",
    "    #     messages,\n",
    "    #     tokenize=False,\n",
    "    #     add_generation_prompt=True\n",
    "    # )\n",
    "\n",
    "    outputs = get_completion(text, model, tokenizer=tokenizer, max_tokens=512, temperature=1, top_p=1, max_model_len=2048)\n",
    "\n",
    "    # 输出是一个包含 prompt、生成文本和其他信息的 RequestOutput 对象列表。\n",
    "    # 打印输出。\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\") return outputs return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe6701-ac80-4d06-beaf-7dcfa9dba561",
   "metadata": {},
   "source": [
    "```python\n",
    "from vllm import LLM, SamplingParams\r\n",
    "from transformers import AutoTokenizer\r\n",
    "import os\r\n",
    "import json\r\n",
    "\r\n",
    "# 自动下载模型时，指定使用modelscope。不设置的话，会从 huggingface 下载\r\n",
    "# os.environ['VLLM_USE_MODELSCOPE']='True'\r\n",
    "\r\n",
    "def get_completion(prompts, model, tokenizer=None, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):\r\n",
    "    stop_token_ids = [151329, 151336, 151338]\r\n",
    "    # 创建采样参数。temperature 控制生成文本的多样性，top_p 控制核心采样的概率\r\n",
    "    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)\r\n",
    "    # 初始化 vLLM 推理引擎\r\n",
    "    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)\r\n",
    "    outputs = llm.generate(prompts, sampling_params)\r\n",
    "    return outputs\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    # 初始化 vLLM 推理引擎\r\n",
    "    model='/home/01_llm/GLM-4' # 指定模型路径\r\n",
    "    # model=\"THUDM/glm-4-9b-chat\" # 指定模型名称，自动下载模型\r\n",
    "    tokenizer = None\r\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) # 加载分词器后传入vLLM 模型，但不是必要的。\r\n",
    "\r\n",
    "    text = [\"给我介绍一下大型语言模型。\",\r\n",
    "           \"告诉我如何变强。\"]\r\n",
    "    # messages = [\r\n",
    "    #     {\"role\": \"system\", \"content\": \"你是一个有用的助手。\"},\r\n",
    "    #     {\"role\": \"user\", \"content\": prompt}\r\n",
    "    # ]\r\n",
    "    # 作为聊天模板的消息，不是必要的。\r\n",
    "    # text = tokenizer.apply_chat_template(\r\n",
    "    #     messages,\r\n",
    "    #     tokenize=False,\r\n",
    "    #     add_generation_prompt=True\r\n",
    "    # )\r\n",
    "\r\n",
    "    outputs = get_completion(text, model, tokenizer=tokenizer, max_tokens=512, temperature=1, top_p=1, max_model_len=2048)\r\n",
    "\r\n",
    "    # 输出是一个包含 prompt、生成文本和其他信息的 RequestOutput 对象列表。\r\n",
    "    # 打印输出。\r\n",
    "    for output in outputs:\r\n",
    "        prompt = output.prompt\r\n",
    "        generated_text = output.outputs[0].text\r\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\") return outputs return outputs\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d48f94-7139-4fb3-b3e2-f1318dccf69b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;编辑完成后用python的方法启动文件，经过推理后会返回基于vLLM框架下GLM4-9b-chat的返回结果。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064bffa2-0510-4d73-b19f-ee4e4f12cb04",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240929150934384.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff98518-d5ff-481b-84eb-7c1de95a4636",
   "metadata": {},
   "source": [
    "## 7.Windows环境中实现Linux操作系统办法：WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb16b7-f44c-4feb-840f-88f7cfd53a25",
   "metadata": {},
   "source": [
    "&emsp;&emsp;WSL（Windows Subsystem for Linux）是一个由微软开发的工具，它允许用户在 Windows 操作系统上运行 Linux 发行版。WSL 提供了一种与传统虚拟机不同的轻量级解决方案，使得用户能够无缝地在 Windows 和 Linux 之间切换，并在同一环境中运行 Linux 命令行工具和应用程序。\n",
    "\n",
    "&emsp;&emsp;WSL 的主要特点：\n",
    "- 无缝集成：用户可以在 Windows 和 Linux 之间轻松共享文件和目录，同时可以使用 Windows 的应用程序和命令行工具与 Linux 应用程序进行交互。\n",
    "- 多种发行版：WSL 支持多个 Linux 发行版，包括 Ubuntu、Debian、Kali Linux 等，用户可以根据需要选择合适的发行版进行安装。\n",
    "- 轻量级：WSL 不需要传统的虚拟机管理程序，节省了系统资源。WSL 直接在 Windows 上运行 Linux 内核，因此启动速度更快。\n",
    "- 开发友好：WSL 为开发者提供了一个熟悉的 Linux 环境，使得开发人员能够使用 Linux 工具链和包管理器，方便进行开发、测试和部署工作。\n",
    "- WSL 2：WSL 的第二个版本（WSL 2）引入了一个完整的 Linux 内核，提供更好的性能和兼容性，支持更多 Linux 应用程序和功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b5530-7b46-4c8a-91de-14adfeda6a21",
   "metadata": {},
   "source": [
    "*WSL*部署办法："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56833f54-4a55-4609-a4c7-60aeb44582d5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;WSL 2.0 相较于 WSL 1.0 具有完整的 Linux 内核、托管虚拟机以及全面的系统调用兼容性，因此我们选择安装使用 WSL 2.0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b260b3-3a7b-43dc-8c93-64b29af02fb6",
   "metadata": {},
   "source": [
    "首先通过win+R打开运行，然后输入winver检查windows版本。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8301ea02-ac82-4355-bcac-538bc78b8a95",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928113357014.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc9560c-337b-41b6-ae8e-59543a027259",
   "metadata": {},
   "source": [
    "安装WSL 2.0需要操作系统的版本使用 Windows 10 版本 19044+ 或 Windows 114，确认好可以关闭页面。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07dc7b32-3e71-45c3-a1a3-cc5d19020ae6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928113418295.png\" width=50%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71596e2c-9068-44fc-85e1-6dae19512ebb",
   "metadata": {},
   "source": [
    "接下啦以管理员的身份运行命令提示符：在 Windows 搜索框中输入“cmd”或“命令提示符”。右键点击“命令提示符”，选择“以管理员身份运行”。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "000aa7a7-b4c4-4a1a-ac5b-54c538debc76",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928113231006.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bda05-14e1-42ef-a3d9-4d864a7f40e8",
   "metadata": {},
   "source": [
    "在管理员命令提示符中，输入以下 DISM 命令\n",
    "```\n",
    "dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n",
    "```\n",
    "- 如果出现用户账户控制（UAC）提示，选择“是”以允许该操作。\r\n",
    "\r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "191dfd7e-b5a3-4623-8401-3e8ab0075e3b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928113027243.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a662b-a6d5-4b0e-b79a-6ba278185a44",
   "metadata": {},
   "source": [
    "启用虚拟化:以管理员打开powershell输入下列命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3631f-a738-4c68-9dac-4a45b986d340",
   "metadata": {},
   "source": [
    "```\n",
    "dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /noresta\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff83dc3e-4fc7-4f43-8260-9c67db20a56b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928173610584.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8de86-d51a-41ba-ab04-2b1a14b91322",
   "metadata": {},
   "source": [
    "接下来安装WSL，通过命令 wsl --install实现安装："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7af6dae0-d276-4990-96d3-4cf64f789688",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928174052400.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925bc4e-66ad-4246-b729-13638442147d",
   "metadata": {},
   "source": [
    "安装完毕wsl后可以通过以下命令展示可以下载的Linux系统的列表\n",
    "```\n",
    "wsl --list --online\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ca85b51-639b-4144-882c-0adae9eb172e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928180449373.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157a81c-4ca6-4a64-b55e-b4a89508ad09",
   "metadata": {},
   "source": [
    "通过命令 wsl -l -v 可以查看下载的WSL信息情况， 这里我们选择下载Ubuntu 22.04版本。\n",
    "```\n",
    "wsl --install -d Ubuntu-22.04\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e476101c-404e-419f-bd0b-2dec32b0592d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928180812664.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db582613-02f5-4557-ac89-da0a7a9a53e7",
   "metadata": {},
   "source": [
    "下载完成之后会创建新的账户，输入用户名和密码完成账号的创建（在输入密码的过程中不会显示字符，这不是bug）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "908d842c-4d35-453e-a9ae-3e30d119d315",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928181757762.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb11261-b89d-4805-a3a6-3f8ea5d3fd0a",
   "metadata": {},
   "source": [
    "在完成注册之后通过以下命令可以查询当前安装的Ubuntu版本信息：\n",
    "```\n",
    "lsb_release -a\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07c9f29c-6385-45b0-96ae-8c0cceae595b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928182355161.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a5413-651d-44ba-a806-f5236d94f6c4",
   "metadata": {},
   "source": [
    "WSLg（Windows Subsystem for Linux GUI）允许用户在 WSL（Windows Subsystem for Linux）中运行 Linux 图形用户界面（GUI）应用程序。它使得用户可以无缝地在 Windows 和 Linux 之间切换，直接在 Windows 桌面上使用 Linux GUI 应用,即使得开发人员和用户可以在 Windows 环境中运行 Linux 的图形软件。\r\n",
    "\r\n",
    "主要特点：\r\n",
    "\r\n",
    "1. **图形界面支持**：支持在 Windows 上运行 Linux 的 GUI 应用程序，用户可以像使用原生 Windows 应用一样操作。\r\n",
    "2. **无缝集成**：可以通过任务栏、开始菜单等方式访问 Linux 应用，增强了跨平台使用体验。\r\n",
    "3. **简化配置**：WSLg 免去了复杂的设置步骤，用户只需在 WSL 中安装所需的 GUI 应用即可提升了工作效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd80e60-2ecf-4e73-84df-53726ee9f99f",
   "metadata": {},
   "source": [
    "我们刚刚下载好的版本就是WSL 2，通过wsl -l -v可以确认"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75e001f9-1476-451a-9905-c3e3d170430e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928183718213.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf436e4-ccc3-4c1a-98c0-4a9301da56c2",
   "metadata": {},
   "source": [
    "如果输出是1，说明下载的是 WSL 1 版本，输入以下指令可以实现转换：\n",
    "```\n",
    "wsl --set-default-version 2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecf84d-2a81-4665-973b-84ff7676c103",
   "metadata": {},
   "source": [
    "- 完成程序下载后记得重启电脑实现激活。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008bfce3-a3ad-4bc4-90b4-a0159ade5380",
   "metadata": {},
   "source": [
    "接下来进行一些简单的测试，我们下载一个 Linux GUI 程序进行Linux图形文件的测试,输入以下指令下载：\n",
    "```\n",
    "sudo apt install gnome-text-editor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb4d5f-eefe-466b-887f-0a2075da9228",
   "metadata": {},
   "source": [
    "下载完成后在开始菜单也可以看到这些Linux GUI的应用程序的快捷方式了。\r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb60594e-7c4e-4ef4-b8db-048d9cf71e91",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928184323958.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf505460-344f-466c-9fd7-42fd3cbef83e",
   "metadata": {},
   "source": [
    "通过指令 sudo snap install gnome-text-editor 可以激活该程序，说明已经安装好了可以在windows环境下运行的Ubuntu 22.04系统，并且支持打开图形文件。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49837eae-2392-4262-a61f-6db5b386d306",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928184627116.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb319a-3cc5-49f1-b134-2941b9b8a452",
   "metadata": {},
   "source": [
    "**双环境文件互操作办法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d8bf8-cd4d-407b-9e5d-b2cb9edd2a7f",
   "metadata": {},
   "source": [
    "Linux 操作 Windows 文件：\n",
    "Windows 下的所有文件都被挂载在了 `/mnt` 下，在Windows 终端中的任意目录下输入 `wsl`，即可进入 Linux 对应的路径。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650734b-d546-44b1-818a-e6b730ca887a",
   "metadata": {},
   "source": [
    "Windows 上操作 Linux：\r\n",
    "\r\n",
    "在 Windows 文件资源管理器左侧可以找到 Linux 的标志，点击 Linux 就可以操作 Linux 文件。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48e4680a-ba13-465f-9c4a-0f07008e53c4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928184829517.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ab514-13cd-40d9-9ddc-eb599969842d",
   "metadata": {},
   "source": [
    "**使用不完全指南**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82025417-7bb8-41d4-b529-5005f6983881",
   "metadata": {},
   "source": [
    "如果 Windows 主系统中已经安装了 NVIDIA 驱动，Ubuntu 中不需要再额外安装 NVIDIA 驱动，也无需按照微软官方文档中的建议安装 nvidia-docker。可以直接在 Ubuntu 中输入 nvidia-smi，验证是否能够检测到 NVIDIA 驱动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07b3da-d3b9-4c66-9cf4-4aab177189ea",
   "metadata": {},
   "source": [
    "WSL2安装完以后，windows主系统中，资源管理器里能看到ubuntu中的文件夹，使用powershell命令行中也能直接访问，但是两个系统共享储存环境，只需要/mnt/文件 的方式即可访问，无需在两个环境下互相备份文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee33c6-188e-4128-968d-d5de829215ce",
   "metadata": {},
   "source": [
    "- 当然使用这种方法也会遇到奇奇怪怪的问题，建议大家遇到问题可以将报错先复制到浏览器中进行查询，一般都会有一个指导性的解释，也可以参考如下的经验汇总帖对问题进行排查：\n",
    "- https://blog.csdn.net/magefreehome/article/details/107885573"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
