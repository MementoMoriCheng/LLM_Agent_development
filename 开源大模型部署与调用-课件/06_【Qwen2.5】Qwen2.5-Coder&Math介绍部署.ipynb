{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a0e5d5-517b-4eb5-83a1-f9588f1f4433",
   "metadata": {},
   "source": [
    "# <center> Qwen2.5-Coder&Math介绍部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298903f-8bd2-4f4e-ae6b-38cc3c627b62",
   "metadata": {},
   "source": [
    ">前言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01e962-8e42-42f9-8b61-9f343c6d7a32",
   "metadata": {},
   "source": [
    "\n",
    "Qwen2.5-Coder 和 Qwen2.5-Math 是 Qwen2.5 系列中的两款专家型语言模型，它们在各自的领域表现出色，并提供了多种不同参数版本，供开发者根据需求进行灵活部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2f0e9-6031-4bd0-8b58-59d05ade63f2",
   "metadata": {},
   "source": [
    ">目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef96e0-d7e4-47bb-abc0-1a061c73aed0",
   "metadata": {},
   "source": [
    "一 Qwen2.5-Coder&Math 模型介绍<br>\n",
    "二 Qwen2.5-Coder&Math 技术文章分析<br>\n",
    "三 本地部署流程<br>\n",
    "四 线上部署流程<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72edcfed-2e71-4cd7-9eae-1a6b88984ac9",
   "metadata": {},
   "source": [
    "# 1.Qwen2.5-Coder&Math 模型介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0eebdc-57af-4692-9432-32406ded300a",
   "metadata": {},
   "source": [
    "以下信息源于Qwen官网：https://qwenlm.github.io/zh/blog/qwen2.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423800b-1d9b-4ea2-b5ef-2840755e43a3",
   "metadata": {},
   "source": [
    "通义千问（ Qwen ）是由阿里巴巴通义千问团队开发的大规模语言和多模态系列模型。通义千问可以执行自然语言理解、文本生成、视觉理解、音频理解、工具调用、角色扮演、智能体等多种任务。语言和多模态模型均在大规模、多语言、多模态数据上进行预训练，并在高质量语料上后训练以与人类偏好对齐。\n",
    "\n",
    "通义千问分为闭源和开源两大版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b5b9c-8d6c-45e4-b73b-2f61a2ec7e8c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108135615459.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468e8c8-1781-4c93-b5d2-be7b0b6a5694",
   "metadata": {},
   "source": [
    "最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，参数规模从0.5B到72B不等，包括："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ace40-20fb-41e9-9b4c-c389693512bd",
   "metadata": {},
   "source": [
    "- Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B;\n",
    "- Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B;\n",
    "- Qwen2.5-Math: 1.5B, 7B, 以及72B。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80710748-d198-4c52-9be9-d2743a0fefba",
   "metadata": {},
   "source": [
    "### Coder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733b3f8-784a-4a11-adc6-4ed00f7db614",
   "metadata": {},
   "source": [
    "专业领域的专家语言模型，即用于编程的 Qwen2.5-Coder 和用于数学的 Qwen2.5-Math，相比其前身 CodeQwen1.5 和 Qwen2-Math 有了实质性的改进。 具体来说，Qwen2.5-Coder 在包含 5.5 T tokens 编程相关数据上进行了训练，使即使较小的编程专用模型也能在编程评估基准测试中表现出媲美大型语言模型的竞争力。 同时，Qwen2.5-Math 支持 中文 和 英文，并整合了多种推理方法，包括CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca62f9-e7e6-4183-806a-ffb433d9f851",
   "metadata": {},
   "source": [
    ">Chain of Thought (CoT)：CoT 是一种按步骤思考的推理方法，模型通过一系列线性的推理步骤逐步接近问题的答案。在这个过程中，模型会生成中间推理步骤，让每一步的逻辑都透明可见。这种方式的优势是可以帮助模型在复杂任务中保持逻辑的一致性，使得推理过程更加清晰，从而提高回答的准确性。CoT 常用于解决数学题或需要严谨推理的语言理解任务。\n",
    "\n",
    ">Program of Thought (PoT)：PoT 是一种结合编程能力的推理方法。与 CoT 的逐步推理不同，PoT 利用代码和算法来直接生成问题的答案。模型通过生成代码（如 Python 程序）来模拟解题过程，这使得它特别适合处理涉及复杂数学运算或数据处理的问题。PoT 可以让模型利用编程语言的功能处理逻辑和计算，而不仅仅依赖模型内部的推理机制。\n",
    "\n",
    ">Tool-Integrated Reasoning (TIR)：TIR 是一种工具整合推理方式，模型在推理过程中可以调用外部工具（如计算器、数据库、API等）来辅助解题。TIR 的优势在于它能够扩展模型的能力，使其能够处理自身难以独立解决的任务。比如，对于复杂的数学运算，模型可以借助计算器；对于外部知识需求，可以通过查阅资料库。TIR 的整合使得模型推理更加灵活和准确，适合需要外部信息或复杂操作的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3fa82-442f-4143-b974-ff8f9f6022ad",
   "metadata": {},
   "source": [
    "自 CodeQwen1.5 推出以来，该模型成功吸引了众多用户，用于完成各种编程任务，如**调试、编程问题解答以及代码建议等**。最新迭代版本 Qwen2.5-Coder 专为编程应用设计。尽管体积较小，但在多种编程语言和任务中，其表现优于许多大型语言模型，充分展现了其卓越的编程能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b190c91-a8dc-4a25-952f-0897247527b1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108140646874.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4bdea-f2ec-4671-943b-d586f8e78e2c",
   "metadata": {},
   "source": [
    "Qwen2.5-Coder 支持最高 128K 的上下文长度，覆盖 92 种编程语言，在代码生成、多语言代码生成、代码补全和代码修复等多项代码相关任务的评估中表现出显著提升。值得一提的是，尽管开源版本的参数量仅为 7B，Qwen2.5-Coder 依然在性能上超越了诸如 DeepSeek-Coder-V2-Lite 和 CodeStral-22B 等更大规模的模型，成为目前最强大的基础代码模型之一。除了代码任务之外，Qwen2.5-Coder 在 GSM8K 和 Math 等数学评估中也展现了出色的数学能力。此外，对 MMLU 和 ARC 等数据集的测试结果显示，该模型在保留 Qwen2.5 强大编程能力的同时，也保持了卓越的通用任务表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383db1f-2156-4887-b677-fe778a76c2d0",
   "metadata": {},
   "source": [
    "\n",
    "Qwen2.5-Coder-Instruct 在多个关键领域展现出卓越性能：\n",
    "\n",
    "- **多编程语言能力**：在多语言评估方面，Qwen2.5-Coder-Instruct 使用 McEval 进行了扩展，涵盖了 40 多种编程语言。评估结果显示，模型在许多主流及小众编程语言中均表现出色，充分展现了其多语言支持能力。\n",
    "\n",
    "- **代码推理能力**：代码推理能力被认为与一般推理能力紧密相关。在使用 CRUXEval 进行基准测试后，结果表明 Qwen2.5-Coder-Instruct 在代码推理任务中表现优异。值得注意的是，随着代码推理能力的提升，模型在处理复杂指令时的表现也有所改善，这启发了进一步探索代码能力如何助力于通用推理能力的提升。\n",
    "\n",
    "- **数学推理能力**：数学与编程紧密相连，数学是代码的理论基础，而代码是数学应用的重要工具。在数学和代码相关任务中，Qwen2.5-Coder-Instruct 均表现出色，展现出其作为“理科专家”的优异能力。\n",
    "\n",
    "- **基础通用能力**：针对模型的通用能力进行了评估，结果表明 Qwen2.5-Coder-Instruct 继承并保持了 Qwen2.5 的基础能力优势，展示出在多领域任务中的出色适应性和综合表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4923539-6fb3-41ec-898c-6d9f0a5c2628",
   "metadata": {},
   "source": [
    "另外值得一提的是Qwen2.5-Coder的 Text2SQL 的性能也很不错。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99adead-9144-4827-8efb-abc9862f948c",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc500f-5b34-49e0-81ae-e7ddd878b2f5",
   "metadata": {},
   "source": [
    "在数学专用语言模型领域，9月推出了首批模型 Qwen2-Math。本次更新的 Qwen2.5-Math 相较于 Qwen2-Math，在更大规模的数学相关数据集上进行了预训练，其中包括由 Qwen2-Math 生成的合成数据。此外，新版本进一步增强了中文支持，并通过赋予模型 Chain of Thought（CoT）、Program of Thought（PoT）以及 Tool-Integrated Reasoning（TIR）等推理能力，大幅提升了推理性能。\n",
    "\n",
    "Qwen2.5-Math-72B-Instruct 的整体表现显著优于其前代模型 Qwen2-Math-72B-Instruct 以及 GPT4-o。在同类竞品的对比中，即使是体积较小的 Qwen2.5-Math-1.5B-Instruct 也能在与大型语言模型的竞争中展现出高度竞争力，充分证明了其在数学领域的出色表现和潜力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d26f35-a696-40d5-9ce9-7f47dc6bb0e2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/2024-08-qwen2.5-math-allsize.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadbc0d-b98f-41dd-95e3-5e9c1306e770",
   "metadata": {},
   "source": [
    "Qwen2.5-Math 主要专为解决中英文数学题而设计，支持通过 Chain of Thought（CoT）或 Tool-Integrated Reasoning（TIR）等推理方式进行解题。需要注意的是，该系列模型并不适合用于非数学领域的任务。\n",
    "\n",
    "**Qwen2.5-Math 系列包括以下几类模型：**\n",
    "\n",
    "- 基础模型：Qwen2.5-Math-1.5B、Qwen2.5-Math-7B 和 Qwen2.5-Math-72B，适合针对数学问题进行基础性推理和计算。\n",
    "- 指令微调模型：Qwen2.5-Math-1.5B-Instruct、Qwen2.5-Math-7B-Instruct 和 Qwen2.5-Math-72B-Instruct，这些模型经过指令微调，更加擅长处理数学题解和推理任务。\n",
    "- 数学奖励模型：Qwen2.5-Math-RM-72B，用于评估和优化数学解答的质量，以提高模型在数学任务上的表现。\n",
    " \n",
    "综上，Qwen2.5-Math 系列专注于数学领域的应用，不推荐在其他任务上使用，以确保模型性能的最佳发挥。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ded809-9d0c-4f03-a654-17138fae4a76",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108143411749.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b2e16-d146-49b3-a55e-671d28335566",
   "metadata": {},
   "source": [
    "实验结果显示，Qwen2.5-Math-72B-Instruct 在英文和中文基准上分别比上一代 Qwen2-Math-72B-Instruct 平均提升了 4.4 分和 6.1 分，成为当前最强的开源数学模型。\n",
    "\n",
    "旗舰模型 Qwen2.5-Math-72B-Instruct 的表现显著优于其他开源模型以及一些领先的闭源模型（如 GPT-4o 和 Gemini Math-Specialized 1.5 Pro）。在 RM@8 的 TIR 设置下，Qwen2.5-Math-72B-Instruct 在 MATH 基准上取得了 92.9 的高分。得益于 72B 模型的大规模合成预训练和监督微调数据，Qwen2.5-Math-7B-Instruct 的性能也超过了前代 Qwen2-Math-Instruct 72B。在 CoT 和 TIR 设置下，其 MATH 基准得分分别达到了 83.6 和 85.3。\n",
    "\n",
    "即使是最小的 Qwen2.5-Math-1.5B-Instruct 模型，在使用 Python 解释器辅助时，数学基准得分也能接近 80，表现优于目前大多数同类模型。这充分体现了 Qwen2.5-Math 系列在不同规模模型上的卓越性能和优越的数学推理能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f97c2-77d7-442b-bf13-8033b9f65eff",
   "metadata": {},
   "source": [
    "### 线上体验平台"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ddad4-568d-48a8-9e24-4c1ff1ecd045",
   "metadata": {},
   "source": [
    "在 Hugging Face和ModelScope平台上，用户可以方便地进行该模型的线上体验。这种在线测试不仅可以帮助用户直观地了解模型的性能，还能够根据具体任务需求进行评估。因此，建议大家在正式使用该模型之前，先通过这种方式进行测试，以确保模型的能力和特性符合自身的需求。在测试过程中，用户可以尝试不同类型的输入和任务，从而更好地评估模型在实际应用中的表现。这一过程有助于发现模型的优势与局限，使用户在后续的应用中做出更为明智的选择。通过线上体验，还可以及时获取最新的功能更新和使用技巧，从而优化工作流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a9c78-5f96-49d9-be84-8ec507daaeeb",
   "metadata": {},
   "source": [
    "Qwen2.5-Math\n",
    "\n",
    "https://www.modelscope.cn/studios/qwen/Qwen-Math-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0f37e-f141-40db-9828-ae94c57fc84d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241106183732469.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74d501-b05a-4c89-a392-817a9bea2a85",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/Qwen/Qwen2-Math-Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23fc66-dfe0-4924-a59e-1d327ea64636",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241106183812587.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3977f8-13a9-4b13-b494-0b855c848cff",
   "metadata": {},
   "source": [
    "# 2. Qwen2.5-Coder&Math 技术文章分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d538f-a595-412c-83d8-9e6cb257bd9d",
   "metadata": {},
   "source": [
    "Qwen2.5-Coder技术论文地址： https://arxiv.org/pdf/2409.12186"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff388ad-a230-4f75-a8f8-2ae1261dcabe",
   "metadata": {},
   "source": [
    "Qwen2.5-Math技术论文地址：https://arxiv.org/pdf/2409.12122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71465073-abe8-4482-b721-65cbab7886f4",
   "metadata": {},
   "source": [
    ">本节内容是基于Qwen2.5-Coder&Math 技术文章进行进一步的分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5220fd-a853-4bc1-95e6-0849e4cd9483",
   "metadata": {},
   "source": [
    "## Coder技术文章分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2a6fa-f486-4b50-9e93-7f0fbd1061fa",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105175525969.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30355673-788f-40b5-893e-a74bffeb49ba",
   "metadata": {},
   "source": [
    "### 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a712a9-5b7e-4bb8-8b21-652c84d270d4",
   "metadata": {},
   "source": [
    "\n",
    "从 Qwen 2 代开始，模型架构发生了一些关键变化，主要体现在加深小模型的深度、削减宽度，并且增大前馈网络（FFN）的升维维度。这一设计趋势反映了当前大语言模型（LLM）领域的一种主流认识，即：\n",
    "\n",
    "窄而深的模型优于宽而浅的模型：在相同参数量的情况下，加深模型深度（即增加层数）并缩小每一层的宽度（即降低隐藏层维度）通常能提升模型的推理和表达能力。更深的架构使得模型可以逐层处理和捕捉更高阶、更复杂的特征，这对处理长序列任务或需要复杂推理的任务尤为有利。\n",
    "\n",
    "增大 FFN 升维维度增强表征能力：通过增加前馈网络（FFN）中升维部分的维度（即 FFN 中的隐层维度，通常是 Transformer 隐层维度的 2-4 倍），可以显著提高模型的表征能力。这是因为更大的升维可以捕获更多的非线性特征，增强模型对输入数据的复杂模式识别能力，从而在各种任务上取得更好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce799873-215a-4ed4-87c8-8f3ada4543f4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108150319780.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e46132-80b0-4fdd-8e15-2672962c440b",
   "metadata": {},
   "source": [
    "本次开源的 Qwen2.5-Coder 系列涵盖了三个模型尺寸：1.5B、7B 和即将推出的 32B。然而，并没有提供 Qwen 系列中正统版本和 Math 专用模型的最大号 72B。实际上，对于代码专用模型来说，未来的演化趋势似乎正在分化：一方面，模型逐渐变小，以便于专有化部署和高频使用；另一方面，代码能力也逐渐被整合到通用大模型中，如 GPT、Claude 以及最新的 DeepSeek 等。因此，开发一个更大的专用代码模型的需求显得不再那么迫切。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f074d5-ecea-49fd-a39b-10a4fd0492d6",
   "metadata": {},
   "source": [
    "不过，值得注意的是，Qwen2.5-32B 和 Qwen1.5-32B 的架构存在一些微小差异。其中，最显著的变化是中间 FFN 升维部分的维度调整：从原先的 256 * 107 改为 256 * 108，即对齐到 1024。这一微调看似细微，但背后可能有几个潜在的考虑：\n",
    "\n",
    "1. 计算效率的提升：对齐到 1024 这一常见的计算基准，可以更好地利用现代硬件（尤其是 GPU 和 TPU）的计算优势，减少内存碎片，提高矩阵运算的效率。\n",
    "\n",
    "2. 模型的一致性和可扩展性：统一升维后的维度，简化了架构设计，使得不同尺寸的模型在设计上更加一致，有利于后续模型扩展或进行统一优化。\n",
    "\n",
    "3. 增强模型的表达能力：虽然 256 * 108 相比 256 * 107 提升幅度不大，但这种微小的调整在大规模训练中可能带来更好的收敛性，进一步增强模型的非线性表达能力，提升在特定任务中的表现。\n",
    "\n",
    "总体来看，这些细节调整体现了 Qwen 团队在优化模型架构时的精细化设计策略，旨在平衡模型性能、计算效率和硬件适配性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6895cb-437c-4e8d-a49a-21c0e4840997",
   "metadata": {},
   "source": [
    "从 Qwen 2 代开始，Qwen2.5-Coder 的 tokenizer 保持了一致的设计，但在最新版本中，特别针对 FIM（Fill-in-the-Middle）任务引入了一些特殊的 tokens。FIM 是一种常见的代码补全任务，其目标是根据给定的上下文，在代码中间进行补全。具体而言，FIM 的任务形式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2e1e5-63a1-4032-a4ba-d63f194c7220",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108150920810.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e06f40-960d-4d6e-9ae2-b6f2e47cc655",
   "metadata": {},
   "source": [
    "```\n",
    "任务描述：\n",
    "\n",
    "给定上下文代码，要求模型补全中间缺失的部分。\n",
    "示例格式如下：\n",
    "\n",
    "<< 上文代码 >>\n",
    "{{ 中间需要补全的部分 }}\n",
    "<< 下文代码 >>\n",
    "\n",
    "输入调整：\n",
    "\n",
    "由于生成模型只能看到前文（即单向注意力机制），直接使用上述格式会导致模型无法访问下文的信息。因此，为了实现 FIM 任务，输入会经过重新调整，具体方法是将下文放到前面，如下所示：\n",
    "\n",
    "<< 上文代码 >>\n",
    "<< 下文代码 >>\n",
    "{{ 告知模型，开始生成中间需要补全的部分 }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002896e7-25e7-4a94-8a75-e9fa249c4a79",
   "metadata": {},
   "source": [
    "这种输入格式的调整，使得模型在生成时能综合上下文的信息，更好地完成补全任务。\n",
    "特殊 Tokens 设计\n",
    "为了支持 FIM 任务，Qwen2.5-Coder 引入了一些特殊的 tokens，这些 tokens 的设计与常用的 FIM 标准基本保持一致，并没有特别自定义。这些特殊 tokens 包括但不限于：\n",
    "\n",
    "1. FIM 任务相关 Tokens：\n",
    "\n",
    "`<|fim_start|>`：标识 FIM 任务的开始，通常用于告知模型开始中间部分的补全。\n",
    "\n",
    "`<|fim_end|>`：标识 FIM 任务的结束，帮助模型识别补全任务的边界。\n",
    "\n",
    "2. Repo 级别的补全 Tokens：\n",
    "\n",
    "`<|repo_name|>`：用于 repo 级别的代码补全任务，帮助模型识别具体的代码仓库名称。\n",
    "\n",
    "`<|file_sep|>`：用于区分不同文件内容，方便模型在处理跨文件代码补全任务时，识别文件边界。\n",
    "这些特殊 tokens 的引入，增强了模型在执行 FIM 任务时的表达能力，使其能够更好地处理上下文，并针对不同的任务场景（如 repo 级别的补全任务）提供更加精确的补全结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda763e-0fca-4a47-8446-c9f3971db922",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108151013304.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49fb8b-52bc-4194-a56b-8b7e3ad28062",
   "metadata": {},
   "source": [
    "### 数据组成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bfc46-9ea4-4a1e-bd2d-b229432314de",
   "metadata": {},
   "source": [
    "在当前大模型（LLM）的发展阶段，有两个主要参数对其性能产生了关键影响：Scaling Law（扩展法则）和 训练数据的质量。这两个部分相辅相成，是推动大语言模型性能提升的两个核心因素。在实际开发中，模型参数量的增加可以带来泛化能力的提升，而高质量的训练数据则能让模型在处理特定任务时表现更加精准。未来的发展方向可能会是寻找两者之间的最佳平衡点，即在合理的参数规模下，利用高质量的训练数据，实现性能的最优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37d62d-04a0-4dab-a641-0c374a6a58e7",
   "metadata": {},
   "source": [
    "在 Qwen2.5-Coder 的开发过程中，训练数据的选择与过滤策略在模型性能提升中扮演了至关重要的角色。以下是几种主要的数据类型及其处理策略："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9ceb1-3ea4-4d70-8ba5-fde5c9f59c4c",
   "metadata": {},
   "source": [
    "1. 代码-文本关联数据（Text-Code Grounding Data）\n",
    "\n",
    "Qwen2.5-Coder 使用了大量的代码-文本关联数据，并且进行了严格的筛选和过滤。具体策略包括：\n",
    "\n",
    "数据质量过滤：虽然具体的过滤细节并未在报告中详述，但明确指出，高质量的小规模数据集要优于数量多但质量参差不齐的数据集。这表明，在模型训练中，数据的质量比数量更为重要，尤其是在代码生成和补全等高精度任务中。\n",
    "\n",
    "2. 合成数据（Synthetic Data）\n",
    "\n",
    "合成数据在此次训练中占据了重要地位。具体策略包括：\n",
    "\n",
    "合成数据来源：合成数据由 Code-Qwen1.5 生成。报告中未透露更多细节，但这一点表明，随着自然语料逐渐消耗殆尽，使用合成数据进行训练的趋势越来越明显。\n",
    "\n",
    "合成数据的训练阶段：合成数据的使用逐步从传统的后训练阶段（Post-training）转向预训练阶段（Pre-training）。这是因为随着模型生成能力的提升，合成数据的质量和多样性能够显著改善模型的泛化性能和实际任务表现。\n",
    "\n",
    "3. 数学数据（Mathematical Data）\n",
    "\n",
    "数学数据与代码数据在训练中相辅相成，且有显著的互相促进效果：\n",
    "\n",
    "数学数据来源：数学部分的数据直接引用了 Qwen2.5-Math 使用的数据集。由于数学任务往往涉及复杂的逻辑推理和计算步骤，与代码任务有类似的结构性需求，因此这两类数据在联合训练时能够有效提升模型的推理能力。值得注意的是，这种数学数据的加入并没有对模型在代码任务上的表现产生负面影响。\n",
    "\n",
    "相互促进效果：在模型开发的共识中，数学和代码数据相互补充，能显著提升模型的逻辑推理能力。数学数据的引入帮助模型处理更为复杂的计算和逻辑任务，而代码数据则提高了模型对语法和结构化语言的理解。\n",
    "\n",
    "4. 文本数据（Textual Data）\n",
    "\n",
    "Qwen2.5-Coder 还使用了大量文本数据，但对其进行了特定的过滤，以确保数据的针对性和质量：\n",
    "\n",
    "文本数据来源：文本数据部分来源于 Qwen2.5 的数据集。但在使用过程中，去除了所有包含代码的文本片段，以减少噪声和避免重复学习。这种策略旨在增强模型对纯文本和自然语言的理解能力，同时避免与代码数据的重叠影响模型的专注性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd4efa-2c49-43c2-b649-e8a81fc436e6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108154122977.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2baae0-f401-430f-86c5-aa66ba167e4a",
   "metadata": {},
   "source": [
    "\n",
    "另外在预训练Qwen2.5-Coder选用一定比例的数据混合，在最后的模型中选择的数据集的组成采用了 7:2:1 的比例，即 代码数据、文本数据 和 数学数据 分别占比 70%、20% 和 10%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc3f52-9b9f-47fc-a103-c52c545121da",
   "metadata": {},
   "source": [
    "选择 7:2:1 这一数据比例，主要基于以下几点考量：\n",
    "\n",
    "保持通用能力：尽管 Qwen2.5-Coder 主要面向代码任务，但文本数据的加入对于模型的自然语言理解和通用任务能力至关重要。如果文本数据比例过低，模型的通用能力和对编程场景中的自然语言指令理解能力将受到影响，进而影响到实际应用中的代码生成和补全效果。\n",
    "\n",
    "优化代码能力：代码数据的占比为70%，确保了模型在编程任务上具备足够的训练样本，从而在代码生成、调试和优化任务上表现出色。\n",
    "\n",
    "提升推理能力：数学数据与代码数据在逻辑结构和推理能力上有较高的相似性。通过引入10%的数学数据，模型能够在逻辑推理和复杂计算任务上获得提升，有效支持代码补全、错误排查等需要精确推理的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a507f2-3a36-44c2-a52c-9272e27c6e7c",
   "metadata": {},
   "source": [
    "## Math技术文章分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ac5bc-ce09-4a17-84d4-74cd233527a5",
   "metadata": {},
   "source": [
    "Qwen2.5-Math有以下几个关键的技术亮点：\n",
    "```\n",
    "（1）在预训练阶段大量使用来自Qwen2-Math的合成数学数据。\n",
    "（2）在后训练阶段迭代生成微调数据并在奖励模型的指导下进行强化训练。\n",
    "（3）支持双语（英语和中文）解题，以及思路链和工具集成推理能力。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c6ded-7d8f-4b21-9d6b-d98de607790b",
   "metadata": {},
   "source": [
    "在Qwen2.5-Math的训练流程中使用了一个重要的方法——自我改进（Self-Improvement）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeabca8-9686-4776-bea7-7437e37b62d6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108160657184.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccf0eb-8d52-4623-829c-2c0defbb9fe2",
   "metadata": {},
   "source": [
    "**自我改进的整体思路**\n",
    "Qwen2.5 系列模型在研发过程中引入了**自我改进（Self-Improvement）**的策略，即利用现有模型生成新的高质量训练数据，再用于下一轮训练，以不断提升模型的表现。这个过程可以被看作是通过多次迭代来优化模型的能力，使得模型在训练过程中具备了“自我增强”的特性。具体来说，主要体现在以下三个方面："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45dffc-fe8f-4e2a-bb0a-e4c45ebe2478",
   "metadata": {},
   "source": [
    "**自我改进的三个阶段**\n",
    "\n",
    "第一阶段：后训练（Post-training）中的自我改进\n",
    "\n",
    "- SFT 和 RM 模型的交互迭代\n",
    "  \n",
    "这一阶段主要包括**SFT（Supervised Fine-Tuning）模型与RM（Reward Model）**之间的交互迭代过程。通过反复地训练奖励模型（Train RM）和优化 SFT 模型（Evolve SFT），实现模型能力的逐步提升。\n",
    "- SFT 模型负责通过监督学习进行微调，以获得符合人类意图的回答。\n",
    "- **奖励模型（RM）**则用于对 SFT 模型生成的输出进行评分和评价，以指导其生成更加符合期望的回答。\n",
    "这个循环过程可以视为模型的第一次自我改进，也是在后训练阶段的一个核心环节。通过多个循环，SFT 模型逐渐学习到了高分样本的特征，性能得到进一步优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec427c9a-dab3-40ec-ae46-fa4c7113461e",
   "metadata": {},
   "source": [
    "第二阶段：RLHF 强化学习（Reinforcement Learning with Human Feedback）\n",
    "- 使用奖励模型进行 RLHF（GRPO 方法）\n",
    "\n",
    "在完成了 SFT 和 RM 之间的交互训练后，接下来会使用强化学习（RL）的方式对模型进行进一步优化，具体使用的算法是GRPO（Generalized Policy Optimization）。通过 RLHF 的训练，使得模型不仅依赖于固定数据样本，还能够根据用户反馈动态调整生成的答案，从而提升模型的实用性和人类偏好一致性。\n",
    "\n",
    "RLHF 阶段的改进是模型的第二次自我改进。在这个阶段，模型经过了三阶段标准流程的强化：预训练（Pre-training）、监督微调（SFT）、强化学习（RLHF）。这一阶段生成的模型称为Qwen2.5-Math-Instruct，类似于市场上的其他 RLHF 模型，如 DeepSeek-Math-RL。\n",
    "\n",
    "值得一提的是，与 Qwen2.5-Coder-Instruct 不同，Qwen2.5-Math-Instruct 是经过完整 RLHF 流程优化后的模型，而非单纯的 SFT 微调模型。这种设计强调了 Qwen2.5-Math 在数学推理任务上的进一步优化和强化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea85f46-4d43-4840-be51-0e58dbce9290",
   "metadata": {},
   "source": [
    "第三阶段：预训练（Pre-training）中的自我改进\n",
    "- 合成数据增强\n",
    "\n",
    "在 Qwen2.5-Math 的预训练阶段，使用Synthetic Math Pre-training Data（合成数学预训练数据）作为额外的数据源，结合 Qwen2-Math-Instruct 模型来扩充和增强训练数据。这一阶段可以视为第三次自我改进，模型通过生成大量高质量的合成数据，反过来提升自身在数学任务上的表现。\n",
    "\n",
    "合成数据的使用代表了一个明显的趋势，即从传统的后期微调（Post-training）逐渐向早期预训练（Pre-training）扩展。随着现有自然数据的质量和数量逐渐达到瓶颈，合成数据成为提升模型性能的新途径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619687f-4567-4f15-bf65-29323166ba64",
   "metadata": {},
   "source": [
    "自我改进策略的优势和挑战\n",
    "\n",
    "这种自我改进的策略类似于“左脚踩右脚上天”，即模型通过生成自身的数据，不断循环迭代以提升性能。虽然从理论上看，这是一种自然且直观的方式，但实际操作中存在多项挑战：\n",
    "\n",
    "- 质量控制\n",
    "如何确保每一轮生成的数据质量较高是自我改进中的一个关键问题。为了防止低质量数据导致模型性能退化，Qwen2.5 系列在数据筛选和过滤上投入了大量精力。例如，使用 LLM 进行评分和过滤，并结合人工规则进行进一步筛选。\n",
    "\n",
    "- 算力需求\n",
    "自我改进需要多轮的训练迭代，每轮都需要生成大量合成数据，这对计算资源提出了极高要求。尤其在模型规模不断扩大的情况下，训练时间和算力需求也显著增加。\n",
    "\n",
    "- 数据清洗与去重\n",
    "数据质量控制还涉及到去重、清洗和过滤，以去除重复、噪声或低质量的数据。虽然这类方法已经成为行业共识，但具体的实现通常涉及复杂的工程技术，而报告中并未提供完整的代码或细节流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3338f-f8e3-4f36-9a95-a4aa08203e59",
   "metadata": {},
   "source": [
    "**Qwen2-Math 数据集与预处理**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69c7ea-dce3-4929-ad64-77e2dc08d3a7",
   "metadata": {},
   "source": [
    "1. 数据集来源与规模\n",
    "\n",
    "Qwen2 在训练 Qwen2-Math 时，使用了总计 700B tokens 的数据集，数据来源于 web 数据，主要来自github上的开源资源。这是当前大规模预训练模型常见的做法，因其能够涵盖广泛的知识领域。\n",
    "\n",
    "2. 快速分类方法：FastText\n",
    "\n",
    "FastText 被用来从大量的通用数据中筛选出与数学相关的语料。尽管 FastText 的性能可能不及像 BERT 这样的深度模型，但其速度更快，能在处理大规模数据时提供更高的效率。\n",
    "在报告中提到，FastText 不仅高效，而且每个训练周期都会逐步迭代，进一步增强模型的分类能力。\n",
    "\n",
    "3. 元数据与数据增强\n",
    "\n",
    "为了扩充数学相关的数据池，Qwen2 还利用了元数据（例如 URL）进行增强。这种方法的核心思想是通过分析网页的结构信息（如链接和网页内容）来判断其与数学内容的关联性。\n",
    "\n",
    "4. 数据去重：MinHash与LSH\n",
    "\n",
    "使用了MinHash 去重技术，这是大规模数据处理中常用的工具。它与 LSH（局部敏感哈希） 结合使用，能够有效地去除重复或高度相似的内容。\n",
    "MinHash：生成文档的“摘要”，用于快速比较文档之间的相似度。\n",
    "LSH：利用这些摘要进行快速近似匹配，从而筛选出重复或冗余的内容。\n",
    "\n",
    "5. 数据打分与质量控制\n",
    "\n",
    "对所有语料进行了打分，采用了 Qwen2-0.5B-Instruct 小模型来进行打分。这是一个重要的质量控制环节，高分的语料将被优先用于训练，低质量的语料则会被过滤掉。\n",
    "使用较小的模型（如 0.5B）打分可以加速评分过程，并且对训练数据中的噪声不敏感，因为这些噪声对预训练的影响相对较小。\n",
    "\n",
    "6. 合成数据的生成\n",
    "\n",
    "报告提到，合成数据在训练中也起到了重要作用，特别是在生成 QA 对 时。Qwen2-72B-Instruct 被用来生成并优化预训练数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9e95a-9dd7-49d5-bf3b-a7737365204b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108161814869.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b5b64-a69c-4c11-aa35-c7bc909ccd75",
   "metadata": {},
   "source": [
    "# Coder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef56817-e272-455c-ba56-33c398b590bb",
   "metadata": {},
   "source": [
    "# 3. 本地部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39792a-057d-4531-becf-aa7728f47dea",
   "metadata": {},
   "source": [
    ">以下操作都是在命令行实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ca1d6-e01e-404a-bb15-ad1f46e2a0d9",
   "metadata": {},
   "source": [
    "- **Step 1. 创建conda虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78362b4-33ad-4711-9559-16e8de62429a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Conda创建虚拟环境的意义在于提供了一个隔离的、独立的环境，用于Python项目和其依赖包的管理。每个虚拟环境都有自己的Python运行时和一组库。这意味着我们可以在不同的环境中安装不同版本的库而互不影响。官方给出的Python版本要求是大于`3.9`。创建虚拟环境的办法可以通过使用以下命令创建：\n",
    "\n",
    "```bash\n",
    "# qwen-coder是你想要给环境的名称，python=3.11 指定了要安装的Python版本。你可以根据需要选择不同的名称和/或Python版本。\n",
    "\n",
    "conda create -n qwen-coder python=3.11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e7532-1491-4383-bc23-b0d33a021a44",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105103532713.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae6448-6a00-4f34-a9e0-1b9a26f1ef64",
   "metadata": {},
   "source": [
    "&emsp;&emsp;创建虚拟环境后，需要激活它。使用以下命令来激活刚刚创建的环境。如果成功激活，可以看到在命令行的最前方的括号中，就标识了当前的虚拟环境。虚拟环境创建完成后接下来安装torch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90878895-1a21-4896-bded-ed067105251e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105103523756.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a4e06-db7c-4b74-bdeb-55a309285e8a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果忘记或者想要管理自己建立的虚拟环境，可以通过conda env list命令来查看所有已创建的环境名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6b19c-f6ac-497f-a577-b5f09a55aa8d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143512354.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b320f-d2ea-4a8e-b486-f5fb52427a04",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果需要卸载指定的虚拟环境则通过以下指令实现：\n",
    "```\n",
    "conda env remove --name envname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc478894-edba-445c-84b5-d92b78eff2ed",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143643575.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5921b5-de9d-44f3-8db7-bde16e7094fa",
   "metadata": {},
   "source": [
    "- 需要注意的是无法卸载当前激活的环境，建议卸载时先切换到base环境中再执行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a94ff4-e283-4f47-be01-79e8833d6c81",
   "metadata": {},
   "source": [
    "- **Step 2. 查看当前驱动最高支持的CUDA版本**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702fa02-5e90-4ad1-b3ae-0214994984ac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们需要根据CUDA版本选择Pytorch框架，先看下当前的CUDA版本：\n",
    "```\n",
    "nvidia -smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade3eca-4459-4c9a-8d1e-3769040eb925",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924161818464.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8acc0-5a46-4f7e-b8f1-20355cc865a3",
   "metadata": {},
   "source": [
    "- **Step 3. 在虚拟环境中安装Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd3ee0-a765-4f65-8e78-8951dc80ec77",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进入Pytorch官网：https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3926d5-c43c-46aa-93a6-50420dfa50a8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240103163206436.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9a404-5969-4fbf-bbd4-69b02255e1a3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当前的电脑CUDA的最高版本要求是12.2，所以需要找到不大于12.2版本的Pytorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5daa9-01e8-41e3-993b-1fc637475ae6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041455184.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ff3f5-5072-44de-8f32-344a87d00ca6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直接复制对应的命令，进入终端执行即可。这实际上安装的是为 CUDA 12.1 优化的 PyTorch 版本。这个 PyTorch 版本预编译并打包了与 CUDA 12.1 版本相对应的二进制文件和库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa304901-b51d-4e5a-a2bd-9b838f3177d6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105110047278.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9e7c0-17fc-4960-bf07-017428dbc67e",
   "metadata": {},
   "source": [
    "- **Step 4. 安装Pytorch验证**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cfca4-51ef-4868-a24c-9c1f06448799",
   "metadata": {},
   "source": [
    "&emsp;&emsp;待安装完成后，如果想要检查是否成功安装了GPU版本的PyTorch，可以通过几个简单的步骤在Python环境中进行验证：\n",
    "```bash\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9436f-753a-4fad-9927-43cc939e5657",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154248388.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76867500-dc76-4a9c-ae60-0d04c61bf0cd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果输出是版本号数字，则表示GPU版本的PyTorch已经安装成功并且可以使用CUDA，如果显示ModelNotFoundError，则表明没有安装GPU版本的PyTorch，或者CUDA环境没有正确配置，此时根据教程，重新检查自己的执行过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24eaa7-3b62-4723-bd09-7b145d06a171",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154338394.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dd7d9-421e-4627-989a-10797f3226c6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然通过pip show的方式可以很简洁的查看已安装包的详细信息。pip show <package_name> 可以展示出包的版本、依赖关系（展示一个包依赖哪些其他包）、定位包安装位置、验证安装确实包是否正确安装及详情。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33cb64-d112-4412-adf0-086b06246fb8",
   "metadata": {},
   "source": [
    "- **Step 5. 安装必要的依赖包**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0814a-4e6d-4c41-a9c0-e9308d0a40d3",
   "metadata": {},
   "source": [
    "Transfomers是大模型推理时所需要使用的框架，建议使用的版本`Transfomers>=4.37.0`，通过以下指令可以下载最新版本的Transfomers："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6caf9ea-edc3-42a6-8df1-4b4ce193597c",
   "metadata": {},
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b879e-d932-460e-abc4-ee9f87b71437",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105105546261.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539c25d-ec6d-4cec-960f-3e6676d622f2",
   "metadata": {},
   "source": [
    "安装完成后可以通过以下命令检查是否安装：\n",
    "```\n",
    "pip show transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26285bf6-5b11-4b65-b1fb-f2fb8a295063",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107183539421.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929537f0-ab81-4557-a62e-a391027f2409",
   "metadata": {},
   "source": [
    "接下来需要安装下载工具modelscope以及接下来用来加速模型的训练和部署的库accelerate，通过以下代码进行对应工具的部署：\n",
    "```\n",
    "pip install modelscope\n",
    "pip install accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f91f5-9170-4467-8dfe-534dbfb0472e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105104019415.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5e25a-c530-42a7-879b-d9035ba46134",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105110657041.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23614da7-aad1-4c45-8fd6-fdcdb24c6903",
   "metadata": {},
   "source": [
    "- **Step 6. 下载模型文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcad530-cb63-4f91-8df1-391f3ca7172b",
   "metadata": {},
   "source": [
    "首先我们要创建一个新的文件用于储存下载的代码信息，通过指令`mkdir +name`可以创建一个新的文件夹，创建完毕后用命令`cd +name`可以进入到该文件夹以便于稍后的操作。通过指令`pwd`可以查看当前所在位置的路径信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a9cd8-0366-4eca-b09e-03e47c57c947",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105103632755.png\" width=50%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a8079-fca2-4286-b4b2-5849854617e3",
   "metadata": {},
   "source": [
    "- **Step 7. 在modelscope源下载模型文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874f633-ecf2-44d9-a908-d62f426dea55",
   "metadata": {},
   "source": [
    "在确认进入所在工作文件夹后首先进行模型的项目文件的下载："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad284a-7c11-4928-afdc-d71447610161",
   "metadata": {},
   "source": [
    "这种办法是一种在执行主机上不需要科学上网的流程，以便于一些特殊环境下实现部署安装（如在服务器环境）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4bde3-2306-491f-80bd-db813510f1bb",
   "metadata": {},
   "source": [
    "首先要下载model scope的工具包以便我们在国内环境下完成模型的下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6acbf93-aa3a-4614-95d3-1776f1dab79e",
   "metadata": {},
   "source": [
    "```\n",
    "pip install modelscope\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9d639-ca12-4a85-bdd3-2366c4e9eb8e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105104019415.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f0820-c19d-4bbb-818b-91366cdfde89",
   "metadata": {},
   "source": [
    "接下来在魔搭社区的模型托管页面分别下载对应的文件权重模型文件。在Modelscope的官网搜索模型的关键字可以找到该模型托管的页面，在模型栏可以找到具体的下载方式：\n",
    "\n",
    "https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bbc12d9-a39b-48ed-9e06-bbbae3e9a98f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108113917141.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c84a3d-770c-4c80-a570-74d7c9a366ce",
   "metadata": {},
   "source": [
    "通过以下方式可以指定你所要下载的文件地址，这样做可以指定下载文件的存储路径，需注意将`./dir-name`替换成你自己的文件路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf1ba4-9f8f-4f1e-8fa0-f4923446903e",
   "metadata": {},
   "source": [
    "modelscope download --model Qwen/Qwen2.5-coder-7B-Instruct  --local_dir ./dir-name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e62c740d-29b1-49f9-9a6a-658068d34e43",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105105456966.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb27970-6fab-480a-8fc6-f44f008641b6",
   "metadata": {},
   "source": [
    "下载完成后全部的文件列表如下："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58096cef-8f5d-4564-a7cb-e86728e75a1b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105105505662.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab3d59-9fea-4363-8bd0-20192e5e5b9c",
   "metadata": {},
   "source": [
    "到此全部的安装流程就已经结束了，接下来我们将进行启动演示的测试，在这个测试里我们使用transformers框架的库进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0ba03-a411-4503-b73f-5ff2d8b9cbac",
   "metadata": {},
   "source": [
    "通过`vim run.py`命令打开一个新的python文件并开始编辑，将以下代码复制进`vim run.py`中，注意进行调用地址的修改，保存并退出`wq`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526eb8e-6088-4b8d-b8ff-2584d012e50d",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# Now you do not need to add \"trust_remote_code=True\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B\")   #注意将Qwen/Qwen2.5-Coder-7B修改成你代码存储文件夹的绝对路径\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-7B\", device_map=\"auto\").eval() #注意将Qwen/Qwen2.5-Coder-7B修改成你代码存储文件夹的绝对路径\n",
    "\n",
    "# tokenize the input into tokens\n",
    "input_text = \"#write a quick sort algorithm\"\n",
    "model_inputs = TOKENIZER([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Use `max_new_tokens` to control the maximum output length.\n",
    "generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]\n",
    "# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.\n",
    "output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {input_text}\\n\\nGenerated text: {output_text}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06924729-3975-4e6b-ab72-058105dd4450",
   "metadata": {},
   "source": [
    "通过`python run.py`命令便可启动这个对话程序，如下图先进行各个权重模块（check point）的加载，随后返回推理结果：用python语言设计一个快速排序的小程序。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "421c864f-44a9-431e-a2d8-72111b312181",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105111336216.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc08f54-3d39-40d6-a582-7b7344249143",
   "metadata": {},
   "source": [
    "进行中文prompt的问答也是支持的，返回的结果在必要部分都会转换成中文："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab7b847a-3508-403d-9d31-d35251919fbc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105111741488.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18475343-039c-4a9d-87ff-ad90e63c6686",
   "metadata": {},
   "source": [
    "后台监测可以看到推理一共使用了将近20G的显存资源。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "876368a2-1cbf-4c9f-ae96-29d213a5b08e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105110811396.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3eecb-6d13-46fc-9bce-99719f856d8e",
   "metadata": {},
   "source": [
    "### 使用vLLM框架进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e49f2-ce01-4628-937a-1877ac56db67",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先来看一下什么是vLLM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11a42c-61bf-4502-8b46-7931c1118421",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM 是一个用于大型语言模型推理和服务的快速且易于使用的库，简单理解就是一个大模型的推理框架。这是一个用于快速 LLM 推理和服务的开源库。它提供的吞吐量比 HuggingFace Transformers 高 24 倍，而无需更改任何模型架构。vLLM在延迟方面也有显著优势。\n",
    "GLM4的官方集成了vLLM开源框架来实现部署和加速推理。\n",
    "\n",
    "&emsp;&emsp;vLLM论文地址：https://arxiv.org/pdf/2309.06180\n",
    "\n",
    "&emsp;&emsp;vLLM官方地址：https://docs.vllm.ai/en/latest/index.html\n",
    "\n",
    "&emsp;&emsp;vLLM支持模型列表\n",
    "https://docs.vllm.ai/en/latest/models/supported_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513045c-1ed1-46dc-b915-0b22696323b5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401292226013.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7151485-60aa-4417-b423-602073be5e83",
   "metadata": {},
   "source": [
    "&emsp;&emsp;vLLM有几个优化点值得我们去关注： 其一是*PagedAttention* 高效管理注意力键值内存，其二是vLLM支持多GPU和多节点推理，适用于大规模部署场景。其三，vLLM支持多种量化方案，如GPTQ、AWQ和FP8，优化了CUDA内核以提高推理效率。最后，它还可以为大语言模型构建API服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d763c9-8697-498c-b750-f9b675a96c68",
   "metadata": {},
   "source": [
    "接下来进行vLLM的部署与推理模型："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6484f1-02d7-4238-b200-9ceb1485c338",
   "metadata": {},
   "source": [
    "vLLM框架来帮我们加速推理,使用这个框架的好处是vllm会在推理过程中减少显存消耗，另外它作为一个专门服务于大模型的框架，可以更流程化的调用模型。通过以下指令可以下载最新版本的vLLM，这也是Qwen官方推荐的："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5fcfd2b-5305-450b-839b-88a7487739e0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105112642253.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505583b-5f16-4f7f-addd-c00d9a11b206",
   "metadata": {},
   "source": [
    "安装完成后可以通过以下命令查看下载是否成功，以及相应的版本信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7216bb-a0e5-487d-bf26-3360937be1ee",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241024103043928.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c02832-2fab-4ba6-858f-73909fef9f81",
   "metadata": {},
   "source": [
    "通过`vim run_vllm.py`命令打开一个新的python文件并开始编辑，将以下代码复制进文件中，注意进行调用地址的修改，保存并退出`wq`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08bebbe-897c-43e3-b27f-f04514b5065a",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/LLM/coder\")\n",
    "\n",
    "# max_tokens is for the maximum length for generation.\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=1024)\n",
    "\n",
    "# Input the model name or path. Can be GPTQ or AWQ models.\n",
    "llm = LLM(model=\"/home/LLM/coder\")\n",
    "\n",
    "# Prepare your prompts\n",
    "prompt = \"#write a quick sort algorithm.\\ndef quick_sort(\"\n",
    "\n",
    "# generate outputs\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "750981be-8156-446e-b204-1adb8231053c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107192841572.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd1dda-14c1-4e23-953d-9d715079416d",
   "metadata": {},
   "source": [
    "使用vLLM进行推理只消耗将近15G的显存，这个优化效果会随着模型的参数增大而逐渐明显。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "135c3e90-e60d-4d3a-970c-aa4ad46e885f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107192644550.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82de471-2bc7-401a-92a2-4708ade048d2",
   "metadata": {},
   "source": [
    "### 使用Ollama进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192f782-4e47-4c26-a139-d098bd763fdd",
   "metadata": {},
   "source": [
    "Ollama 是一个开源的大语言模型服务工具，专注于简化本地模型的创建、管理和部署流程。它可以帮助开发人员和数据科学家轻松地在本地或私有环境中使用大语言模型，而不必依赖于云服务，从而保证了数据隐私和灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8d8c9-e1df-4990-9d51-5e6e2ebb11b4",
   "metadata": {},
   "source": [
    "Ollama 非常适合需要在本地运行大语言模型的开发者和企业，如：\n",
    "\n",
    "- 开发和测试：在本地快速创建和测试新的语言模型。\n",
    "- 隐私保护：在本地部署模型，适用于有严格隐私需求的企业。\n",
    "- 多模型管理：轻松管理和部署多个模型，适合有大量模型管理需求的团队。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bd3fd-075e-46dc-86ba-cde26a6a7d40",
   "metadata": {},
   "source": [
    "在官网可以看到Ollama支持的模型列表 https://ollama.com/library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8d02e-f7e2-4181-a427-288a07691d26",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023151401629.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc475f-f143-4ecc-81a6-cd8230602613",
   "metadata": {},
   "source": [
    "每个模型下面有支持的功能和参数型号以及基本的模型描述，点击进入对应模型可以看到下载所需占用的内存大小。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36970b28-a11a-4375-84f2-58dc5aade6e6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105114604263.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbdac7-6c56-4cee-a061-4ebc68f86503",
   "metadata": {},
   "source": [
    "**使用Ollama实现LLM下载流程**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b990b-15e6-436f-ab42-aae6f8208c43",
   "metadata": {},
   "source": [
    "Ollama安装硬件要求：\n",
    "\n",
    "- Windows：3060以上显卡+8G以上显存+16G内存，硬盘空间至少20G显卡\n",
    "- \r\n",
    "Mac：M1或M2芯片 16G内存，20G以上硬盘空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca8c6a-f3d6-48cf-b28b-c4926afea122",
   "metadata": {},
   "source": [
    "下载Ollama的指令如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf4050-7ec8-4bb8-b04b-62ac5636e7ba",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b033c9-3f25-4de1-bfb4-7cf9b604de95",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011114550703.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d4328-d11a-49fd-a756-4cbb9820c9d5",
   "metadata": {},
   "source": [
    "下载完成后检测,如果返回版本号则说明成功下载：\n",
    "```\n",
    "ollama -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ae259-39f6-41a8-a852-a29ee604f6c0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011114741149.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab6d7f-7add-47e6-a491-58db2370f89e",
   "metadata": {},
   "source": [
    "通过指令**ollama help**可以查看该系统可执行的命令："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a7c44-29c8-4da0-9d50-9b7922b8bb72",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011144419942.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dca286-efd2-4dab-96c1-1c04f256b9a3",
   "metadata": {},
   "source": [
    "通过以下指令可以，检查ollama可运行的模型列表，可以看见之前下载的Llama 3-8b模型的信息呈现在列："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa670d88-6ff2-4653-b0aa-90c0c1c90b83",
   "metadata": {},
   "source": [
    "```\n",
    "ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b099c-f369-40f5-846f-3f1bda9a8b62",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241011115126172.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694f013-48c8-4892-a732-e7996e4fa772",
   "metadata": {},
   "source": [
    "**下载模型**\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1d2d8-dcd0-4888-aa7b-aa54d3906646",
   "metadata": {},
   "source": [
    "在终端中执行命令 `ollama run qwen2.5`或`ollama run qwen2.5：7b` ，即可下载 qwen2.5：7b 模型。模型下载完成后，会自动启动大模型，进入命令行交互模式，直接输入指令，就可以和模型进行对话了对应参数的模型的下载方式可以通过在Ollama官网查看到下载指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2482f28-ff17-486a-bbf6-56c82bdc8ebd",
   "metadata": {},
   "source": [
    ">以下是进行qwen2.5:72b模型进行下载&运行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014791de-e7a3-4ea5-aacb-1d95661657b3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241023151317687.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034562d-d480-452d-944d-78402ab11d5c",
   "metadata": {},
   "source": [
    "下载70B的模型只需要一行指令即可完成部署："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5e8d5-fa82-43c2-a7cb-c8c82bbd02fa",
   "metadata": {},
   "source": [
    "```\n",
    "ollama run qwen2.5-coder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d47eaf-9595-4c5f-840b-5a932ee9fcc6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105172802057.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79f688-9221-4b1f-842c-5f153030f424",
   "metadata": {},
   "source": [
    "完成下载后会直接进入模型启动状态，如果退出或刷新界面，再次输入指令`ollama run qwen2.5:72b`即可启动对应模型。\n",
    "在第一次输入该指令会进行模型的下载后开始推理任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59559d8-6b65-4add-b50d-d55a5720002b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105173255210.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a9732-34de-407a-b55b-b550e99e8f5d",
   "metadata": {},
   "source": [
    "由于Ollama支持的模型都是int4量化后的版本，这样做的优势在于大幅减少了推理时所需的显存消耗（如下图在使用Ollama框架进行Qwen-2.5-coder-7B的推理的时候近使用了将近6G的显存），不过这样做的坏处是会丢失一定的精度，导致有推理时准确性下降的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28cd1b-8a8f-41a7-ad68-bfe8c8bb031f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105173400927.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd342057-06f5-4d55-b25c-04f9dace43aa",
   "metadata": {},
   "source": [
    "对该模型进行代码能力测试后可以看到，Qwen-2.5-coder-7B具有不错的代码生成能力以及代码纠正能力，可以一定程度上用来辅助代码编程或实现一些自动化代码流程。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "727a45c9-8567-4a06-863f-e928a31e0db0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105174110746.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb42f2-1ad3-4b62-b6b0-5874ba60026d",
   "metadata": {},
   "source": [
    "可以直接将遇到的代码报错信息发到ollama进行交互，可以看到返回有效的修改建议。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19a77699-8895-4b74-85a1-cd63cba57495",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105174244515.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f8b55-36f3-487b-ad35-1bb6a83729c4",
   "metadata": {},
   "source": [
    ">使用 /bye 退出正在进行的 Ollama进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a3021-a9b8-4084-bff1-22a5e0383357",
   "metadata": {},
   "source": [
    "**4.3 如何卸载安装**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469b97e-28e5-4533-99f8-fc2655a693af",
   "metadata": {},
   "source": [
    "直接在你的安装目录下，删除ollama文件夹即可。所有下载的数据和大模型文件都在里面，Ollama 的默认安装目录通常是用户主目录下的 .ollama 文件夹。例如，在 Unix 系统（如 macOS 和 Linux）上，默认安装目录为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb50eec-e1ac-498c-ae8f-2ff620203663",
   "metadata": {},
   "source": [
    "~/.ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b4b60-4d5f-4531-a43d-1c73406a768e",
   "metadata": {},
   "source": [
    "通过指令`Ollama list`可以查看所有已经安装好的Ollama支持的模型列表，其中latest标记是指该系列最具代表模型。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52891e3f-6119-4249-b691-3188630d61f2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241106105029174.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107241e-37ff-4c0e-a5ec-1309648c8134",
   "metadata": {},
   "source": [
    "或可以通过指令 `ollama rm modlename`的方式来移除对应的大模型文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba946ef-c78b-4589-9bea-92380d06e502",
   "metadata": {},
   "source": [
    "# 4.线上部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8200bc-2af2-4a73-a298-279bd105f79f",
   "metadata": {},
   "source": [
    "本节内容介绍的是在AutoDL平台上租赁算⼒资源，并完成Qwen-2.5-coder-7B的部署调⽤的流程。这种云平台部署⼀种轻量化的使用⽅式，⾮常适合⼊⻔级的测试开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ed0e6-c9ed-46dd-aa89-cc7a92259df4",
   "metadata": {},
   "source": [
    "### 1.物理机 or 云服务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dff283-d926-468e-90d3-c096268be79f",
   "metadata": {},
   "source": [
    "- 完全⼩⽩，对⼤模型技术没有了解，建议⽤新⼈账号⽩嫖各⼤云服务平台的免费算⼒，再考虑购买\n",
    "或者租赁。\n",
    "- 如果经常做微调实验，或实验室学⽣系统学习，有⾃⼰的物理机将更加⽅便，按照学习实践部分内\n",
    "容采购即可。\n",
    "- 为⽤⼾提供相关的推理服务，⾸选云服务，有更⼤参数量，更好性能的模型选择，随⽤随停，按量\n",
    "计费。\n",
    "- “独⻆兽”公司AI应⽤/⼤模型AI技术创新公司…需要⼤规模⼤批量的微调训练或者对内/对外\n",
    "提供⼤量推理服务，按需配备⾼性能GPU服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d10264-85ce-4b7c-8ec4-78415c6b0c5f",
   "metadata": {},
   "source": [
    "算⼒平台\n",
    "主要适⽤于学习和训练，不适⽤于企业级部署提供服务。\n",
    "- ModelScope：阿⾥出品，中国的“HuggingFace”，模型开源社区，绑定阿⾥云有（24GB显存\n",
    "+36⼩时）GPU环境。https://www.modelscope.cn/home\n",
    "- Colab：⾕歌出品，升级服务仅需 9 美⾦。https://colab.research.google.com/\n",
    "- Kaggle：免费，每周 30 ⼩时 T4，P100 可⽤。https://www.kaggle.com/\n",
    "- AutoDL：价格亲⺠，⽀持 Jupyter Notebook 及 ssh，国内可⽤。https://www.autodl.com/home\n",
    "- 矩池云：操作便捷，⽀持 Jupyter Notebook 及 ssh，国内可⽤。https://matgo.cn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a08a1-ab00-4f83-8320-eafdfed1c0f5",
   "metadata": {},
   "source": [
    "### 2.算力准备 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5420f3-f9ae-4352-98a4-6219570f6fdb",
   "metadata": {},
   "source": [
    "在正式安装之前，需要先确保拥有⾜够的算⼒资源，以下推荐的是⼀种轻量化的部署⽅式，⾮常适合 ⼊⻔级的测试开发：⾸先可以从以下链接中进⼊AutoDL的官⽅⽹址，在右上⻆的选项⾥可以注册/登 录. https://www.autodl.com/home"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9237a7e2-45a4-4421-996f-2c0648315940",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108110626928.png\" width=80%></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ec944cc-c3c8-4e73-8770-831d8ecacd29",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108110657680.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bbf27-b078-4c0e-bef9-b5e29a6e741c",
   "metadata": {},
   "source": [
    "进⼊界⾯之后点击右上⻆的⽤⼾信息可以查看余额和进⾏充值，其⾦额可以⾃定义。注意：只有账⼾有余额才能在后续算⼒市场租赁主机。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d690e41-5941-4383-9daf-72e1f92e2066",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108110742630.png\" width=60%></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d175b75-fe60-44d3-9273-f451d1a56e7f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108110811505.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3defb-f512-4d7d-bb18-e63e67c1bb06",
   "metadata": {},
   "source": [
    "确认⽤⼾余额充裕后点击左上⻆的算⼒市场，租赁合适的主机，推荐的配置为：计费⽅式选择按量计\n",
    "费、地区任选、GPU型号选择RTX3090/24GB、GPU数量选择为1。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1cdbea7-894d-4ca4-a191-499f1253e595",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108110912073.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cce8b4-24fb-42f8-a288-9f10a86431a4",
   "metadata": {},
   "source": [
    "\r\n",
    "选择好合适的主机后需要在下⽅的镜像栏中选择适合的框架⸺框架名称：PyTorch，框架版本： 2.0.0，Python版本：3.8（Ubuntu20.04），Cuda版本11.8.选择好之后点击右下⻆的⽴即创建便可完 成配置。\n",
    "\n",
    " 其中 PyTorch 是⼀个流⾏的深度学习框架，⽀持⼤规模模型的训练和推理。Python 3.8 是⼀个稳定且 常⽤的版本，兼容⼤多数机器学习库和⼯具。选择 Ubuntu 20.04 作为操作系统版本是因为其⻓期⽀持 和⼴泛使⽤，特别适合在⽣产环境中部署。Cuda 是 NVIDIA 提供的并⾏计算平台和编程模型，⽀持 GPU 加速。选择 Cuda 11.8 版本是因为它与 PyTorch 2.0.0 兼容。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25126e25-2e44-4b41-a46a-423923703d24",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108111030290.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc82e4-af02-47c5-8a31-bab6c94619b9",
   "metadata": {},
   "source": [
    "创建完成后，点击左边栏的容器实例便可随时找到配置好的实例，在快捷⼯具栏中点击Jupyter lab开\n",
    "始模型的安装部署。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e0477a6-d52a-4476-9bfb-a5b2d8ef5270",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108111111192.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf1d96-754d-421b-b2d0-22a91ed2d51e",
   "metadata": {},
   "source": [
    "### 3.换源和安装依赖包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40b0e3-667f-425a-ab5e-e3b3f471c93e",
   "metadata": {},
   "source": [
    "进⼊Jupyter lab打开终端开始环境配置，⾸先要进⾏的是 pip 换源和安装依赖包。点击启动终端，在\n",
    "其中逐⾏输⼊以下代码以实现功能。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfcd8289-3374-43a3-ac6b-fd2c7ba0fa21",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108111339686.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d3ddc-162a-428a-9c7a-b00bd16cb9c0",
   "metadata": {},
   "source": [
    "在终端通过命令升级 pip，确保使⽤的是最新版本的 pip，这样可以避免在安装库时出现兼容性问题。\n",
    "更换 pip 的默认源为清华⼤学的镜像源，以加速 Python 库的下载和安装。\n",
    "以下是安装的库的介绍：\n",
    "\n",
    "- modelscope: ⽤于模型推理和部署的库，⽀持多种机器学习和深度学习模型。\n",
    "- transformers: 包含了⼤量预训练的 Transformer 模型，包括 BERT、GPT 等等。\n",
    "- sentencepiece: ⼀个⽤于处理⽂本的库，特别是对⼦词单元进⾏分词操作，常⽤于⾃然语⾔处理任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98820843-791d-4199-8e13-ce236963458b",
   "metadata": {},
   "source": [
    "```\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "pip install modelscope\n",
    "pip install transformers\n",
    "pip install sentencepiece\n",
    "pip install accelerate\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bda08081-a2ef-4915-b035-e4afa2a32caf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108111807570.png\" width=80%></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "855006c7-dac6-4ab4-828d-66bb7c8bde0e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108111822483.png\" width=80%></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3f4226e-1dc9-4ef1-b9d5-1b18de440898",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108111908845.png\" width=80%></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64ca9939-032d-4474-b305-5be361b6699b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107162524170.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831992c-b719-4b77-80e9-8314023afae6",
   "metadata": {},
   "source": [
    "### 4.模型下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996c3df-90f4-4509-bc63-214648fc5cd4",
   "metadata": {},
   "source": [
    "在启动⻚打开新的Jupiter notebook进⾏模型的下载。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8514be24-5ff9-4cb1-95d9-208ec47ab171",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108112033931.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c12e45-8d00-4cae-8be0-c12414ea3795",
   "metadata": {},
   "source": [
    "在jupyer notebook中执行以下代码便可实现模型的下载和推理任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f0b01d-1963-497d-bd8e-c7b91a426f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c50dbc3-fe4f-40e2-98a2-ec53fe9c5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f88d80-59ec-41df-9ab7-f05e3f2dfd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ac97e533314f43b3ccc1aabac2f760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146e3f59b33b4a15be24a5302a3ad390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e891811b1a47249e81dae23cfd9bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0995e641b95a45e18a4b74ea12c534f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2e61e461b047efbfffd0e4af545b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7148a7383f54042ae657edc6b2f9737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00001-of-00004.safetensors]:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72afc60a18bf4452841b6daf312aa775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00002-of-00004.safetensors]:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeb62613f894d3793c58ca1cca195f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00003-of-00004.safetensors]:   0%|          | 0.00/4.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac39d61ce2174df69bfd9cca6e89a658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00004-of-00004.safetensors]:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf7e20e8289416399865193c66cd626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors.index.json]:   0%|          | 0.00/27.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeb895a1378467ca293d8e4ff1216db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/5.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6705e7e1e99d431fab708493468ccf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8258cd22814943319e8dfb51ee75f88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1606886f14f944caa6b231c20160f663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e54775a08d419bb8f86201b1ba6792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2df428-c00c-4402-9323-11910f80aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d3405b-bb82-4dac-a75e-8ccfbfb371ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc57ca4-780c-48d5-82a9-716e41609466",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f306a2-e019-40b3-b273-34637e092203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! QuickSort is a popular and efficient sorting algorithm that uses a divide-and-conquer approach to sort elements. Below is a simple implementation of the QuickSort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    # Base case: if the array is empty or has one element, it's already sorted\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    \n",
      "    # Choose a pivot element from the array\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    \n",
      "    # Partition the array into three parts:\n",
      "    # - left: elements less than the pivot\n",
      "    # - middle: elements equal to the pivot\n",
      "    # - right: elements greater than the pivot\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    \n",
      "    # Recursively apply quicksort to the left and right partitions,\n",
      "    # and concatenate the results with the middle partition\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Example usage:\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "sorted_arr = quicksort(arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Base Case**: If the array has zero or one element, it is already sorted, so we return it as is.\n",
      "2. **Pivot Selection**: We choose the middle element of the array as the pivot. This helps in achieving average-case performance.\n",
      "3. **Partitioning**: We create three lists:\n",
      "   - `left`: All elements less than the pivot.\n",
      "   - `middle`: All elements equal to the pivot.\n",
      "   - `right`: All elements greater than the pivot.\n",
      "4. **Recursive Sorting**: We recursively apply the `quicksort` function to the `left` and `right` lists.\n",
      "5. **Concatenation**: Finally, we concatenate the sorted `left` list, the `middle` list, and the sorted `right` list to get the final sorted array.\n",
      "\n",
      "This implementation is straightforward but not the most optimized in terms of space complexity due to the use of additional lists. For an in-place version, you can modify the algorithm to avoid creating new lists.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2f8ce-f7fe-47c2-b575-f01c672ee628",
   "metadata": {},
   "source": [
    "完成任务后记得关闭实例，AutoDL平台会默认保存全部文件和环境信息15天，如果超过日期没有再次激活会自动释放存储空间，所以要记得定时启动项目避免资料丢失。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22ffd1b9-5ad9-433b-83b8-b72d7aec3bad",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108112851235.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c0209-7679-4298-9f21-917d870937e7",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6ad01-05f0-4857-9017-24d554d43a7c",
   "metadata": {},
   "source": [
    "# 5.本地部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405954d6-4504-445b-b7c4-bbd011c996f9",
   "metadata": {},
   "source": [
    ">以下操作都是在命令行实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68edd09-6357-4d68-84da-d26e2c4c6d98",
   "metadata": {},
   "source": [
    "- **Step 1. 创建conda虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f4cf5-7072-4b38-9867-5e9f4d687283",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Conda创建虚拟环境的意义在于提供了一个隔离的、独立的环境，用于Python项目和其依赖包的管理。每个虚拟环境都有自己的Python运行时和一组库。这意味着我们可以在不同的环境中安装不同版本的库而互不影响。官方给出的Python版本要求是大于`3.9`。创建虚拟环境的办法可以通过使用以下命令创建：\n",
    "\n",
    "```bash\n",
    "# qwen-math是你想要给环境的名称，python=3.11 指定了要安装的Python版本。你可以根据需要选择不同的名称和/或Python版本。\n",
    "\n",
    "conda create -n qwen-math python=3.11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e3ec1-04a4-44cd-87bd-dbe0489a78bd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241106114831800.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832d28e-6617-407b-a296-6b5a720a2119",
   "metadata": {},
   "source": [
    "&emsp;&emsp;创建虚拟环境后，需要激活它。使用以下命令来激活刚刚创建的环境。如果成功激活，可以看到在命令行的最前方的括号中，就标识了当前的虚拟环境。虚拟环境创建完成后接下来安装torch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd5495-2ad7-402b-9553-1fcb4bd6786b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241106114943800.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac3f19-1b29-4fbe-8e4f-9f658c039610",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果忘记或者想要管理自己建立的虚拟环境，可以通过conda env list命令来查看所有已创建的环境名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7821f1-cbfe-4d36-b1ad-4d0168a7a515",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143512354.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b378b-97ed-4d4e-adfc-92886a102ff4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果需要卸载指定的虚拟环境则通过以下指令实现：\n",
    "```\n",
    "conda env remove --name envname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf38d0a-e84d-4322-b257-2cb45fb1e194",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240928143643575.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acd79f-39f4-4464-b2f4-8588c350139e",
   "metadata": {},
   "source": [
    "- 需要注意的是无法卸载当前激活的环境，建议卸载时先切换到base环境中再执行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ec0e6-690a-4e16-a611-d0b38e60ca49",
   "metadata": {},
   "source": [
    "- **Step 2. 查看当前驱动最高支持的CUDA版本**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73ef10-7f8d-4a84-808a-8167f97bfbb2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们需要根据CUDA版本选择Pytorch框架，先看下当前的CUDA版本：\n",
    "```\n",
    "nvidia -smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9e646-4363-4dac-ac0d-8058c6542406",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20240924161818464.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44be84a-dbd3-47e8-8b79-aa8d044252e6",
   "metadata": {},
   "source": [
    "- **Step 3. 在虚拟环境中安装Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712fd5d-87b7-4bdf-aa39-13b2daea1306",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进入Pytorch官网：https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c71152-4fd7-4e44-9506-6520b4b5fac7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240103163206436.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798fa913-d787-429b-882c-05f8573c48ac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当前的电脑CUDA的最高版本要求是12.2，所以需要找到不大于12.2版本的Pytorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b517b-2e55-4c00-b43d-a4f2d97dc4ff",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401041455184.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60145097-fe23-4f3e-9d5a-eba4873f7de0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直接复制对应的命令，进入终端执行即可。这实际上安装的是为 CUDA 12.1 优化的 PyTorch 版本。这个 PyTorch 版本预编译并打包了与 CUDA 12.1 版本相对应的二进制文件和库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f967a85-5b63-4350-aabf-2d7a99af58bc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105110047278.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0861c27-29f4-4a91-9a03-3376a932104c",
   "metadata": {},
   "source": [
    "- **Step 4. 安装Pytorch验证**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362973c-e807-4d0e-bc27-bcefcbd918a8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;待安装完成后，如果想要检查是否成功安装了GPU版本的PyTorch，可以通过几个简单的步骤在Python环境中进行验证：\n",
    "```bash\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee46a1c-aa98-4801-afa2-4de1cc8399d1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154248388.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad8320-3f20-423a-9cd3-a50dfce1834d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果输出是版本号数字，则表示GPU版本的PyTorch已经安装成功并且可以使用CUDA，如果显示ModelNotFoundError，则表明没有安装GPU版本的PyTorch，或者CUDA环境没有正确配置，此时根据教程，重新检查自己的执行过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c5bf9-85a7-447e-b7a1-eee767e164f0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241022154338394.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a0d17-19cb-4868-b7a8-b65a02afe217",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然通过pip show的方式可以很简洁的查看已安装包的详细信息。pip show <package_name> 可以展示出包的版本、依赖关系（展示一个包依赖哪些其他包）、定位包安装位置、验证安装确实包是否正确安装及详情。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fd2f1-3d2d-4e39-8ccc-c8ae9bd621c6",
   "metadata": {},
   "source": [
    "- **Step 5. 安装必要的依赖包**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8c4fa-8545-425e-9565-e1571c3aa7db",
   "metadata": {},
   "source": [
    "Transfomers是大模型推理时所需要使用的框架，建议使用的版本`Transfomers>=4.37.0`，通过以下指令可以下载最新版本的Transfomers："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a30c9-90d7-4e0d-9468-4c7c9210f187",
   "metadata": {},
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72021556-a9b8-4512-a0a3-5ff294a013f6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105105546261.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f334de-909e-41c4-b1de-9d8e90e04ef2",
   "metadata": {},
   "source": [
    "安装完成后可以通过以下命令检查是否安装：\n",
    "```\n",
    "pip show transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a814c9-1d0f-4e1a-b6e2-22236f986e96",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107183539421.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652ae8f-63e3-46eb-9dd4-87dcf19c33b2",
   "metadata": {},
   "source": [
    "接下来需要安装下载工具modelscope以及接下来用来加速模型的训练和部署的库accelerate，通过以下代码进行对应工具的部署：\n",
    "```\n",
    "pip install modelscope\n",
    "pip install accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256940-5014-45d9-828c-1fd821885615",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105104019415.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa040d3-3718-4a02-994a-6b0168ac5ee4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105110657041.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db3644-6dc9-4c76-8ccc-3ee69e603dad",
   "metadata": {},
   "source": [
    "- **Step 6. 下载模型文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ffb55-8b70-479b-97a0-179b1ee0880d",
   "metadata": {},
   "source": [
    "首先我们要创建一个新的文件用于储存下载的代码信息，通过指令`mkdir +name`可以创建一个新的文件夹，创建完毕后用命令`cd +name`可以进入到该文件夹以便于稍后的操作。通过指令`pwd`可以查看当前所在位置的路径信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2005980-64f1-4433-93dc-0bdffea63bfa",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105103632755.png\" width=50%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae938a-84c4-475b-9210-d17e01847350",
   "metadata": {},
   "source": [
    "- **Step 7. 在modelscope源下载模型文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf99020-30b0-4921-9649-7c1fb7caecfd",
   "metadata": {},
   "source": [
    "在确认进入所在工作文件夹后首先进行模型的项目文件的下载："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d50468-42dd-4298-937d-692a490d365e",
   "metadata": {},
   "source": [
    "这种办法是一种在执行主机上不需要科学上网的流程，以便于一些特殊环境下实现部署安装（如在服务器环境）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3507c-d557-4aea-9278-5e1bf8805465",
   "metadata": {},
   "source": [
    "首先要下载model scope的工具包以便我们在国内环境下完成模型的下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df72cad-a19b-4648-b694-f6e97bb624e7",
   "metadata": {},
   "source": [
    "```\n",
    "pip install modelscope\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45087ab1-72a2-4814-9834-da2f50c7a891",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105104019415.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818bc1e-0041-46dc-b6aa-af14e02621a1",
   "metadata": {},
   "source": [
    "接下来在魔搭社区的模型托管页面分别下载对应的文件权重模型文件。在Modelscope的官网搜索模型的关键字可以找到该模型托管的页面，在模型栏可以找到具体的下载方式：\n",
    "\n",
    "https://modelscope.cn/models/Qwen/Qwen2.5-Math-1.5B-Instruct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe01be4d-d8e9-4bff-a208-ff85bb1eb282",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108113836195.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3b565-6375-4434-8ba8-de8d14f9e771",
   "metadata": {},
   "source": [
    "通过以下方式可以指定你所要下载的文件地址，这样做可以指定下载文件的存储路径，需注意将`./dir-name`替换成你自己的文件路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b279b1-5d0e-4883-afb9-c58bc5ccf14c",
   "metadata": {},
   "source": [
    "modelscope download --model Qwen/Qwen2.5-Math-1.5B-Instruct  --local_dir ./dir-name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4426a3c8-30d7-42d4-aab1-995626d74b45",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241106154547461.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927c7f2-1276-49d6-9e81-ee1264c014c8",
   "metadata": {},
   "source": [
    "下载完成后全部的文件列表如下："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a21bd3d-b9d2-49a2-867b-02b20dfca995",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107105006299.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc5e18-7051-4cce-9d09-5c8cd79dc0e8",
   "metadata": {},
   "source": [
    "到此全部的安装流程就已经结束了，接下来我们将进行启动演示的测试，在这个测试里我们使用transformers框架的库进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b9b52-e0e5-4e72-afbe-03c90cb62ae8",
   "metadata": {},
   "source": [
    "通过`vim r.py`命令打开一个新的python文件并开始编辑，将以下代码复制进`vim r.py`中，注意进行调用地址的修改，保存并退出`wq`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d9a0d-487d-4ea2-aec4-19b35bdff451",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"transformers\")\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# Now you do not need to add \"trust_remote_code=True\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"/home/data/LLM/Math7B\")\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(\"/home/data/LLM/Math7B\", device_map=\"auto\").eval()\n",
    "\n",
    "# tokenize the input into tokens\n",
    "input_text = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$\"\n",
    "model_inputs = TOKENIZER([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Use `max_new_tokens` to control the maximum output length.\n",
    "generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]\n",
    "# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.\n",
    "output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {input_text}\\n\\nGenerated text: {output_text}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88380e02-f78a-4c41-b350-896c0b0ce28a",
   "metadata": {},
   "source": [
    "通过`python r.py`命令便可启动这个对话程序，如下图先进行各个权重模块（check point）的加载，随后返回推理结果：用python语言设计一个快速排序的小程序。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "697557d8-10f4-4928-9bb7-af885860aeaa",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107155508305.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98239727-c4c6-45da-8a89-ae3a3156cec6",
   "metadata": {},
   "source": [
    "进行中文prompt的问答也是支持的，返回的结果在必要部分都会转换成中文："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cbb6ff4-589d-4410-a6b4-f29381c62661",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241105111741488.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522605b-c604-4a87-8434-8ffdc26dafe5",
   "metadata": {},
   "source": [
    "后台监测可以看到7B模型进行推理一共使用了将近30G的显存资源。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e97d36b2-77d0-4da7-8b91-f36573aa4f80",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241107160017077.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7856be1-f393-40eb-b29c-ea2c0f5d0589",
   "metadata": {},
   "source": [
    "# 6.线上部署流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449f71f-9a8a-4733-8cb9-8c26b08c464f",
   "metadata": {},
   "source": [
    "在AutoDL平台线上部署Math模型的前三步流程和Coder模型的一样，可以参考前文内容先完成前置步骤再进行以下流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f8a62-2a41-494d-900f-71fe59c3d502",
   "metadata": {},
   "source": [
    "在启动⻚打开新的Jupiter notebook进⾏模型的下载。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7751b2-0d96-4d09-9273-511a05ffda97",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/LingYu/image-20241108112033931.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f16a2d-2bc4-488a-9014-6e229bd22eee",
   "metadata": {},
   "source": [
    "在jupyer notebook中执行以下代码便可实现模型的下载和推理任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfc4509-a690-49f6-9e65-228826b57446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d99b68-6946-410b-be46-4ba4ece1f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-MAth-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2498ba6-937e-40cc-ac16-172fdc07cf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9567d3c044a04a75b4be497625d337d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502d0224-8641-41ed-8d83-58ffd8f40ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8620c0f-78af-422e-8a7f-dcd14718cfc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b6c3335-3968-4d02-8984-2a2c93675c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2236c4f8-07a1-46da-a173-591de00f1517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation \\(4x + 5 = 6x + 7\\), we need to isolate the variable \\(x\\). Here are the steps to do that:\n",
      "\n",
      "1. Start with the original equation:\n",
      "   \\[\n",
      "   4x + 5 = 6x + 7\n",
      "   \\]\n",
      "\n",
      "2. Subtract \\(4x\\) from both sides of the equation to move all the \\(x\\)-terms to one side:\n",
      "   \\[\n",
      "   4x + 5 - 4x = 6x + 7 - 4x\n",
      "   \\]\n",
      "   Simplifying both sides, we get:\n",
      "   \\[\n",
      "   5 = 2x + 7\n",
      "   \\]\n",
      "\n",
      "3. Next, subtract 7 from both sides to move the constant term to the other side:\n",
      "   \\[\n",
      "   5 - 7 = 2x + 7 - 7\n",
      "   \\]\n",
      "   Simplifying both sides, we get:\n",
      "   \\[\n",
      "   -2 = 2x\n",
      "   \\]\n",
      "\n",
      "4. Finally, divide both sides by 2 to solve for \\(x\\):\n",
      "   \\[\n",
      "   \\frac{-2}{2} = \\frac{2x}{2}\n",
      "   \\]\n",
      "   Simplifying both sides, we get:\n",
      "   \\[\n",
      "   -1 = x\n",
      "   \\]\n",
      "\n",
      "So, the value of \\(x\\) that satisfies the equation is \\(\\boxed{-1}\\).\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
